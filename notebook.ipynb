{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25623250",
   "metadata": {},
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con CNN\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza un'architettura CNN per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61feb3",
   "metadata": {},
   "source": [
    "## 1. Importazione delle Librerie Necessarie\n",
    "\n",
    "Importiamo tutte le librerie necessarie per l'elaborazione audio, deep learning e visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99928a9a",
   "metadata": {},
   "source": [
    "## 2. Configurazione dell'Ambiente di Esecuzione\n",
    "\n",
    "In questa sezione configuriamo l'ambiente di esecuzione in modo che il notebook funzioni sia su Kaggle, che su Google Colab, che in locale. Il codice rileverà automaticamente l'ambiente e configurerà i percorsi di conseguenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44310596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente rilevato: local\n"
     ]
    }
   ],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            # quindi non impostiamo ancora i percorsi derivati\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio\n",
    "        self.SR = 32000  # Sample rate\n",
    "        self.DURATION = 5  # Durata dei clip in secondi\n",
    "        self.N_MELS = 128  # Numero di bande Mel\n",
    "        self.N_FFT = 2048  # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 512  # Hop length per STFT\n",
    "        self.FMIN = 0  # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = self.SR / 2  # Frequenza massima\n",
    "        \n",
    "        # Parametri per il training\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.EPOCHS = 5  # Mantieni basso per il modello baseline\n",
    "        self.LEARNING_RATE = 1e-3\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 0  # Numero di worker per il DataLoader\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/simplecnn/PyTorch/default/1\"\n",
    "    cache_model_file = os.path.join(cache_model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(cache_model_file)\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/simplecnn/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    model_file = os.path.join(model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {model_file}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f17ef",
   "metadata": {},
   "source": [
    "## 3. Configurazione del Modello e Parametri\n",
    "\n",
    "Definiamo i parametri di configurazione per il preprocessamento audio, la creazione dello spettrogramma Mel e l'addestramento della CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I parametri principali sono già definiti nella classe Config\n",
    "# Verifichiamo l'esistenza delle directory e creiamo quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")\n",
    "print(f\"- Learning rate: {config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b0570",
   "metadata": {},
   "source": [
    "## 4. Caricamento e Preprocessing dei Dati\n",
    "\n",
    "In questa sezione carichiamo i metadati dal file CSV di training, creiamo codifiche one-hot per le etichette delle specie e implementiamo funzioni per il caricamento e preprocessamento dei file audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    taxonomy_df = pd.read_csv(config.TAXONOMY_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "# Crea un test set solo se NON siamo su Kaggle\n",
    "if config.environment != 'kaggle':\n",
    "    # Crea un subset per il testing dai dati di validation\n",
    "    test_size = 0.1  # Percentuale di dati di validation da usare come test\n",
    "    test_indices, _ = train_test_split(range(len(X_val_df)), test_size=test_size, random_state=42)\n",
    "    X_test_df = X_val_df.iloc[test_indices].reset_index(drop=True)\n",
    "    y_test_one_hot = y_val_one_hot[test_indices]\n",
    "    print(f\"Dimensioni Test Set (creato da validation): {X_test_df.shape}, Etichette: {y_test_one_hot.shape}\")\n",
    "    \n",
    "    # Creiamo il test dataset come BirdDataset regolare\n",
    "    test_dataset = BirdDataset(X_test_df, config.TRAIN_AUDIO_DIR, y_test_one_hot)\n",
    "else:\n",
    "    print(\"Ambiente Kaggle rilevato: utilizzerò direttamente i dati di test dalla cartella test_soundscapes\")\n",
    "    \n",
    "    # Per Kaggle, dovremo creare un dataset speciale per le soundscapes di test\n",
    "    # Questo verrà utilizzato direttamente nella fase di generazione della submission\n",
    "    # Non creiamo X_test_df e test_dataset per ora\n",
    "    X_test_df = None\n",
    "    y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3adbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, target_sr=config.SR, duration=config.DURATION):\n",
    "    \"\"\"\n",
    "    Carica un file audio, lo padda/tronca e lo converte in spettrogramma Mel.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Percorso del file audio\n",
    "        target_sr: Sample rate target\n",
    "        duration: Durata target in secondi\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Spettrogramma Mel log-normalizzato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carica il file audio\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        # Padda o tronca alla durata target\n",
    "        target_len = int(target_sr * duration)\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:target_len]\n",
    "        \n",
    "        # Calcola lo spettrogramma Mel\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr,\n",
    "            n_fft=config.N_FFT,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            n_mels=config.N_MELS,\n",
    "            fmin=config.FMIN,\n",
    "            fmax=config.FMAX\n",
    "        )\n",
    "        \n",
    "        # Converti in scala logaritmica (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalizza (opzionale, ma spesso utile)\n",
    "        min_val = np.min(log_mel_spec)\n",
    "        max_val = np.max(log_mel_spec)\n",
    "        if max_val > min_val:\n",
    "            log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            log_mel_spec = np.zeros_like(log_mel_spec)  # Gestisci clip silenziosi\n",
    "        \n",
    "        return log_mel_spec\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "        # Restituisci un array fittizio della forma attesa in caso di errore\n",
    "        time_steps = int(target_sr * duration / config.HOP_LENGTH) + 1\n",
    "        return np.zeros((config.N_MELS, time_steps), dtype=np.float32)\n",
    "\n",
    "# Test della funzione di preprocessing su un file\n",
    "if len(train_df) > 0:\n",
    "    sample_idx = 0\n",
    "    sample_file = train_df.loc[sample_idx, 'filename']\n",
    "    sample_path = os.path.join(config.TRAIN_AUDIO_DIR, sample_file)\n",
    "    print(f\"Processing sample file: {sample_path}\")\n",
    "    \n",
    "    if os.path.exists(sample_path):\n",
    "        mel_spec_sample = load_and_preprocess_audio(sample_path)\n",
    "        print(f\"Forma dello spettrogramma: {mel_spec_sample.shape}\")  # Dovrebbe essere (N_MELS, time_steps)\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(\n",
    "            mel_spec_sample, sr=config.SR, hop_length=config.HOP_LENGTH,\n",
    "            x_axis='time', y_axis='mel', fmin=config.FMIN, fmax=config.FMAX\n",
    "        )\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(f'Spettrogramma Mel per {sample_file}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"File audio di esempio non trovato: {sample_path}\")\n",
    "else:\n",
    "    print(\"Il dataframe di training è vuoto. Impossibile processare l'esempio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31556e1",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch per Dati Audio\n",
    "\n",
    "Creiamo un dataset PyTorch personalizzato per caricare e preparare i nostri dati audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301eb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset per caricare e preparare i file audio di uccelli.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con i metadati\n",
    "            audio_dir: Directory contenente i file audio\n",
    "            labels_one_hot: Array di etichette one-hot\n",
    "            transform: Trasformazioni da applicare (opzionale)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.transform = transform  # Per potenziali augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Costruzione del percorso del file\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        # Verifica se il file esiste prima di caricarlo\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Attenzione: File non trovato in {file_path}. Verifica il percorso e la struttura dei dati.\")\n",
    "            # Restituisci dati fittizi o solleva un errore\n",
    "            time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "            dummy_spec = torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)\n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return dummy_spec, dummy_label\n",
    "            \n",
    "        # Carica e preprocessa l'audio -> Spettrogramma Mel\n",
    "        mel_spec = load_and_preprocess_audio(file_path)\n",
    "        \n",
    "        # Aggiungi dimensione del canale (necessaria per CNN) -> (1, n_mels, time_steps)\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "        \n",
    "        # Converti in tensor\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "        \n",
    "        # Ottieni le etichette corrispondenti\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Applica trasformazioni se presenti\n",
    "        if self.transform:\n",
    "            mel_spec_tensor = self.transform(mel_spec_tensor)\n",
    "            \n",
    "        return mel_spec_tensor, label_tensor\n",
    "\n",
    "# Creiamo i dataset e i dataloader\n",
    "train_dataset = BirdDataset(X_train_df, config.TRAIN_AUDIO_DIR, y_train_one_hot)\n",
    "val_dataset = BirdDataset(X_val_df, config.TRAIN_AUDIO_DIR, y_val_one_hot)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Crea il test loader solo se non siamo su Kaggle\n",
    "if config.environment != 'kaggle':\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\"Numero di sample nel training set: {len(train_dataset)}\")\n",
    "    print(f\"Numero di sample nel validation set: {len(val_dataset)}\")\n",
    "    print(f\"Numero di sample nel test set: {len(test_dataset)}\")\n",
    "    print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "    print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")\n",
    "    print(f\"Numero di batch di test: {len(test_loader)}\")\n",
    "else:\n",
    "    # Su Kaggle non creiamo un test_loader standard\n",
    "    test_loader = None\n",
    "    \n",
    "    print(f\"Numero di sample nel training set: {len(train_dataset)}\")\n",
    "    print(f\"Numero di sample nel validation set: {len(val_dataset)}\")\n",
    "    print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "    print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")\n",
    "    print(\"Test set: utilizzerò direttamente i file nella cartella test_soundscapes\")\n",
    "\n",
    "print(f\"Numero di sample nel training set: {len(train_dataset)}\")\n",
    "print(f\"Numero di sample nel validation set: {len(val_dataset)}\")\n",
    "print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e60da",
   "metadata": {},
   "source": [
    "## 6. Definizione del Modello CNN\n",
    "\n",
    "Implementiamo una semplice architettura CNN per la classificazione degli spettrogrammi Mel degli audio di uccelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a661eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBirdCNN(nn.Module):\n",
    "    def __init__(self, num_classes=config.N_CLASSES):\n",
    "        super(SimpleBirdCNN, self).__init__()\n",
    "        # Input shape: (batch_size, 1, n_mels, time_steps)\n",
    "        # Esempio time_steps per audio di 5s: int(32000 * 5 / 512) + 1 = 313\n",
    "        \n",
    "        # Primo blocco convoluzionale\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Secondo blocco convoluzionale\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Terzo blocco convoluzionale\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Utilizziamo AdaptiveMaxPool2d per garantire una dimensione di output fissa\n",
    "        # indipendentemente dalle dimensioni di input\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((4, 4))  # Output fisso (4, 4)\n",
    "        flattened_size = 128 * 4 * 4  # 128 filtri × 4 × 4 output\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Aggiungiamo dropout per regolarizzazione\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        # Nessuna attivazione finale perché useremo BCEWithLogitsLoss\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass attraverso i layer convoluzionali\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.adaptive_pool(x)  # Garantisci dimensione fissa\n",
    "        \n",
    "        # Appiattisci e passa attraverso i layer fully connected\n",
    "        x = self.flatten(x) if hasattr(self, 'flatten') else x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu4(self.fc1(x)))\n",
    "        x = self.fc2(x)  # Output raw logits\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Inizializza il modello\n",
    "model = SimpleBirdCNN(num_classes=config.N_CLASSES).to(config.DEVICE)\n",
    "\n",
    "# Definisci loss e optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Adatto per classificazione multi-etichetta\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "print(f\"Modello caricato su {config.DEVICE}\")\n",
    "print(f\"Numero di classi: {config.N_CLASSES}\")\n",
    "print(f\"Riepilogo del modello:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84557c",
   "metadata": {},
   "source": [
    "## 7. Addestramento e Validazione del Modello\n",
    "\n",
    "Implementiamo il ciclo di addestramento con supporto per il caricamento di modelli pre-addestrati o l'addestramento da zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                model_save_path=None, model_load_path=None):\n",
    "    \"\"\"\n",
    "    Addestra il modello e valuta su validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da addestrare\n",
    "        train_loader: DataLoader per dati di training\n",
    "        val_loader: DataLoader per dati di validation\n",
    "        criterion: Funzione di loss\n",
    "        optimizer: Ottimizzatore\n",
    "        epochs: Numero di epoche di training\n",
    "        device: Device per l'addestramento ('cuda' o 'cpu')\n",
    "        model_save_path: Path dove salvare il modello addestrato\n",
    "        model_load_path: Path da cui caricare un modello pre-addestrato\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, total_training_time)\n",
    "    \"\"\"\n",
    "    # Flag che indica se serve fare training\n",
    "    needs_training = True\n",
    "    \n",
    "    # Controlla se il file modello esiste\n",
    "    if model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Modello pre-addestrato trovato in {model_load_path}. Caricamento...\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            print(\"Modello caricato con successo.\")\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            total_training_time = 0\n",
    "            needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del modello: {e}. Si procederà con il training.\")\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "    else:\n",
    "        print(\"Nessun modello pre-addestrato trovato. Si procederà con il training.\")\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Duration: {epoch_duration:.2f} sec\")\n",
    "        \n",
    "        end_time_total = time.time()\n",
    "        total_training_time = end_time_total - start_time_total\n",
    "        print(f\"\\nTraining terminato in {total_training_time/60:.2f} minuti\")\n",
    "        \n",
    "        # Salva il modello addestrato\n",
    "        if model_save_path:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Modello salvato in {model_save_path}\")\n",
    "    else:\n",
    "        print(\"Training saltato perché il modello è stato caricato.\")\n",
    "    \n",
    "    # Visualizza le curve di loss\n",
    "    if train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Curve di Loss di Training e Validation')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time\n",
    "\n",
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    model_load_path = os.path.join(config.MODELS_DIR, \"/kaggle/input/simplecnn/pytorch/default/1/baseline_bird_cnn_model_val.pth\")\n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, \"birdclef_trained_model.pth\")\n",
    "else:\n",
    "    # Per altri ambienti, usa i path locali\n",
    "    model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_model_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Addestra il modello\n",
    "train_losses, val_losses, training_time = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cdcb9f",
   "metadata": {},
   "source": [
    "## 8. Valutazione sul Dataset di Test\n",
    "\n",
    "Valutiamo le prestazioni del modello sul nostro dataset di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion=None, device=config.DEVICE):\n",
    "    \"\"\"\n",
    "    Valuta il modello sul dataset di test.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da valutare\n",
    "        test_loader: DataLoader per dati di test\n",
    "        criterion: Funzione di loss (opzionale)\n",
    "        device: Device per l'inferenza ('cuda' o 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Risultati della valutazione\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Valutazione\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if criterion:\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "            \n",
    "            # Applica sigmoid per ottenere probabilità\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Raccogli previsioni e target\n",
    "            all_predictions.append(probabilities.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatena batch\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calcola metriche\n",
    "    if criterion:\n",
    "        avg_loss = test_loss / len(test_loader)\n",
    "    else:\n",
    "        avg_loss = None\n",
    "    \n",
    "    # Calcola accuracy e altre metriche\n",
    "    predicted_labels = (all_predictions >= 0.5).astype(int)\n",
    "    accuracy = np.mean(predicted_labels == all_targets)\n",
    "    \n",
    "    # Si possono aggiungere altre metriche come precision, recall, F1, ecc.\n",
    "    \n",
    "    results = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Valuta il modello sul test set\n",
    "if test_loader is not None:\n",
    "    print(\"\\nValutazione sul test set creato dai dati di validazione:\")\n",
    "    test_results = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    print(f\"Loss: {test_results['loss']:.4f}\" if test_results['loss'] else \"Loss: N/A\")\n",
    "    print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    \n",
    "        # Visualizzazione delle curve ROC e altre metriche\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    # Calcola media delle curve ROC per tutte le classi\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    roc_auc_scores = []\n",
    "\n",
    "    for i in range(config.N_CLASSES):\n",
    "        fpr, tpr, _ = roc_curve(test_results['targets'][:, i], test_results['predictions'][:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        \n",
    "        # Visualizza solo alcune curve per evitare sovraffollamento\n",
    "        if i % 50 == 0:  # Mostra una curva ogni 50 classi\n",
    "            plt.plot(fpr, tpr, alpha=0.3, label=f'Classe {i} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Calcola e visualizza la curva ROC media\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    mean_auc = np.mean(roc_auc_scores)\n",
    "    plt.title(f'Curve ROC per classificazione multi-etichetta (AUC medio = {mean_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"AUC medio su tutte le classi: {mean_auc:.4f}\")\n",
    "\n",
    "    # Top 10 classi con maggior AUC\n",
    "    top_classes = sorted([(i, auc_score) for i, auc_score in enumerate(roc_auc_scores)], \n",
    "                        key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"\\nTop 10 classi per performance (AUC):\")\n",
    "    for idx, score in top_classes:\n",
    "        species_name = int_to_species.get(idx, f\"Classe {idx}\")\n",
    "        print(f\"{species_name}: {score:.4f}\")\n",
    "\n",
    "    # Bottom 10 classi con minor AUC\n",
    "    bottom_classes = sorted([(i, auc_score) for i, auc_score in enumerate(roc_auc_scores)], \n",
    "                        key=lambda x: x[1])[:10]\n",
    "    print(\"\\nBottom 10 classi per performance (AUC):\")\n",
    "    for idx, score in bottom_classes:\n",
    "        species_name = int_to_species.get(idx, f\"Classe {idx}\")\n",
    "        print(f\"{species_name}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSalto la valutazione sul test set creato da validation perché siamo su Kaggle.\")\n",
    "    print(\"Il modello sarà valutato direttamente sulla submission generata dai file test_soundscapes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.5 Test su Soundscapes in ambiente Kaggle\n",
    "\n",
    "# Questa cella viene eseguita solo se siamo su Kaggle\n",
    "if config.environment == 'kaggle':\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST SU SOUNDSCAPES IN AMBIENTE KAGGLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Verifica se la directory test_soundscapes esiste\n",
    "    if not os.path.exists(config.TEST_SOUNDSCAPES_DIR):\n",
    "        print(f\"La directory {config.TEST_SOUNDSCAPES_DIR} non esiste.\")\n",
    "    else:\n",
    "        # Elenca i file audio nella directory test\n",
    "        test_files = [f for f in os.listdir(config.TEST_SOUNDSCAPES_DIR) \n",
    "                     if f.endswith('.ogg') or f.endswith('.mp3') or f.endswith('.wav')]\n",
    "        print(f\"Trovati {len(test_files)} file audio nella directory test_soundscapes\")\n",
    "        \n",
    "        # Mostra i primi 5 file (se disponibili)\n",
    "        if test_files:\n",
    "            print(\"Primi file disponibili:\")\n",
    "            for i, file in enumerate(test_files[:5]):\n",
    "                print(f\"  - {file}\")\n",
    "            if len(test_files) > 5:\n",
    "                print(f\"  - ... e altri {len(test_files) - 5} file\")\n",
    "        \n",
    "        # Analisi di un campione casuale per verifica\n",
    "        if test_files:\n",
    "            import random\n",
    "            sample_file = random.choice(test_files)\n",
    "            sample_path = os.path.join(config.TEST_SOUNDSCAPES_DIR, sample_file)\n",
    "            print(f\"\\nAnalisi preliminare del file campione: {sample_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Carica e visualizza forma d'onda\n",
    "                y, sr = librosa.load(sample_path, sr=config.SR, duration=30)  # Carica solo i primi 30 secondi\n",
    "                duration = len(y) / sr\n",
    "                \n",
    "                print(f\"Durata: {duration:.2f} secondi\")\n",
    "                print(f\"Sample rate: {sr} Hz\")\n",
    "                \n",
    "                # Visualizza forma d'onda\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.plot(np.linspace(0, duration, len(y)), y)\n",
    "                plt.title(f'Forma d\\'onda di {sample_file}')\n",
    "                plt.xlabel('Tempo (secondi)')\n",
    "                plt.ylabel('Ampiezza')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Mostra spettrogramma\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "                librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "                plt.colorbar(format='%+2.0f dB')\n",
    "                plt.title(f'Spettrogramma di {sample_file}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Esegui predizione sul campione\n",
    "                print(\"\\nEsecuzione predizione sul file campione...\")\n",
    "                predictions = predict_soundscape(sample_path, model, window_duration=config.TEST_CLIP_DURATION)\n",
    "                \n",
    "                # Mostra le prime N predizioni (probabilità più alte)\n",
    "                if predictions:\n",
    "                    print(f\"\\nRisultati per {len(predictions)} segmenti audio:\")\n",
    "                    \n",
    "                    # Calcola la media delle probabilità su tutti i segmenti\n",
    "                    avg_probs = np.zeros(config.N_CLASSES)\n",
    "                    for pred in predictions:\n",
    "                        avg_probs += pred['probabilities']\n",
    "                    avg_probs /= len(predictions)\n",
    "                    \n",
    "                    # Mostra le 10 specie con maggiore probabilità media\n",
    "                    top_species_indices = np.argsort(avg_probs)[-10:][::-1]\n",
    "                    print(\"\\nTop 10 specie rilevate (media su tutti i segmenti):\")\n",
    "                    for i, idx in enumerate(top_species_indices):\n",
    "                        species_name = int_to_species[idx]\n",
    "                        print(f\"{i+1}. {species_name}: {avg_probs[idx]:.4f}\")\n",
    "                else:\n",
    "                    print(\"Nessuna predizione generata per il file campione.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nell'analisi del file campione: {e}\")\n",
    "        \n",
    "        print(\"\\nPronto per la generazione della submission completa...\")\n",
    "else:\n",
    "    print(\"\\nAmbiente non-Kaggle rilevato. Salta test su soundscapes.\")\n",
    "    print(\"Per testare su file soundscapes, imposta MANUAL_ENVIRONMENT = 'kaggle'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4f5c9",
   "metadata": {},
   "source": [
    "## 9. Generazione della Submission\n",
    "\n",
    "Elaborazione delle soundscape di test e generazione del file di submission nel formato richiesto dalla competizione BirdClef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_soundscape(audio_path, model, device=config.DEVICE, window_duration=config.TEST_CLIP_DURATION):\n",
    "    \"\"\"\n",
    "    Effettua predizioni su un file audio di soundscape.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Percorso del file audio\n",
    "        model: Modello PyTorch\n",
    "        device: Device per l'inferenza\n",
    "        window_duration: Durata finestra per l'analisi\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista di predizioni per il file\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Carica l'audio completo\n",
    "        y, sr = librosa.load(audio_path, sr=config.SR)\n",
    "        \n",
    "        # Ottieni il nome del file per l'identificazione\n",
    "        file_name = os.path.basename(audio_path)\n",
    "        \n",
    "        # Calcola la lunghezza della finestra e lo stride\n",
    "        window_length = int(window_duration * sr)\n",
    "        stride = window_length // 2  # 50% di overlap\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # Dividi l'audio in finestre e fai previsioni su ciascuna\n",
    "        for start in range(0, len(y) - window_length + 1, stride):\n",
    "            # Estrai segmento audio\n",
    "            segment = y[start:start + window_length]\n",
    "            \n",
    "            # Calcola spettrogramma Mel\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=segment, sr=sr,\n",
    "                n_fft=config.N_FFT,\n",
    "                hop_length=config.HOP_LENGTH,\n",
    "                n_mels=config.N_MELS,\n",
    "                fmin=config.FMIN,\n",
    "                fmax=config.FMAX\n",
    "            )\n",
    "            \n",
    "            # Converti in scala logaritmica (dB)\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Normalizza\n",
    "            min_val = np.min(log_mel_spec)\n",
    "            max_val = np.max(log_mel_spec)\n",
    "            if max_val > min_val:\n",
    "                log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                log_mel_spec = np.zeros_like(log_mel_spec)\n",
    "            \n",
    "            # Aggiungi dimensione batch e canale\n",
    "            log_mel_spec = np.expand_dims(np.expand_dims(log_mel_spec, axis=0), axis=0)\n",
    "            \n",
    "            # Converti in tensor\n",
    "            input_tensor = torch.tensor(log_mel_spec, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Effettua predizione\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                probabilities = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Calcola row_id per il formato di submission\n",
    "            end_seconds = (start + window_length) / sr\n",
    "            row_id = f\"{file_name[:-4]}_{int(end_seconds)}\"\n",
    "            \n",
    "            # Aggiungi predizione\n",
    "            predictions.append({\n",
    "                'row_id': row_id,\n",
    "                'probabilities': probabilities,\n",
    "                'end_time': end_seconds\n",
    "            })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore nell'elaborazione di {audio_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_submission(model, test_dir=config.TEST_SOUNDSCAPES_DIR, output_path=None):\n",
    "    \"\"\"\n",
    "    Genera un file di submission per la competizione BirdClef.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch\n",
    "        test_dir: Directory contenente i file audio di test\n",
    "        output_path: Percorso dove salvare il file di submission\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame di submission\n",
    "    \"\"\"\n",
    "    if not output_path:\n",
    "        output_path = os.path.join(config.OUTPUT_DIR, \"submission.csv\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # Verifica se la directory di test esiste e contiene file\n",
    "    if os.path.exists(test_dir) and len(os.listdir(test_dir)) > 0:\n",
    "        print(f\"Directory di test trovata: {test_dir}\")\n",
    "        \n",
    "        # Ottieni lista di file audio\n",
    "        test_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) \n",
    "                     if f.endswith('.ogg') or f.endswith('.mp3') or f.endswith('.wav')]\n",
    "        print(f\"Numero di file audio di test trovati: {len(test_files)}\")\n",
    "        \n",
    "        # Processa ogni file\n",
    "        for test_file in tqdm(test_files, desc=\"Elaborazione soundscapes di test\"):\n",
    "            file_predictions = predict_soundscape(test_file, model)\n",
    "            all_predictions.extend(file_predictions)\n",
    "        \n",
    "        # Converti previsioni in DataFrame con formato richiesto\n",
    "        submission_df = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "        temp_data = []\n",
    "        \n",
    "        for pred in all_predictions:\n",
    "            row_data = {'row_id': pred['row_id']}\n",
    "            for i, species in enumerate(all_species):\n",
    "                row_data[species] = pred['probabilities'][i]\n",
    "            temp_data.append(row_data)\n",
    "        \n",
    "        if temp_data:\n",
    "            submission_df = pd.DataFrame(temp_data)\n",
    "            # Assicura che le colonne siano nell'ordine corretto\n",
    "            submission_df = submission_df[['row_id'] + all_species]\n",
    "        else:\n",
    "            print(\"Nessuna predizione è stata generata.\")\n",
    "            # Crea df vuoto con colonne corrette\n",
    "            submission_df = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "        \n",
    "        # Salva file di submission\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"File di submission salvato in: {output_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Directory di test non trovata o vuota: {test_dir}\")\n",
    "        print(\"Creazione di una submission fittizia basata sul formato richiesto.\")\n",
    "        \n",
    "        # Crea una submission fittizia basata sul formato del sample submission\n",
    "        if os.path.exists(config.SAMPLE_SUB_PATH):\n",
    "            sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "            dummy_sub = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "            dummy_sub['row_id'] = sample_sub_df['row_id']\n",
    "            \n",
    "            # Riempi con valori di probabilità di default\n",
    "            default_prob = 1.0 / config.N_CLASSES if config.N_CLASSES > 0 else 0\n",
    "            for species in all_species:\n",
    "                dummy_sub[species] = default_prob\n",
    "            \n",
    "            dummy_sub.to_csv(output_path, index=False)\n",
    "            print(f\"Submission fittizia salvata in: {output_path}\")\n",
    "            return dummy_sub\n",
    "        else:\n",
    "            print(f\"Sample submission non trovata in: {config.SAMPLE_SUB_PATH}\")\n",
    "            return None\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Genera submission solo se siamo su Kaggle o se la directory di test esiste\n",
    "submission_path = os.path.join(output_dirs['submissions'], f\"submission_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "\n",
    "if config.environment == 'kaggle' or (os.path.exists(config.TEST_SOUNDSCAPES_DIR) and len(os.listdir(config.TEST_SOUNDSCAPES_DIR)) > 0):\n",
    "    print(\"\\nGenerazione del file di submission...\")\n",
    "    submission_df = generate_submission(model, output_path=submission_path)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\nAnteprima del file di submission:\")\n",
    "        print(submission_df.head())\n",
    "else:\n",
    "    print(\"\\nSalto la generazione della submission perché non siamo su Kaggle e non ci sono file di test disponibili.\")\n",
    "    print(f\"Per generare una submission, posiziona i file audio di test in: {config.TEST_SOUNDSCAPES_DIR}\")\n",
    "\n",
    "print(\"\\nEsecuzione completata!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
