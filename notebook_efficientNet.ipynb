{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con CNN\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza un'architettura CNN per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importazione delle Librerie Necessarie\n",
    "\n",
    "Importiamo tutte le librerie necessarie per l'elaborazione audio, deep learning e visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:39:56.650248Z",
     "iopub.status.busy": "2025-05-14T17:39:56.649850Z",
     "iopub.status.idle": "2025-05-14T17:40:06.789170Z",
     "shell.execute_reply": "2025-05-14T17:40:06.787940Z",
     "shell.execute_reply.started": "2025-05-14T17:39:56.650210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# Sostituisci le importazioni di Transformers con timm\n",
    "import timm\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"timm versione: {timm.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Imposta questo a True per abilitare la cancellazione\n",
    "clear_working_dir = True\n",
    "\n",
    "working_dir = '/kaggle/working/'\n",
    "\n",
    "if clear_working_dir:\n",
    "    for filename in os.listdir(working_dir):\n",
    "        file_path = os.path.join(working_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # elimina file o link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # elimina directory\n",
    "        except Exception as e:\n",
    "            print(f'Errore durante la rimozione di {file_path}: {e}')\n",
    "    print(f\"Tutti i file in {working_dir} sono stati rimossi.\")\n",
    "else:\n",
    "    print(\"Pulizia disabilitata (clear_working_dir = False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurazione dell'Ambiente di Esecuzione\n",
    "\n",
    "In questa sezione configuriamo l'ambiente di esecuzione in modo che il notebook funzioni sia su Kaggle, che su Google Colab, che in locale. Il codice rileverà automaticamente l'ambiente e configurerà i percorsi di conseguenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.791974Z",
     "iopub.status.busy": "2025-05-14T17:40:06.791352Z",
     "iopub.status.idle": "2025-05-14T17:40:06.801870Z",
     "shell.execute_reply": "2025-05-14T17:40:06.800513Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.791943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.803494Z",
     "iopub.status.busy": "2025-05-14T17:40:06.803146Z",
     "iopub.status.idle": "2025-05-14T17:40:06.830225Z",
     "shell.execute_reply": "2025-05-14T17:40:06.828841Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.803460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            # quindi non impostiamo ancora i percorsi derivati\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio - già allineati con vincitori\n",
    "        self.SR = 32000      # Sample rate\n",
    "        self.DURATION = 5    # Durata dei clip in secondi\n",
    "        self.N_MELS = 128    # Numero di bande Mel\n",
    "        self.N_FFT = 1024    # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 500  # Hop length per STFT\n",
    "        self.FMIN = 40       # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = 15000    # Frequenza massima\n",
    "        self.POWER = 2       # Esponente per calcolo spettrogramma\n",
    "            \n",
    "        # Parametri per il training - aggiornati secondo i vincitori\n",
    "        self.BATCH_SIZE = 96  # Aumentato da 32 a 96 come dai vincitori\n",
    "        self.EPOCHS = 10     # Numero di epoche per il training\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 4  # Aumentato per migliorare il data loading\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.832040Z",
     "iopub.status.busy": "2025-05-14T17:40:06.831609Z",
     "iopub.status.idle": "2025-05-14T17:40:06.865978Z",
     "shell.execute_reply": "2025-05-14T17:40:06.864801Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.832012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/simplecnn/PyTorch/default/1\"\n",
    "    cache_model_file = os.path.join(cache_model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(cache_model_file)\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/simplecnn/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    model_file = os.path.join(model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {model_file}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurazione del Modello e Parametri\n",
    "\n",
    "Definiamo i parametri di configurazione per il preprocessamento audio, la creazione dello spettrogramma Mel e l'addestramento della CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.869228Z",
     "iopub.status.busy": "2025-05-14T17:40:06.868890Z",
     "iopub.status.idle": "2025-05-14T17:40:06.900017Z",
     "shell.execute_reply": "2025-05-14T17:40:06.898591Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.869203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I parametri principali sono già definiti nella classe Config\n",
    "# Verifichiamo l'esistenza delle directory e creiamo quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caricamento e Preprocessing dei Dati\n",
    "\n",
    "In questa sezione carichiamo i metadati dal file CSV di training, creiamo codifiche one-hot per le etichette delle specie e implementiamo funzioni per il caricamento e preprocessamento dei file audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.901809Z",
     "iopub.status.busy": "2025-05-14T17:40:06.901297Z",
     "iopub.status.idle": "2025-05-14T17:40:08.193126Z",
     "shell.execute_reply": "2025-05-14T17:40:08.192038Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.901725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "    \n",
    "# Per Kaggle, dovremo creare un dataset speciale per le soundscapes di test\n",
    "# Questo verrà utilizzato direttamente nella fase di generazione della submission\n",
    "# Non creiamo X_test_df e test_dataset per ora\n",
    "X_test_df = None\n",
    "y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione di bilanciamento del dataset - Cancella una percentuale di esempi dalle classi molto numerose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset_df(train_df, labels_one_hot, abundant_class_threshold=200, remove_percentage=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame bilanciato rimuovendo parte degli esempi con rating bassi dalle classi abbondanti.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame originale\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        abundant_class_threshold: Soglia per definire una classe come \"abbondante\"\n",
    "        remove_percentage: Percentuale di esempi con rating 1-3 da rimuovere dalle classi abbondanti\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame bilanciato, etichette one-hot bilanciate)\n",
    "    \"\"\"\n",
    "    # Conta esempi per ogni classe\n",
    "    class_counts = train_df['primary_label'].value_counts()\n",
    "    \n",
    "    # Identifica classi abbondanti\n",
    "    abundant_classes = class_counts[class_counts > abundant_class_threshold].index.tolist()\n",
    "    print(f\"Classi identificate come abbondanti (>{abundant_class_threshold} esempi): {len(abundant_classes)}\")\n",
    "    \n",
    "    # Copia il DataFrame originale\n",
    "    balanced_df = train_df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Contatori per statistiche\n",
    "    total_removed = 0\n",
    "    removed_by_class = {}\n",
    "    \n",
    "    # Per ogni classe abbondante\n",
    "    for cls in abundant_classes:\n",
    "        # Filtra esempi con rating 1-3 per questa classe\n",
    "        low_quality_mask = (balanced_df['primary_label'] == cls) & (balanced_df['rating'].isin([1, 2, 3]))\n",
    "        low_quality_indices = balanced_df[low_quality_mask].index.tolist()\n",
    "        \n",
    "        # Numero di esempi da rimuovere\n",
    "        n_to_remove = int(len(low_quality_indices) * remove_percentage)\n",
    "        \n",
    "        # Seleziona casualmente gli indici da rimuovere\n",
    "        np.random.seed(random_state)\n",
    "        if n_to_remove > 0:\n",
    "            indices_to_remove = np.random.choice(low_quality_indices, size=n_to_remove, replace=False)\n",
    "            \n",
    "            # Memorizza gli indici da rimuovere\n",
    "            rows_to_drop.extend(indices_to_remove)\n",
    "            \n",
    "            # Aggiorna statistiche\n",
    "            removed_by_class[cls] = n_to_remove\n",
    "            total_removed += n_to_remove\n",
    "    \n",
    "    # Rimuovi le righe selezionate\n",
    "    if rows_to_drop:\n",
    "        balanced_df = balanced_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Aggiorna anche le etichette one-hot rimuovendo gli stessi indici\n",
    "        mask = np.ones(len(train_df), dtype=bool)\n",
    "        mask[rows_to_drop] = False\n",
    "        balanced_labels = labels_one_hot[mask]\n",
    "    else:\n",
    "        balanced_labels = labels_one_hot\n",
    "    \n",
    "    # Statistiche finali\n",
    "    print(f\"Totale esempi rimossi: {total_removed} ({total_removed/len(train_df):.1%} del dataset originale)\")\n",
    "    print(f\"Dimensione dataset originale: {len(train_df)}\")\n",
    "    print(f\"Dimensione dataset bilanciato: {len(balanced_df)}\")\n",
    "    \n",
    "    # Visualizza le prime 5 classi con maggiori rimozioni\n",
    "    if removed_by_class:\n",
    "        top_removed = sorted(removed_by_class.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nClassi con maggior numero di esempi rimossi:\")\n",
    "        for cls, count in top_removed:\n",
    "            original = class_counts[cls]\n",
    "            remaining = original - count\n",
    "            print(f\"- {cls}: {count} rimossi, {remaining}/{original} rimanenti ({remaining/original:.1%})\")\n",
    "    else:\n",
    "        print(\"Nessun esempio rimosso.\")\n",
    "    \n",
    "    return balanced_df, balanced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Analisi Esplorativa dei Dati (EDA)\n",
    "\n",
    "In questa sezione esploreremo le caratteristiche del dataset per comprendere meglio la distribuzione delle specie, le proprietà audio e identificare eventuali pattern nei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:08.194532Z",
     "iopub.status.busy": "2025-05-14T17:40:08.194196Z",
     "iopub.status.idle": "2025-05-14T17:40:32.789695Z",
     "shell.execute_reply": "2025-05-14T17:40:32.788430Z",
     "shell.execute_reply.started": "2025-05-14T17:40:08.194504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configurazione stile visualizzazioni\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"=== Statistiche di base del dataset ===\")\n",
    "print(f\"Numero totale di registrazioni: {len(train_df)}\")\n",
    "print(f\"Numero di specie uniche nel dataset: {len(all_species)}\")\n",
    "print(f\"Campi disponibili nei metadati: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Verifichiamo i dati mancanti\n",
    "missing_data = train_df.isnull().sum()\n",
    "print(\"\\n=== Valori mancanti ===\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# 1. Distribuzione delle specie nel dataset (visualizzazione migliorata)\n",
    "print(\"\\n=== Analisi delle Specie ===\")\n",
    "primary_species_count = train_df['primary_label'].value_counts()\n",
    "\n",
    "# Plot combinato: distribuzione delle specie con evidenza delle classi rare\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# A sinistra: top 20 specie più rappresentate\n",
    "sns.barplot(x=primary_species_count.head(20).index, y=primary_species_count.head(20).values, ax=ax1)\n",
    "ax1.set_title('Top 20 Specie per Numero di Registrazioni')\n",
    "ax1.set_xlabel('Specie')\n",
    "ax1.set_ylabel('Numero di Registrazioni')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# A destra: distribuzione del numero di esempi per specie\n",
    "sns.histplot(primary_species_count, bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribuzione del Numero di Registrazioni per Specie')\n",
    "ax2.set_xlabel('Numero di Registrazioni')\n",
    "ax2.set_ylabel('Conteggio Specie')\n",
    "ax2.axvline(x=primary_species_count.median(), color='r', linestyle='--', \n",
    "            label=f'Mediana: {primary_species_count.median()}')\n",
    "ax2.axvline(x=primary_species_count.mean(), color='g', linestyle='--', \n",
    "            label=f'Media: {primary_species_count.mean():.1f}')\n",
    "ax2.axvline(x=50, color='orange', linestyle=':', label='Soglia classi rare (50)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcolo dell'indice di Gini per misurare lo sbilanciamento\n",
    "def gini_coefficient(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n+1)\n",
    "    return (np.sum((2*index - n - 1) * x)) / (n * np.sum(x))\n",
    "\n",
    "gini = gini_coefficient(primary_species_count.values)\n",
    "print(f\"\\nIndice di Gini per la distribuzione delle specie: {gini:.4f}\")\n",
    "print(f\"Questo indica {'un alto' if gini > 0.6 else 'un moderato' if gini > 0.3 else 'un basso'} livello di sbilanciamento nel dataset.\")\n",
    "\n",
    "# 2. NUOVA ANALISI: Rating per le classi con pochi esempi\n",
    "# Definisco la soglia per le classi rare (< 50 esempi)\n",
    "RARE_CLASS_THRESHOLD = 50\n",
    "rare_species = primary_species_count[primary_species_count < RARE_CLASS_THRESHOLD].index.tolist()\n",
    "print(f\"\\n=== Analisi dei Rating per Classi Rare (<{RARE_CLASS_THRESHOLD} esempi) ===\")\n",
    "print(f\"Numero di classi rare: {len(rare_species)} su {len(all_species)} totali ({len(rare_species)/len(all_species):.1%})\")\n",
    "\n",
    "# Raccolgo i dati sui rating per le classi rare\n",
    "rare_class_ratings = []\n",
    "for species in rare_species:\n",
    "    species_df = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_df['rating'].fillna(0).tolist()  # Sostituisco NaN con 0 (nessun rating)\n",
    "    \n",
    "    # Statistiche per questa specie\n",
    "    rare_class_ratings.append({\n",
    "        'species': species,\n",
    "        'count': len(species_df),\n",
    "        'avg_rating': np.mean(ratings),\n",
    "        'ratings': ratings,\n",
    "        'rating_counts': {r: ratings.count(r) for r in set(ratings)}\n",
    "    })\n",
    "\n",
    "# Creo DataFrame per analisi\n",
    "rare_ratings_df = pd.DataFrame(rare_class_ratings)\n",
    "rare_ratings_df = rare_ratings_df.sort_values('count')\n",
    "\n",
    "# Visualizzazione: Rating medi vs Conteggio per le classi rare\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Heatmap: distribuzione dei rating per classi rare\n",
    "n_rare_to_show = min(30, len(rare_ratings_df))  # Mostra max 30 classi per leggibilità\n",
    "rare_sample = rare_ratings_df.head(n_rare_to_show)\n",
    "\n",
    "# Preparo i dati per la heatmap\n",
    "heatmap_data = []\n",
    "rating_values = [0, 1, 2, 3, 4, 5]  # Tutti i possibili rating\n",
    "for _, row in rare_sample.iterrows():\n",
    "    species_data = [row['species'], row['count']]\n",
    "    for rating in rating_values:\n",
    "        species_data.append(row['rating_counts'].get(rating, 0))\n",
    "    heatmap_data.append(species_data)\n",
    "\n",
    "# Creo DataFrame per la heatmap\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data, \n",
    "    columns=['species', 'count'] + [f'rating_{r}' for r in rating_values]\n",
    ")\n",
    "\n",
    "# Plot combinato con scatter plot e heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Scatter plot: rating medio vs numero esempi\n",
    "sns.scatterplot(\n",
    "    x='count', \n",
    "    y='avg_rating', \n",
    "    data=rare_ratings_df, \n",
    "    ax=ax1, \n",
    "    alpha=0.7,\n",
    "    hue='count',\n",
    "    palette='viridis',\n",
    "    size='count',\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "ax1.set_title('Rating Medio vs Numero di Esempi per Classi Rare')\n",
    "ax1.set_xlabel('Numero di Esempi')\n",
    "ax1.set_ylabel('Rating Medio')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mostra statistiche\n",
    "avg_rating_rare = rare_ratings_df['avg_rating'].mean()\n",
    "ax1.axhline(y=avg_rating_rare, color='r', linestyle='--', \n",
    "           label=f'Rating medio classi rare: {avg_rating_rare:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Heatmap: distribuzione dei rating per le classi più rare\n",
    "pivot_data = pd.DataFrame({\n",
    "    'species': heatmap_df['species'],\n",
    "    'Rating 0': heatmap_df['rating_0'],\n",
    "    'Rating 1': heatmap_df['rating_1'],\n",
    "    'Rating 2': heatmap_df['rating_2'],\n",
    "    'Rating 3': heatmap_df['rating_3'],\n",
    "    'Rating 4': heatmap_df['rating_4'],\n",
    "    'Rating 5': heatmap_df['rating_5'],\n",
    "}).set_index('species')\n",
    "\n",
    "sns.heatmap(pivot_data, cmap=\"YlGnBu\", annot=True, fmt='g', ax=ax2)\n",
    "ax2.set_title(f'Distribuzione dei Rating nelle {n_rare_to_show} Classi più Rare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche aggregate sui rating per le classi rare\n",
    "print(\"\\nStatistiche sui rating per le classi rare:\")\n",
    "print(f\"- Rating medio complessivo: {rare_ratings_df['avg_rating'].mean():.2f}\")\n",
    "print(f\"- Percentuale di registrazioni senza rating (0): {sum(r['rating_counts'].get(0, 0) for r in rare_class_ratings) / sum(r['count'] for r in rare_class_ratings):.1%}\")\n",
    "\n",
    "# Analisi delle classi estremamente rare (≤ 5 esempi)\n",
    "very_rare_species = primary_species_count[primary_species_count <= 5].index.tolist()\n",
    "print(f\"\\nClassi estremamente rare (≤ 5 esempi): {len(very_rare_species)}\")\n",
    "\n",
    "very_rare_df = train_df[train_df['primary_label'].isin(very_rare_species)]\n",
    "print(\"Dettaglio delle registrazioni per le classi estremamente rare:\")\n",
    "for species in very_rare_species:\n",
    "    species_data = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_data['rating'].fillna(0).tolist()\n",
    "    print(f\"- {species}: {len(species_data)} esempi, ratings: {ratings}\")\n",
    "\n",
    "\n",
    "# Identificazione delle classi con solo rating molto bassi (1-2)\n",
    "print(\"\\n=== Analisi delle Classi con SOLO Rating Molto Bassi (1-2) ===\")\n",
    "\n",
    "# Funzione per verificare se una specie ha esclusivamente rating tra 1-2\n",
    "def has_only_low_ratings(species_ratings):\n",
    "    valid_ratings = [r for r in species_ratings if pd.notna(r) and r != 0]  # Escludi NaN e rating=0\n",
    "    if not valid_ratings:  # Se non ci sono rating validi\n",
    "        return False\n",
    "    return all(1 <= r <= 2 for r in valid_ratings)  # Modificato: ora solo 1-2\n",
    "\n",
    "# Raggruppa per specie e analizza\n",
    "low_rating_species = []\n",
    "for species, group in train_df.groupby('primary_label'):\n",
    "    ratings = group['rating'].tolist()\n",
    "    if has_only_low_ratings(ratings):\n",
    "        low_rating_species.append({\n",
    "            'species': species,\n",
    "            'count': len(ratings),\n",
    "            'avg_rating': np.nanmean([r for r in ratings if pd.notna(r) and r != 0]),\n",
    "            'ratings': sorted([r for r in ratings if pd.notna(r) and r != 0])\n",
    "        })\n",
    "\n",
    "# Crea DataFrame e visualizza risultati\n",
    "if low_rating_species:\n",
    "    low_rating_df = pd.DataFrame(low_rating_species).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Trovate {len(low_rating_species)} classi con SOLO rating molto bassi (1-2):\")\n",
    "    \n",
    "    # Visualizzazione\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(low_rating_df['species'], low_rating_df['avg_rating'], alpha=0.7, color='tomato')\n",
    "    plt.axhline(y=1.5, color='r', linestyle='--', label='Media teorica = 1.5')\n",
    "    plt.title('Classi con Esclusivamente Rating Molto Bassi (1-2)')\n",
    "    plt.xlabel('Specie')\n",
    "    plt.ylabel('Rating Medio')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostra dettagli delle prime 10 classi\n",
    "    print(\"\\nDettagli delle prime 10 classi con solo rating molto bassi:\")\n",
    "    for i, row in low_rating_df.head(10).iterrows():\n",
    "        print(f\"- {row['species']}: {row['count']} clip, rating medio: {row['avg_rating']:.2f}, ratings: {row['ratings']}\")\n",
    "    \n",
    "    # Cerca sovrapposizione con classi rare\n",
    "    overlap = [s for s in low_rating_df['species'] if s in rare_species]\n",
    "    print(f\"\\nSovrapposizione con classi rare (<{RARE_CLASS_THRESHOLD} esempi): {len(overlap)} classi\")\n",
    "    if overlap:\n",
    "        print(f\"Le classi rare che hanno solo rating molto bassi: {overlap[:10]}{'...' if len(overlap) > 10 else ''}\")\n",
    "else:\n",
    "    print(\"Nessuna classe ha esclusivamente rating da 1 a 2.\")\n",
    "\n",
    "print(\"\\n=== Conclusioni dall'Analisi delle Classi Rare ===\")\n",
    "print(f\"1. Abbiamo {len(rare_species)} classi rare (<{RARE_CLASS_THRESHOLD} esempi)\")\n",
    "print(f\"2. Di queste, {len(very_rare_species)} hanno 5 o meno esempi\")\n",
    "print(\"3. La qualità delle registrazioni (rating) è un fattore critico per le classi rare\")\n",
    "print(\"4. Le classi estremamente rare richiedono tecniche speciali (data augmentation, few-shot learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Implementazione delle Tecniche dei Vincitori\n",
    "\n",
    "In questa sezione implementiamo le tre tecniche di data augmentation che hanno contribuito significativamente alle performance dei vincitori:\n",
    "1. **Random Segment Selection** - Estrae segmenti casuali dalle registrazioni audio\n",
    "2. **XY Masking** - Applica maschere casuali sugli assi tempo e frequenza degli spettrogrammi Mel\n",
    "3. **Horizontal CutMix** - Combina parti di spettrogrammi da diverse registrazioni\n",
    "\n",
    "La classe `AudioAugmentations` gestisce tutte queste trasformazioni in modo unificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentations:\n",
    "    def __init__(self, p_random_segment=0.5, p_xy_mask=0.5, p_horizontal_cutmix=0.25):\n",
    "        \"\"\"\n",
    "        Inizializza le trasformazioni per data augmentation audio.\n",
    "        \n",
    "        Args:\n",
    "            p_random_segment: Probabilità di utilizzare un segmento casuale\n",
    "            p_xy_mask: Probabilità di applicare il mascheramento XY\n",
    "            p_horizontal_cutmix: Probabilità di applicare horizontal cutmix\n",
    "        \"\"\"\n",
    "        self.p_random_segment = p_random_segment\n",
    "        self.p_xy_mask = p_xy_mask\n",
    "        self.p_horizontal_cutmix = p_horizontal_cutmix\n",
    "    \n",
    "    def apply_xy_masking(self, spec):\n",
    "        \"\"\"Applica maschere casuali sull'asse X (tempo) e Y (frequenza) allo spettrogramma\"\"\"\n",
    "        mask = spec.clone()\n",
    "        height, width = mask.shape[-2], mask.shape[-1]\n",
    "        \n",
    "        # Masking temporale (asse X)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_width = int(width * np.random.uniform(0.1, 0.2))  # 10-20% width\n",
    "            mask_start = np.random.randint(0, width - mask_width)\n",
    "            mask[..., mask_start:mask_start+mask_width] = 0\n",
    "            \n",
    "        # Masking frequenziale (asse Y)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_height = int(height * np.random.uniform(0.1, 0.2))  # 10-20% height\n",
    "            mask_start = np.random.randint(0, height - mask_height)\n",
    "            mask[:, :, mask_start:mask_start+mask_height, :] = 0\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal CutMix: Implementazione della Funzione di Collate\n",
    "\n",
    "Questa funzione personalizzata viene utilizzata nel DataLoader per implementare l'Horizontal CutMix,\n",
    "che combina sezioni temporali di spettrogrammi diversi all'interno dello stesso batch.\n",
    "Le etichette vengono miscelate proporzionalmente alla quantità di dati combinati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_cutmix_collate(batch, p_cutmix=0.25):\n",
    "    \"\"\"\n",
    "    Collate function che applica horizontal cutmix tra elementi del batch con probabilità p_cutmix.\n",
    "    \n",
    "    Args:\n",
    "        batch: Lista di tuple (input, target)\n",
    "        p_cutmix: Probabilità di applicare cutmix ad ogni coppia di esempi\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (inputs_batch, targets_batch)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Estrae input e target dal batch\n",
    "    for input_tensor, target_tensor in batch:\n",
    "        inputs.append(input_tensor)\n",
    "        targets.append(target_tensor)\n",
    "    \n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    # Applica horizontal cutmix\n",
    "    if batch_size > 1:  # Serve almeno 2 elementi per fare cutmix\n",
    "        for i in range(batch_size):\n",
    "            if np.random.rand() < p_cutmix:\n",
    "                # Seleziona un altro esempio casuale da mixare\n",
    "                j = np.random.randint(0, batch_size)\n",
    "                if i != j:\n",
    "                    # Determina il punto di taglio orizzontale (sulla dimensione temporale)\n",
    "                    width = inputs.shape[3]\n",
    "                    cut_point = np.random.randint(int(width * 0.25), int(width * 0.75))\n",
    "                    \n",
    "                    # Esegue il mixing\n",
    "                    mix_ratio = cut_point / width\n",
    "                    inputs[i, :, :, :cut_point] = inputs[j, :, :, :cut_point]\n",
    "                    \n",
    "                    # Mix delle etichette in proporzione al mix degli input\n",
    "                    targets[i] = targets[i] * (1 - mix_ratio) + targets[j] * mix_ratio\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento e pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Audio con Supporto per Data Augmentation\n",
    "\n",
    "La funzione di caricamento audio è stata modificata per supportare due modalità di estrazione:\n",
    "- **Posizionale** - Estrazione da punti specifici nella registrazione ('start', 'center', 'end')\n",
    "- **Casuale** - Estrazione di un segmento casuale quando `random_segment=True`\n",
    "\n",
    "Questa implementazione permette di applicare la tecnica di Random Segment Selection come parte della data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:32.790961Z",
     "iopub.status.busy": "2025-05-14T17:40:32.790445Z",
     "iopub.status.idle": "2025-05-14T17:40:32.803627Z",
     "shell.execute_reply": "2025-05-14T17:40:32.802015Z",
     "shell.execute_reply.started": "2025-05-14T17:40:32.790937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, target_sr=config.SR, duration=config.DURATION, segment_position='center', random_segment=False):\n",
    "    \"\"\"\n",
    "    Carica un file audio, estrae un segmento specifico e lo converte in spettrogramma Mel.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Percorso del file audio\n",
    "        target_sr: Sample rate target\n",
    "        duration: Durata target in secondi\n",
    "        segment_position: Posizione del segmento ('start', 'center', 'end')\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Spettrogramma Mel log-normalizzato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carica il file audio\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        # Lunghezza target in campioni\n",
    "        target_len = int(target_sr * duration)\n",
    "        total_len = len(y)\n",
    "        \n",
    "        # Gestisci clip troppo corte\n",
    "        if total_len < target_len:\n",
    "            import math\n",
    "            n_copy = math.ceil(target_len / total_len)\n",
    "            if n_copy > 1:\n",
    "                y = np.tile(y, n_copy)\n",
    "            total_len = len(y)\n",
    "        \n",
    "        # Seleziona il segmento\n",
    "        if random_segment and total_len > target_len:\n",
    "            # Estrai segmento casuale\n",
    "            max_start_idx = total_len - target_len\n",
    "            start_idx = np.random.randint(0, max_start_idx)\n",
    "        else:\n",
    "            # Usa le posizioni predefinite\n",
    "            if segment_position == 'start':\n",
    "                start_idx = int(total_len * 0.2)\n",
    "                if start_idx + target_len > total_len:\n",
    "                    start_idx = max(0, total_len - target_len)\n",
    "            elif segment_position == 'end':\n",
    "                end_point = int(total_len * 0.8)\n",
    "                start_idx = max(0, end_point - target_len)\n",
    "            else:  # 'center' (default)\n",
    "                start_idx = max(0, int(total_len / 2 - target_len / 2))\n",
    "        \n",
    "        # Estrai il segmento\n",
    "        y = y[start_idx:start_idx + target_len]\n",
    "        \n",
    "        # Padda se necessario\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)), mode='constant')\n",
    "        \n",
    "        # Calcola lo spettrogramma Mel\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr,\n",
    "            n_fft=config.N_FFT,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            n_mels=config.N_MELS,\n",
    "            fmin=config.FMIN,\n",
    "            fmax=config.FMAX,\n",
    "            power=config.POWER\n",
    "        )\n",
    "        \n",
    "        # Converti in scala logaritmica (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalizza\n",
    "        min_val = np.min(log_mel_spec)\n",
    "        max_val = np.max(log_mel_spec)\n",
    "        if max_val > min_val:\n",
    "            log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            log_mel_spec = np.zeros_like(log_mel_spec)\n",
    "        \n",
    "        return log_mel_spec\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "        time_steps = int(target_sr * duration / config.HOP_LENGTH) + 1\n",
    "        return np.zeros((config.N_MELS, time_steps), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch per Dati Audio\n",
    "\n",
    "## Dataset PyTorch con Supporto Integrato per Data Augmentation\n",
    "\n",
    "Implementiamo due classi di dataset:\n",
    "- `BirdDataset` - Dataset standard che estrae il segmento centrale delle clip audio\n",
    "- `AdaptiveMultiSegmentBirdDataset` - Dataset avanzato che:\n",
    "  - Estrae automaticamente diversi segmenti in base alla durata delle registrazioni\n",
    "  - Applica le tecniche di data augmentation durante il caricamento\n",
    "  - Supporta l'estrazione di segmenti adattivi (1-3 per clip in base alla lunghezza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset che estrae solo il segmento centrale per ogni clip audio.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Attenzione: File non trovato in {file_path}.\")\n",
    "            time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "            dummy_spec = torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)\n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return dummy_spec, dummy_label\n",
    "            \n",
    "        # Carica e preprocessa l'audio con solo segmento centrale\n",
    "        mel_spec = load_and_preprocess_audio(file_path, segment_position='center')\n",
    "        \n",
    "        # Aggiungi dimensione del canale e converti in tensor\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "        \n",
    "        # Ottieni le etichette\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            mel_spec_tensor = self.transform(mel_spec_tensor)\n",
    "            \n",
    "        return mel_spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BirdDataset con l'utilizzo di adaptive clip in base alla lunghezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveMultiSegmentBirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, transform=None, augmentations=None):\n",
    "        \"\"\"\n",
    "        Dataset che estrae un numero appropriato di segmenti in base alla lunghezza di ogni clip.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con i metadati\n",
    "            audio_dir: Directory contenente i file audio\n",
    "            labels_one_hot: Array di etichette one-hot\n",
    "            transform: Trasformazioni da applicare (opzionale)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "        # Pre-calcola quali segmenti usare per ogni clip\n",
    "        self.segments_to_use = []\n",
    "        print(\"Analizzando le lunghezze delle clip audio...\")\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparazione dataset adattivo\"):\n",
    "            file_path = os.path.join(audio_dir, row['filename'])\n",
    "            try:\n",
    "                # Carica solo l'informazione sulla durata senza caricare l'intero file\n",
    "                y, sr = librosa.load(file_path, sr=None, duration=0.1)  # Carica solo un breve segmento per ottenere SR\n",
    "                info = librosa.get_duration(filename=file_path, sr=sr)\n",
    "                total_duration = info  # Durata in secondi\n",
    "                \n",
    "                # Determina quali segmenti usare in base alla durata\n",
    "                if total_duration < config.DURATION * 1.5:\n",
    "                    # Clip troppo corta per multiple segmenti, usa solo il centro\n",
    "                    segments = ['center']\n",
    "                elif total_duration < config.DURATION * 2.5:\n",
    "                    # Clip media, usa inizio e fine\n",
    "                    segments = ['start', 'end']\n",
    "                else:\n",
    "                    # Clip abbastanza lunga, usa tutti e tre i segmenti\n",
    "                    segments = ['start', 'center', 'end']\n",
    "                \n",
    "                # Memorizza l'indice originale e i segmenti da utilizzare\n",
    "                for segment in segments:\n",
    "                    self.segments_to_use.append((idx, segment))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # In caso di errore, usa solo il segmento centrale\n",
    "                self.segments_to_use.append((idx, 'center'))\n",
    "                print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segments_to_use)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df_idx, segment_position = self.segments_to_use[idx]\n",
    "        \n",
    "        # Ottieni il record dal DataFrame originale\n",
    "        row = self.df.iloc[df_idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        # Determina se usare un segmento casuale (random segment augmentation)\n",
    "        use_random_segment = self.augmentations and np.random.random() < self.augmentations.p_random_segment\n",
    "        \n",
    "        # Carica e preprocessa l'audio con il segmento selezionato o casuale\n",
    "        mel_spec = load_and_preprocess_audio(\n",
    "            file_path, \n",
    "            segment_position=segment_position,\n",
    "            random_segment=use_random_segment\n",
    "        )\n",
    "        \n",
    "        # Aggiungi dimensione del canale e converti in tensor\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "        \n",
    "        # Applica XY masking se le augmentations sono attive\n",
    "        if self.augmentations and np.random.random() < self.augmentations.p_xy_mask:\n",
    "            mel_spec_tensor = self.augmentations.apply_xy_masking(mel_spec_tensor)\n",
    "        \n",
    "        # Applica altre trasformazioni se necessario\n",
    "        if self.transform:\n",
    "            mel_spec_tensor = self.transform(mel_spec_tensor)\n",
    "            \n",
    "        # Ottieni le etichette corrispondenti\n",
    "        label_tensor = torch.tensor(self.labels[df_idx], dtype=torch.float32)\n",
    "            \n",
    "        return mel_spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crezione dataset e dataloader con AdaptiveMultiSegment\n",
    "\n",
    "### Creazione dei DataLoader con Augmentation\n",
    "\n",
    "In questa sezione:\n",
    "1. Applichiamo il bilanciamento strategico al dataset di training\n",
    "2. Creiamo l'istanza di AudioAugmentations con le probabilità ottimali\n",
    "3. Configuriamo i dataloader con le funzioni di collate personalizzate\n",
    "4. Analizziamo la distribuzione dei segmenti nel dataset risultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica il bilanciamento strategico solo al dataset di training\n",
    "print(\"\\n=== Bilanciamento Strategico del Dataset di Training ===\")\n",
    "X_train_df_balanced, y_train_one_hot_balanced = create_balanced_dataset_df(\n",
    "    X_train_df, \n",
    "    y_train_one_hot,\n",
    "    abundant_class_threshold=150,  # Classi con più di 150 esempi sono considerate abbondanti\n",
    "    remove_percentage=0.4  # Rimuove il 40% degli esempi con rating bassi\n",
    ")\n",
    "\n",
    "# Crea un'istanza delle augmentations audio\n",
    "audio_augmentations = AudioAugmentations(p_random_segment=0.5, p_xy_mask=0.5, p_horizontal_cutmix=0.25)\n",
    "\n",
    "# Creiamo i dataset utilizzando AdaptiveMultiSegmentBirdDataset per il training\n",
    "print(\"Creazione dataset di training con approccio multi-segmento adattivo...\")\n",
    "# Creiamo i dataset utilizzando AudioAugmentations\n",
    "train_dataset = AdaptiveMultiSegmentBirdDataset(\n",
    "    X_train_df_balanced, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_train_one_hot_balanced,\n",
    "    augmentations=audio_augmentations  # Aggiungi augmentations\n",
    ")\n",
    "# Per validation e test, usiamo il dataset standard con segmento centrale\n",
    "print(\"Creazione dataset di validation con segmento centrale...\")\n",
    "# Per validation, non usiamo augmentation\n",
    "val_dataset = BirdDataset(X_val_df, config.TRAIN_AUDIO_DIR, y_val_one_hot)\n",
    "\n",
    "# Stampa informazioni sulla dimensione effettiva del dataset\n",
    "print(f\"\\nNumero di record originali nel training set: {len(X_train_df)}\")\n",
    "print(f\"Numero di campioni effettivi nel training set dopo l'adattamento: {len(train_dataset)}\")\n",
    "print(f\"Rapporto di espansione: {len(train_dataset) / len(X_train_df):.2f}x\")\n",
    "\n",
    "# Analizza la distribuzione dei tipi di segmenti\n",
    "segment_types = [segment for _, segment in train_dataset.segments_to_use]\n",
    "segment_counts = {\n",
    "    'start': segment_types.count('start'),\n",
    "    'center': segment_types.count('center'),\n",
    "    'end': segment_types.count('end')\n",
    "}\n",
    "\n",
    "print(\"\\nDistribuzione dei segmenti nel dataset:\")\n",
    "for segment, count in segment_counts.items():\n",
    "    print(f\"- {segment}: {count} ({count/len(train_dataset):.1%})\")\n",
    "\n",
    "# Creiamo i dataloader con horizontal cutmix per il training\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: horizontal_cutmix_collate(batch, p_cutmix=0.25)  # Custom collate\n",
    ")\n",
    "\n",
    "# Per validation, non usiamo cutmix\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Non creiamo un test_loader per ora\n",
    "test_loader = None\n",
    "    \n",
    "print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")\n",
    "print(\"Test set: utilizzerò direttamente i file nella cartella test_soundscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definizione del Modello CNN\n",
    "\n",
    "## Modello EfficientNet con Head Personalizzata\n",
    "\n",
    "Implementiamo un modello basato su EfficientNet-B0 preaddestrato, con:\n",
    "- Supporto per input a singolo canale (spettrogrammi Mel)\n",
    "- Testa di classificazione personalizzata con dropout e normalizzazione batch\n",
    "- Parametri differenziati per l'ottimizzazione\n",
    "- Gestione automatica dei checkpoint e dei pesi preaddestrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:32.847929Z",
     "iopub.status.busy": "2025-05-14T17:40:32.847570Z",
     "iopub.status.idle": "2025-05-14T17:40:36.478445Z",
     "shell.execute_reply": "2025-05-14T17:40:36.477483Z",
     "shell.execute_reply.started": "2025-05-14T17:40:32.847902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetBirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=config.N_CLASSES, pretrained=True, model_name='efficientnet_b0'):\n",
    "        super(EfficientNetBirdClassifier, self).__init__()\n",
    "        \n",
    "        # Tenta di caricare il modello con pesi pre-addestrati, gestendo fallimenti di connessione\n",
    "        try:\n",
    "            if pretrained:\n",
    "                print(f\"Tentativo di caricare {model_name} con pesi pre-addestrati...\")\n",
    "                self.efficientnet = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=True,\n",
    "                    num_classes=0  # Rimuovi il classificatore originale\n",
    "                )\n",
    "                print(\"Modello caricato con successo con pesi pre-addestrati.\")\n",
    "            else:\n",
    "                # Se pretrained=False, non tentare di scaricare i pesi\n",
    "                self.efficientnet = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=False,  # CORRETTO: ora è False\n",
    "                    num_classes=0  # Rimuovi il classificatore originale\n",
    "                )\n",
    "                print(\"Modello inizializzato senza pesi pre-addestrati (modalità offline).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento dei pesi pre-addestrati: {e}\")\n",
    "            print(\"Inizializzazione del modello senza pesi pre-addestrati...\")\n",
    "            self.efficientnet = timm.create_model(\n",
    "                model_name,\n",
    "                pretrained=False,\n",
    "                num_classes=0  # Rimuovi il classificatore originale\n",
    "            )\n",
    "        \n",
    "        # Ottieni la dimensione dell'output del feature extractor\n",
    "        if hasattr(self.efficientnet, 'num_features'):\n",
    "            classifier_in_features = self.efficientnet.num_features\n",
    "        elif hasattr(self.efficientnet, 'classifier'):\n",
    "            classifier_in_features = self.efficientnet.classifier.in_features\n",
    "        else:\n",
    "            # Valore predefinito per EfficientNet-B0\n",
    "            classifier_in_features = 1280\n",
    "        \n",
    "         # Sostituisci il classificatore semplice con una MLP con dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),  # Primo dropout significativo\n",
    "            nn.Linear(classifier_in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),  # Secondo dropout più leggero\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Se l'input è un'immagine a 1 canale, replicala su 3 canali\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Passa l'input attraverso il backbone per ottenere le features\n",
    "        features = self.efficientnet(x)\n",
    "        \n",
    "        # Passa le feature attraverso il classificatore\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Controlla se esiste un checkpoint precedente\n",
    "has_previous_checkpoint = False\n",
    "if config.environment == 'kaggle':\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(latest_checkpoint)\n",
    "elif config.environment == 'colab':\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(drive_checkpoint)\n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    has_previous_checkpoint = os.path.exists(local_checkpoint)\n",
    "\n",
    "# Inizializza il modello - usa pretrained=False se hai già un checkpoint\n",
    "model = EfficientNetBirdClassifier(\n",
    "    num_classes=config.N_CLASSES, \n",
    "    pretrained=not has_previous_checkpoint  # Scarica i pesi solo se non c'è già un checkpoint\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Definiamo ottimizzatore con learning rate differenziati\n",
    "def get_optimizer(model, lr_base=5e-4, lr_head=1e-3):  # Ridotti significativamente\n",
    "    # Parametri del corpo (pre-addestrati)\n",
    "    backbone_params = [p for name, p in model.named_parameters() \n",
    "                      if 'classifier' not in name]\n",
    "    \n",
    "    # Parametri della testa (da addestrare da zero)\n",
    "    classifier_params = [p for name, p in model.named_parameters() \n",
    "                      if 'classifier' in name]\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': lr_base},  # Ridotto da 1e-3 a 5e-4\n",
    "        {'params': classifier_params, 'lr': lr_head}  # Ridotto da 3e-3 a 1e-3\n",
    "    ], weight_decay=3e-4)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Loss function - manteniamo BCE come richiesto\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Ottimizzatore con learning rate differenziati\n",
    "optimizer = get_optimizer(model)\n",
    "\n",
    "# Learning rate scheduler - aggiornato a CosineAnnealingLR come usato dai vincitori\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.EPOCHS,  # Numero totale di epoche\n",
    "    eta_min=1e-6  # Learning rate minimo\n",
    ")\n",
    "\n",
    "print(f\"Modello EfficientNet-B0 (timm) caricato su {config.DEVICE}\")\n",
    "print(f\"Numero di classi: {config.N_CLASSES}\")\n",
    "print(f\"Parametri totali: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Addestramento e Validazione del Modello\n",
    "\n",
    "## Funzione di Training con Supporto per Checkpoint\n",
    "\n",
    "La funzione di training implementa:\n",
    "- Caricamento automatico dei checkpoint precedenti\n",
    "- Early stopping basato sulle performance di validation\n",
    "- Salvataggio periodico dei checkpoint e del miglior modello\n",
    "- Supporto per scheduler di learning rate (CosineAnnealingLR)\n",
    "- Visualizzazione delle curve di loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:36.480613Z",
     "iopub.status.busy": "2025-05-14T17:40:36.479785Z",
     "iopub.status.idle": "2025-05-14T17:40:36.553261Z",
     "shell.execute_reply": "2025-05-14T17:40:36.552366Z",
     "shell.execute_reply.started": "2025-05-14T17:40:36.480579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                model_save_path=None, model_load_path=None, patience=3,\n",
    "                resume_training=True, scheduler=None):\n",
    "    \"\"\"\n",
    "    Addestra il modello e valuta su validation set con supporto per checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da addestrare\n",
    "        train_loader: DataLoader per dati di training\n",
    "        val_loader: DataLoader per dati di validation\n",
    "        criterion: Funzione di loss\n",
    "        optimizer: Ottimizzatore\n",
    "        epochs: Numero di epoche di training\n",
    "        device: Device per l'addestramento ('cuda' o 'cpu')\n",
    "        model_save_path: Path dove salvare il modello addestrato\n",
    "        model_load_path: Path da cui caricare un modello pre-addestrato\n",
    "        patience: Numero di epoche senza miglioramento prima di terminare l'addestramento\n",
    "        resume_training: Se True, riprende il training da un checkpoint (se disponibile)\n",
    "        scheduler: Learning rate scheduler\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, total_training_time)\n",
    "    \"\"\"\n",
    "    # Directory per i checkpoint in base all'ambiente\n",
    "    checkpoint_dir = None\n",
    "    drive_mounted = False\n",
    "    \n",
    "    # Configura la directory per i checkpoint a seconda dell'ambiente\n",
    "    if config.environment == 'colab':\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            # Controlla se il drive è già montato\n",
    "            if not os.path.exists('/content/drive'):\n",
    "                print(\"Montaggio di Google Drive...\")\n",
    "                drive.mount('/content/drive')\n",
    "                print(\"Google Drive montato con successo.\")\n",
    "            \n",
    "            # Crea directory per i checkpoint se non esiste\n",
    "            checkpoint_dir = '/content/drive/MyDrive/birdclef_checkpoints'\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            print(f\"Directory per i checkpoint creata su Google Drive: {checkpoint_dir}\")\n",
    "            \n",
    "            # Aggiorna il percorso di salvataggio per usare Google Drive\n",
    "            if model_save_path:\n",
    "                filename = os.path.basename(model_save_path)\n",
    "                model_save_path = os.path.join(checkpoint_dir, filename)\n",
    "                print(f\"Il modello sarà salvato in: {model_save_path}\")\n",
    "            \n",
    "            drive_mounted = True\n",
    "        except ImportError:\n",
    "            print(\"Errore: Non riesco ad accedere a Google Drive. Continuo senza persistenza.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
    "            print(\"Continuo senza persistenza su Drive.\")\n",
    "    elif config.environment == 'kaggle':\n",
    "        # In Kaggle, usa la directory di working\n",
    "        checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in Kaggle: {checkpoint_dir}\")\n",
    "    else:\n",
    "        # In locale, usa la directory 'checkpoints' nell'OUTPUT_DIR\n",
    "        checkpoint_dir = os.path.join(config.OUTPUT_DIR, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in locale: {checkpoint_dir}\")\n",
    "    \n",
    "    # Inizializzazione variabili\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    total_training_time = 0\n",
    "    start_epoch = 0\n",
    "    needs_training = True\n",
    "    checkpoint_exists = False\n",
    "    model_loaded = False\n",
    "    \n",
    "    # Verifica se esiste un modello pre-addestrato da caricare\n",
    "    if model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Modello trovato in {model_load_path}. Tentativo di caricamento...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            \n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            print(\"Modello caricato con successo.\")\n",
    "            model_loaded = True\n",
    "            needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "            print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            needs_training = True\n",
    "    else:\n",
    "        print(f\"Modello non trovato in {model_load_path}.\")\n",
    "    \n",
    "    # Cerca un checkpoint SOLO se il caricamento del modello è fallito E resume_training è True\n",
    "    if needs_training and resume_training and checkpoint_dir and not model_loaded:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "        if os.path.exists(latest_checkpoint):\n",
    "            print(f\"Trovato checkpoint in {latest_checkpoint}. Tentativo di caricamento...\")\n",
    "            try:\n",
    "                checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "                \n",
    "                # Verifica che sia un checkpoint compatibile prima di caricarlo\n",
    "                if isinstance(checkpoint, dict) and 'epoch' in checkpoint:\n",
    "                    try:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        start_epoch = checkpoint['epoch'] + 1\n",
    "                        train_losses = checkpoint['train_losses']\n",
    "                        val_losses = checkpoint['val_losses']\n",
    "                        best_val_loss = checkpoint['best_val_loss']\n",
    "                        epochs_without_improvement = checkpoint['epochs_without_improvement']\n",
    "                        total_training_time = checkpoint.get('total_training_time', 0)\n",
    "                        \n",
    "                        # Ricrea lo scheduler con lo stato salvato se presente\n",
    "                        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "                            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                        \n",
    "                        print(f\"Checkpoint caricato con successo (epoca {start_epoch-1})\")\n",
    "                        print(f\"Si riparte dall'epoca {start_epoch}/{epochs}\")\n",
    "                        \n",
    "                        if start_epoch >= epochs:\n",
    "                            needs_training = False\n",
    "                        \n",
    "                        checkpoint_exists = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Il checkpoint non è compatibile con il modello attuale: {e}\")\n",
    "                        print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il caricamento del checkpoint: {e}\")\n",
    "                print(\"Si procederà con il training da zero.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Loop di training sulle epoche (inizia da start_epoch)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            # Aggiornamento scheduler - modificato per CosineAnnealingLR\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_val_loss)  # Per ReduceLROnPlateau\n",
    "                else:\n",
    "                    scheduler.step()  # Per CosineAnnealingLR è step() senza parametri\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            total_training_time += epoch_duration\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Duration: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            # Salvataggio checkpoint per ogni epoca (in qualsiasi ambiente)\n",
    "            if checkpoint_dir:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"birdclef_epoch_{epoch+1}.pth\")\n",
    "                \n",
    "                # Salva checkpoint completo con tutte le informazioni di stato\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_without_improvement': epochs_without_improvement,\n",
    "                    'total_training_time': total_training_time\n",
    "                }\n",
    "                \n",
    "                # Salva anche lo stato dello scheduler se esiste\n",
    "                if scheduler is not None:\n",
    "                    checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                print(f\"Checkpoint completo salvato in {checkpoint_path}\")\n",
    "                \n",
    "                # Aggiorna anche il checkpoint più recente (sovrascrive)\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "            \n",
    "            # Early stopping\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                # Salva il miglior modello separatamente\n",
    "                if model_save_path:\n",
    "                    best_path = model_save_path.replace('.pth', '_best.pth')\n",
    "                    \n",
    "                    # Salva checkpoint completo\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_losses': train_losses,\n",
    "                        'val_losses': val_losses,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }\n",
    "                    \n",
    "                    # Salva anche lo stato dello scheduler\n",
    "                    if scheduler is not None:\n",
    "                        best_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                    \n",
    "                    torch.save(best_checkpoint, best_path)\n",
    "                    print(f\"Salvato miglior modello in {best_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping attivato! Nessun miglioramento per {patience} epoche consecutive.\")\n",
    "                break\n",
    "        \n",
    "        end_time_total = time.time()\n",
    "        if checkpoint_exists:\n",
    "            total_training_time += (end_time_total - start_time_total)\n",
    "        else:\n",
    "            total_training_time = end_time_total - start_time_total\n",
    "            \n",
    "        print(f\"\\nTraining terminato in {total_training_time/60:.2f} minuti totali\")\n",
    "        \n",
    "        # Salva il modello finale\n",
    "        if model_save_path:\n",
    "            final_checkpoint = {\n",
    "                'epoch': epochs-1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'total_training_time': total_training_time\n",
    "            }\n",
    "            \n",
    "            # Salva anche lo stato dello scheduler\n",
    "            if scheduler is not None:\n",
    "                final_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "            torch.save(final_checkpoint, model_save_path)\n",
    "            print(f\"Modello finale salvato in {model_save_path}\")\n",
    "    else:\n",
    "        print(\"Training non necessario: modello già caricato o training ripreso e completato.\")\n",
    "    \n",
    "    # Visualizza le curve di loss\n",
    "    if train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Curve di Loss di Training e Validation')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Salva il grafico\n",
    "        if checkpoint_dir:\n",
    "            plt_path = os.path.join(checkpoint_dir, 'loss_curves.png')\n",
    "            plt.savefig(plt_path)\n",
    "            print(f\"Grafico delle curve di loss salvato in {plt_path}\")\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurazione e Avvio del Training\n",
    "\n",
    "Configuriamo e avviamo il training del modello:\n",
    "- Individuazione automatica dei checkpoint precedenti\n",
    "- Inizializzazione dell'ottimizzatore e dello scheduler\n",
    "- Avvio del training con i parametri ottimizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    # Directory per i checkpoint in Kaggle\n",
    "    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Verifica se esiste un checkpoint precedente\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        model_load_path = latest_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente in {latest_checkpoint}\")\n",
    "    else:\n",
    "        # Usa un modello base precaricato se disponibile\n",
    "        model_load_path = \"/kaggle/input/baseline_10_epoch_clip_adaptive_efficientNet/pytorch/default/1/birdclef_trained_model.pth\"\n",
    "        \n",
    "    # Imposta il percorso di salvataggio\n",
    "    model_save_path = \"/kaggle/working/birdclef_trained_model_efficientNET_timm.pth\"\n",
    "    \n",
    "elif config.environment == 'colab':\n",
    "    # Per Colab, verifica se esiste un checkpoint su Drive\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(drive_checkpoint):\n",
    "        model_load_path = drive_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente su Drive: {drive_checkpoint}\")\n",
    "    else:\n",
    "        # Altrimenti usa un modello base se disponibile\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "    \n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    if os.path.exists(local_checkpoint):\n",
    "        model_load_path = local_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente: {local_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Addestra il modello con CosineAnnealingLR scheduler\n",
    "train_losses, val_losses, training_time = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,  # Passa lo scheduler\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path,\n",
    "    resume_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generazione della Submission\n",
    "\n",
    "## Generazione della Submission\n",
    "\n",
    "Implementiamo la funzione per generare le predizioni sui file di test:\n",
    "- Caricamento e segmentazione delle soundscape di test\n",
    "- Estrazione di spettrogrammi Mel da ciascun segmento\n",
    "- Generazione delle predizioni con il modello addestrato\n",
    "- Creazione del file di submission nel formato richiesto dalla competizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:36.574352Z",
     "iopub.status.busy": "2025-05-14T17:40:36.573965Z",
     "iopub.status.idle": "2025-05-14T17:40:36.639233Z",
     "shell.execute_reply": "2025-05-14T17:40:36.638203Z",
     "shell.execute_reply.started": "2025-05-14T17:40:36.574325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_submission_simple(model, device=config.DEVICE):\n",
    "    \"\"\"\n",
    "    Genera un file di submission usando una segmentazione semplificata.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch addestrato\n",
    "        device: Device per inferenza ('cuda' o 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame di submission\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Set seed per riproducibilità\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Percorso dei test soundscapes\n",
    "    test_soundscape_path = config.TEST_SOUNDSCAPES_DIR\n",
    "    test_soundscapes = [os.path.join(test_soundscape_path, afile) \n",
    "                        for afile in sorted(os.listdir(test_soundscape_path)) \n",
    "                        if afile.endswith('.ogg')]\n",
    "    \n",
    "    print(f\"Elaborazione di {len(test_soundscapes)} file soundscape...\")\n",
    "    \n",
    "    # Crea DataFrame per le predizioni\n",
    "    predictions = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "    \n",
    "    for soundscape in tqdm(test_soundscapes, desc=\"Elaborazione soundscapes\"):\n",
    "        # Carica audio\n",
    "        sig, rate = librosa.load(path=soundscape, sr=config.SR)\n",
    "        \n",
    "        # Split in segmenti da 5 secondi\n",
    "        segment_length = rate * config.TEST_CLIP_DURATION\n",
    "        chunks = []\n",
    "        for i in range(0, len(sig), segment_length):\n",
    "            chunk = sig[i:i+segment_length]\n",
    "            # Padda se necessario\n",
    "            if len(chunk) < segment_length:\n",
    "                chunk = np.pad(chunk, (0, segment_length - len(chunk)), mode='constant')\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Genera predizioni per ogni segmento\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Calcola row_id (nome file + tempo finale del segmento in secondi)\n",
    "            file_name = os.path.basename(soundscape).split('.')[0]\n",
    "            row_id = f\"{file_name}_{i * config.TEST_CLIP_DURATION + config.TEST_CLIP_DURATION}\"\n",
    "            \n",
    "            # Calcola spettrogramma Mel\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=chunk, sr=config.SR,\n",
    "                n_fft=config.N_FFT,\n",
    "                hop_length=config.HOP_LENGTH,\n",
    "                n_mels=config.N_MELS,\n",
    "                fmin=config.FMIN,\n",
    "                fmax=config.FMAX\n",
    "            )\n",
    "            \n",
    "            # Converti in scala logaritmica (dB) e normalizza\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            min_val = np.min(log_mel_spec)\n",
    "            max_val = np.max(log_mel_spec)\n",
    "            if max_val > min_val:\n",
    "                log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                log_mel_spec = np.zeros_like(log_mel_spec)\n",
    "            \n",
    "            # Aggiungi dimensione batch e canale\n",
    "            log_mel_spec = np.expand_dims(np.expand_dims(log_mel_spec, axis=0), axis=0)\n",
    "            \n",
    "            # Converti in tensor\n",
    "            input_tensor = torch.tensor(log_mel_spec, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Effettua predizione\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                scores = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Aggiungi riga al DataFrame di predizioni\n",
    "            new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + all_species)\n",
    "            predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Salva la submission come CSV\n",
    "    predictions.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Genera submission\n",
    "if config.environment == 'kaggle':\n",
    "    print(\"\\nGenerazione del file di submission con il nuovo metodo...\")\n",
    "    submission_df = generate_submission_simple(model)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\nAnteprima del file di submission:\")\n",
    "        print(submission_df.head())\n",
    "else:\n",
    "    print(\"\\nSalto la generazione della submission perché non siamo su Kaggle.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
