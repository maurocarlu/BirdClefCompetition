{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdfb9f2",
   "metadata": {
    "papermill": {
     "duration": 0.010235,
     "end_time": "2025-05-23T08:16:57.646225",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.635990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con CNN\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza un'architettura CNN per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5518a6b",
   "metadata": {
    "papermill": {
     "duration": 0.008058,
     "end_time": "2025-05-23T08:16:57.663760",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.655702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Importazione delle Librerie Necessarie\n",
    "\n",
    "Importiamo tutte le librerie necessarie per l'elaborazione audio, deep learning e visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542df0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:35.694932Z",
     "iopub.status.busy": "2025-06-10T11:16:35.694677Z",
     "iopub.status.idle": "2025-06-10T11:16:48.484917Z",
     "shell.execute_reply": "2025-06-10T11:16:48.484096Z",
     "shell.execute_reply.started": "2025-06-10T11:16:35.694912Z"
    },
    "papermill": {
     "duration": 28.02249,
     "end_time": "2025-05-23T08:17:25.694495",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.672005",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# Sostituisci le importazioni di Transformers con timm\n",
    "import timm\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy import signal\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"timm versione: {timm.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68fd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.486723Z",
     "iopub.status.busy": "2025-06-10T11:16:48.486462Z",
     "iopub.status.idle": "2025-06-10T11:16:48.493061Z",
     "shell.execute_reply": "2025-06-10T11:16:48.492214Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.486705Z"
    },
    "papermill": {
     "duration": 0.022231,
     "end_time": "2025-05-23T08:17:25.727295",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.705064",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Imposta questo a True per abilitare la cancellazione\n",
    "clear_working_dir = True\n",
    "\n",
    "working_dir = '/kaggle/working/'\n",
    "\n",
    "if clear_working_dir:\n",
    "    for filename in os.listdir(working_dir):\n",
    "        file_path = os.path.join(working_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # elimina file o link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # elimina directory\n",
    "        except Exception as e:\n",
    "            print(f'Errore durante la rimozione di {file_path}: {e}')\n",
    "    print(f\"Tutti i file in {working_dir} sono stati rimossi.\")\n",
    "else:\n",
    "    print(\"Pulizia disabilitata (clear_working_dir = False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ca1b2",
   "metadata": {
    "papermill": {
     "duration": 0.009235,
     "end_time": "2025-05-23T08:17:25.745603",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.736368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configurazione dell'Ambiente di Esecuzione\n",
    "\n",
    "In questa sezione configuriamo l'ambiente di esecuzione in modo che il notebook funzioni sia su Kaggle, che su Google Colab, che in locale. Il codice rileverà automaticamente l'ambiente e configurerà i percorsi di conseguenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253f9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.494032Z",
     "iopub.status.busy": "2025-06-10T11:16:48.493746Z",
     "iopub.status.idle": "2025-06-10T11:16:48.550324Z",
     "shell.execute_reply": "2025-06-10T11:16:48.549592Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.494004Z"
    },
    "papermill": {
     "duration": 0.022269,
     "end_time": "2025-05-23T08:17:25.776636",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.754367",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbe73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.552327Z",
     "iopub.status.busy": "2025-06-10T11:16:48.552079Z",
     "iopub.status.idle": "2025-06-10T11:16:48.571116Z",
     "shell.execute_reply": "2025-06-10T11:16:48.570453Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.552310Z"
    },
    "papermill": {
     "duration": 0.029321,
     "end_time": "2025-05-23T08:17:25.814763",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.785442",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            # quindi non impostiamo ancora i percorsi derivati\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio - già allineati con vincitori\n",
    "        self.SR = 32000      # Sample rate\n",
    "        self.DURATION = 5    # Durata dei clip in secondi\n",
    "        self.N_MELS = 128    # Numero di bande Mel\n",
    "        self.N_FFT = 1024    # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 256  # Hop length per STFT\n",
    "        self.FMIN = 48       # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = 15000    # Frequenza massima\n",
    "        self.POWER = 2.0       # Esponente per calcolo spettrogramma\n",
    "\n",
    "        self.WIN_LENGTH = None  # Usa n_fft come default\n",
    "        self.PAD_MODE = \"constant\"  # Padding mode per spettrogrammi\n",
    "        self.MEL_SCALE = \"htk\"      # Scale Mel (Bird25 usa HTK)\n",
    "        self.NORM = \"slaney\" \n",
    "            \n",
    "        # Parametri per il training - aggiornati secondo i vincitori\n",
    "        self.BATCH_SIZE = 64  # Aumentato da 32 a 64 \n",
    "        self.EPOCHS = 23     # Numero di epoche per il training\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 4  # Aumentato per migliorare il data loading\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d552f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.571998Z",
     "iopub.status.busy": "2025-06-10T11:16:48.571749Z",
     "iopub.status.idle": "2025-06-10T11:16:48.652611Z",
     "shell.execute_reply": "2025-06-10T11:16:48.651889Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.571972Z"
    },
    "papermill": {
     "duration": 0.033917,
     "end_time": "2025-05-23T08:17:25.857854",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.823937",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/simplecnn/PyTorch/default/1\"\n",
    "    cache_model_file = os.path.join(cache_model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(cache_model_file)\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/simplecnn/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    model_file = os.path.join(model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {model_file}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b77a54",
   "metadata": {},
   "source": [
    "### Normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ca1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.653594Z",
     "iopub.status.busy": "2025-06-10T11:16:48.653337Z",
     "iopub.status.idle": "2025-06-10T11:16:48.761445Z",
     "shell.execute_reply": "2025-06-10T11:16:48.760655Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.653567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crea una singola istanza della trasformazione MelSpectrogram da riutilizzare\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=config.SR,\n",
    "    n_fft=config.N_FFT,\n",
    "    win_length=None,\n",
    "    hop_length=config.HOP_LENGTH,\n",
    "    f_min=config.FMIN,\n",
    "    f_max=config.FMAX,\n",
    "    n_mels=config.N_MELS,\n",
    "    window_fn=torch.hann_window,\n",
    "    power=config.POWER,\n",
    "    normalized=False,\n",
    "    onesided=True,\n",
    "    norm=\"slaney\",\n",
    "    mel_scale=\"htk\",\n",
    "    pad_mode=\"constant\"\n",
    ")\n",
    "\n",
    "# Funzione di conversione a dB e normalizzazione\n",
    "def amplitude_to_db_minmax(spectrogram):\n",
    "    \"\"\"\n",
    "    Converti in dB e applica normalizzazione Min-Max come Bird25\n",
    "    \"\"\"\n",
    "    # Converti in dB\n",
    "    spectrogram_db = 10.0 * torch.log10(torch.clamp(spectrogram, min=1e-10))\n",
    "    \n",
    "    # Normalizzazione Min-Max [0,1] come Bird25\n",
    "    min_val = torch.min(spectrogram_db)\n",
    "    max_val = torch.max(spectrogram_db)\n",
    "    \n",
    "    # Evita divisione per zero\n",
    "    range_val = max_val - min_val\n",
    "    if range_val > 1e-8:\n",
    "        normalized = (spectrogram_db - min_val) / range_val\n",
    "    else:\n",
    "        normalized = torch.zeros_like(spectrogram_db)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Funzione per gestire file audio troppo corti (come Bird25)\n",
    "def handle_short_audio(audio_data, target_samples):\n",
    "    \"\"\"\n",
    "    Gestisce file audio troppo corti concatenandoli come Bird25\n",
    "    \"\"\"\n",
    "    if len(audio_data) < target_samples:\n",
    "        # Calcola quante copie servono\n",
    "        n_copy = math.ceil(target_samples / len(audio_data))\n",
    "        if n_copy > 1:\n",
    "            # Concatena l'audio n_copy volte\n",
    "            audio_data = np.concatenate([audio_data] * n_copy)\n",
    "    \n",
    "    return audio_data[:target_samples]  # Tronca alla lunghezza esatta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8881d",
   "metadata": {
    "papermill": {
     "duration": 0.010495,
     "end_time": "2025-05-23T08:17:25.878875",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.868380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Configurazione del Modello e Parametri\n",
    "\n",
    "Definiamo i parametri di configurazione per il preprocessamento audio, la creazione dello spettrogramma Mel e l'addestramento della CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3039383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.762648Z",
     "iopub.status.busy": "2025-06-10T11:16:48.762356Z",
     "iopub.status.idle": "2025-06-10T11:16:48.771503Z",
     "shell.execute_reply": "2025-06-10T11:16:48.770689Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.762620Z"
    },
    "papermill": {
     "duration": 0.029142,
     "end_time": "2025-05-23T08:17:25.918990",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.889848",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I parametri principali sono già definiti nella classe Config\n",
    "# Verifichiamo l'esistenza delle directory e creiamo quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1420b9",
   "metadata": {
    "papermill": {
     "duration": 0.009036,
     "end_time": "2025-05-23T08:17:25.937146",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.928110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Caricamento e Preprocessing dei Dati\n",
    "\n",
    "In questa sezione carichiamo i metadati dal file CSV di training, creiamo codifiche one-hot per le etichette delle specie e implementiamo funzioni per il caricamento e preprocessamento dei file audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc24a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.773148Z",
     "iopub.status.busy": "2025-06-10T11:16:48.772425Z",
     "iopub.status.idle": "2025-06-10T11:16:49.917820Z",
     "shell.execute_reply": "2025-06-10T11:16:49.917150Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.773128Z"
    },
    "papermill": {
     "duration": 1.647048,
     "end_time": "2025-05-23T08:17:27.594652",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.947604",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "    \n",
    "# Per Kaggle, dovremo creare un dataset speciale per le soundscapes di test\n",
    "# Questo verrà utilizzato direttamente nella fase di generazione della submission\n",
    "# Non creiamo X_test_df e test_dataset per ora\n",
    "X_test_df = None\n",
    "y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95eca4",
   "metadata": {
    "papermill": {
     "duration": 0.008471,
     "end_time": "2025-05-23T08:17:27.612056",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.603585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Funzione di bilanciamento del dataset - Cancella una percentuale di esempi dalle classi molto numerose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b82d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:49.918767Z",
     "iopub.status.busy": "2025-06-10T11:16:49.918505Z",
     "iopub.status.idle": "2025-06-10T11:16:49.927078Z",
     "shell.execute_reply": "2025-06-10T11:16:49.926322Z",
     "shell.execute_reply.started": "2025-06-10T11:16:49.918749Z"
    },
    "papermill": {
     "duration": 0.029146,
     "end_time": "2025-05-23T08:17:27.649859",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.620713",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataset_df(train_df, labels_one_hot, abundant_class_threshold=200, remove_percentage=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame bilanciato rimuovendo parte degli esempi con rating bassi dalle classi abbondanti.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame originale\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        abundant_class_threshold: Soglia per definire una classe come \"abbondante\"\n",
    "        remove_percentage: Percentuale di esempi con rating 1-3 da rimuovere dalle classi abbondanti\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame bilanciato, etichette one-hot bilanciate)\n",
    "    \"\"\"\n",
    "    # Conta esempi per ogni classe\n",
    "    class_counts = train_df['primary_label'].value_counts()\n",
    "    \n",
    "    # Identifica classi abbondanti\n",
    "    abundant_classes = class_counts[class_counts > abundant_class_threshold].index.tolist()\n",
    "    print(f\"Classi identificate come abbondanti (>{abundant_class_threshold} esempi): {len(abundant_classes)}\")\n",
    "    \n",
    "    # Copia il DataFrame originale\n",
    "    balanced_df = train_df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Contatori per statistiche\n",
    "    total_removed = 0\n",
    "    removed_by_class = {}\n",
    "    \n",
    "    # Per ogni classe abbondante\n",
    "    for cls in abundant_classes:\n",
    "        # Filtra esempi con rating 1-3 per questa classe\n",
    "        low_quality_mask = (balanced_df['primary_label'] == cls) & (balanced_df['rating'].isin([1, 2, 3]))\n",
    "        low_quality_indices = balanced_df[low_quality_mask].index.tolist()\n",
    "        \n",
    "        # Numero di esempi da rimuovere\n",
    "        n_to_remove = int(len(low_quality_indices) * remove_percentage)\n",
    "        \n",
    "        # Seleziona casualmente gli indici da rimuovere\n",
    "        np.random.seed(random_state)\n",
    "        if n_to_remove > 0:\n",
    "            indices_to_remove = np.random.choice(low_quality_indices, size=n_to_remove, replace=False)\n",
    "            \n",
    "            # Memorizza gli indici da rimuovere\n",
    "            rows_to_drop.extend(indices_to_remove)\n",
    "            \n",
    "            # Aggiorna statistiche\n",
    "            removed_by_class[cls] = n_to_remove\n",
    "            total_removed += n_to_remove\n",
    "    \n",
    "    # Rimuovi le righe selezionate\n",
    "    if rows_to_drop:\n",
    "        balanced_df = balanced_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Aggiorna anche le etichette one-hot rimuovendo gli stessi indici\n",
    "        mask = np.ones(len(train_df), dtype=bool)\n",
    "        mask[rows_to_drop] = False\n",
    "        balanced_labels = labels_one_hot[mask]\n",
    "    else:\n",
    "        balanced_labels = labels_one_hot\n",
    "    \n",
    "    # Statistiche finali\n",
    "    print(f\"Totale esempi rimossi: {total_removed} ({total_removed/len(train_df):.1%} del dataset originale)\")\n",
    "    print(f\"Dimensione dataset originale: {len(train_df)}\")\n",
    "    print(f\"Dimensione dataset bilanciato: {len(balanced_df)}\")\n",
    "    \n",
    "    # Visualizza le prime 5 classi con maggiori rimozioni\n",
    "    if removed_by_class:\n",
    "        top_removed = sorted(removed_by_class.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nClassi con maggior numero di esempi rimossi:\")\n",
    "        for cls, count in top_removed:\n",
    "            original = class_counts[cls]\n",
    "            remaining = original - count\n",
    "            print(f\"- {cls}: {count} rimossi, {remaining}/{original} rimanenti ({remaining/original:.1%})\")\n",
    "    else:\n",
    "        print(\"Nessun esempio rimosso.\")\n",
    "    \n",
    "    return balanced_df, balanced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573968a",
   "metadata": {
    "papermill": {
     "duration": 0.008725,
     "end_time": "2025-05-23T08:17:27.668000",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.659275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.5 Analisi Esplorativa dei Dati (EDA)\n",
    "\n",
    "In questa sezione esploreremo le caratteristiche del dataset per comprendere meglio la distribuzione delle specie, le proprietà audio e identificare eventuali pattern nei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:49.929612Z",
     "iopub.status.busy": "2025-06-10T11:16:49.929407Z",
     "iopub.status.idle": "2025-06-10T11:16:52.362879Z",
     "shell.execute_reply": "2025-06-10T11:16:52.361969Z",
     "shell.execute_reply.started": "2025-06-10T11:16:49.929596Z"
    },
    "papermill": {
     "duration": 3.453267,
     "end_time": "2025-05-23T08:17:31.130595",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.677328",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configurazione stile visualizzazioni\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"=== Statistiche di base del dataset ===\")\n",
    "print(f\"Numero totale di registrazioni: {len(train_df)}\")\n",
    "print(f\"Numero di specie uniche nel dataset: {len(all_species)}\")\n",
    "print(f\"Campi disponibili nei metadati: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Verifichiamo i dati mancanti\n",
    "missing_data = train_df.isnull().sum()\n",
    "print(\"\\n=== Valori mancanti ===\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# 1. Distribuzione delle specie nel dataset (visualizzazione migliorata)\n",
    "print(\"\\n=== Analisi delle Specie ===\")\n",
    "primary_species_count = train_df['primary_label'].value_counts()\n",
    "\n",
    "# Plot combinato: distribuzione delle specie con evidenza delle classi rare\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# A sinistra: top 20 specie più rappresentate\n",
    "sns.barplot(x=primary_species_count.head(20).index, y=primary_species_count.head(20).values, ax=ax1)\n",
    "ax1.set_title('Top 20 Specie per Numero di Registrazioni')\n",
    "ax1.set_xlabel('Specie')\n",
    "ax1.set_ylabel('Numero di Registrazioni')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# A destra: distribuzione del numero di esempi per specie\n",
    "sns.histplot(primary_species_count, bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribuzione del Numero di Registrazioni per Specie')\n",
    "ax2.set_xlabel('Numero di Registrazioni')\n",
    "ax2.set_ylabel('Conteggio Specie')\n",
    "ax2.axvline(x=primary_species_count.median(), color='r', linestyle='--', \n",
    "            label=f'Mediana: {primary_species_count.median()}')\n",
    "ax2.axvline(x=primary_species_count.mean(), color='g', linestyle='--', \n",
    "            label=f'Media: {primary_species_count.mean():.1f}')\n",
    "ax2.axvline(x=50, color='orange', linestyle=':', label='Soglia classi rare (50)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcolo dell'indice di Gini per misurare lo sbilanciamento\n",
    "def gini_coefficient(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n+1)\n",
    "    return (np.sum((2*index - n - 1) * x)) / (n * np.sum(x))\n",
    "\n",
    "gini = gini_coefficient(primary_species_count.values)\n",
    "print(f\"\\nIndice di Gini per la distribuzione delle specie: {gini:.4f}\")\n",
    "print(f\"Questo indica {'un alto' if gini > 0.6 else 'un moderato' if gini > 0.3 else 'un basso'} livello di sbilanciamento nel dataset.\")\n",
    "\n",
    "# 2. NUOVA ANALISI: Rating per le classi con pochi esempi\n",
    "# Definisco la soglia per le classi rare (< 50 esempi)\n",
    "RARE_CLASS_THRESHOLD = 50\n",
    "rare_species = primary_species_count[primary_species_count < RARE_CLASS_THRESHOLD].index.tolist()\n",
    "print(f\"\\n=== Analisi dei Rating per Classi Rare (<{RARE_CLASS_THRESHOLD} esempi) ===\")\n",
    "print(f\"Numero di classi rare: {len(rare_species)} su {len(all_species)} totali ({len(rare_species)/len(all_species):.1%})\")\n",
    "\n",
    "# Raccolgo i dati sui rating per le classi rare\n",
    "rare_class_ratings = []\n",
    "for species in rare_species:\n",
    "    species_df = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_df['rating'].fillna(0).tolist()  # Sostituisco NaN con 0 (nessun rating)\n",
    "    \n",
    "    # Statistiche per questa specie\n",
    "    rare_class_ratings.append({\n",
    "        'species': species,\n",
    "        'count': len(species_df),\n",
    "        'avg_rating': np.mean(ratings),\n",
    "        'ratings': ratings,\n",
    "        'rating_counts': {r: ratings.count(r) for r in set(ratings)}\n",
    "    })\n",
    "\n",
    "# Creo DataFrame per analisi\n",
    "rare_ratings_df = pd.DataFrame(rare_class_ratings)\n",
    "rare_ratings_df = rare_ratings_df.sort_values('count')\n",
    "\n",
    "# Visualizzazione: Rating medi vs Conteggio per le classi rare\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Heatmap: distribuzione dei rating per classi rare\n",
    "n_rare_to_show = min(30, len(rare_ratings_df))  # Mostra max 30 classi per leggibilità\n",
    "rare_sample = rare_ratings_df.head(n_rare_to_show)\n",
    "\n",
    "# Preparo i dati per la heatmap\n",
    "heatmap_data = []\n",
    "rating_values = [0, 1, 2, 3, 4, 5]  # Tutti i possibili rating\n",
    "for _, row in rare_sample.iterrows():\n",
    "    species_data = [row['species'], row['count']]\n",
    "    for rating in rating_values:\n",
    "        species_data.append(row['rating_counts'].get(rating, 0))\n",
    "    heatmap_data.append(species_data)\n",
    "\n",
    "# Creo DataFrame per la heatmap\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data, \n",
    "    columns=['species', 'count'] + [f'rating_{r}' for r in rating_values]\n",
    ")\n",
    "\n",
    "# Plot combinato con scatter plot e heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Scatter plot: rating medio vs numero esempi\n",
    "sns.scatterplot(\n",
    "    x='count', \n",
    "    y='avg_rating', \n",
    "    data=rare_ratings_df, \n",
    "    ax=ax1, \n",
    "    alpha=0.7,\n",
    "    hue='count',\n",
    "    palette='viridis',\n",
    "    size='count',\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "ax1.set_title('Rating Medio vs Numero di Esempi per Classi Rare')\n",
    "ax1.set_xlabel('Numero di Esempi')\n",
    "ax1.set_ylabel('Rating Medio')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mostra statistiche\n",
    "avg_rating_rare = rare_ratings_df['avg_rating'].mean()\n",
    "ax1.axhline(y=avg_rating_rare, color='r', linestyle='--', \n",
    "           label=f'Rating medio classi rare: {avg_rating_rare:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Heatmap: distribuzione dei rating per le classi più rare\n",
    "pivot_data = pd.DataFrame({\n",
    "    'species': heatmap_df['species'],\n",
    "    'Rating 0': heatmap_df['rating_0'],\n",
    "    'Rating 1': heatmap_df['rating_1'],\n",
    "    'Rating 2': heatmap_df['rating_2'],\n",
    "    'Rating 3': heatmap_df['rating_3'],\n",
    "    'Rating 4': heatmap_df['rating_4'],\n",
    "    'Rating 5': heatmap_df['rating_5'],\n",
    "}).set_index('species')\n",
    "\n",
    "sns.heatmap(pivot_data, cmap=\"YlGnBu\", annot=True, fmt='g', ax=ax2)\n",
    "ax2.set_title(f'Distribuzione dei Rating nelle {n_rare_to_show} Classi più Rare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche aggregate sui rating per le classi rare\n",
    "print(\"\\nStatistiche sui rating per le classi rare:\")\n",
    "print(f\"- Rating medio complessivo: {rare_ratings_df['avg_rating'].mean():.2f}\")\n",
    "print(f\"- Percentuale di registrazioni senza rating (0): {sum(r['rating_counts'].get(0, 0) for r in rare_class_ratings) / sum(r['count'] for r in rare_class_ratings):.1%}\")\n",
    "\n",
    "# Analisi delle classi estremamente rare (≤ 5 esempi)\n",
    "very_rare_species = primary_species_count[primary_species_count <= 5].index.tolist()\n",
    "print(f\"\\nClassi estremamente rare (≤ 5 esempi): {len(very_rare_species)}\")\n",
    "\n",
    "very_rare_df = train_df[train_df['primary_label'].isin(very_rare_species)]\n",
    "print(\"Dettaglio delle registrazioni per le classi estremamente rare:\")\n",
    "for species in very_rare_species:\n",
    "    species_data = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_data['rating'].fillna(0).tolist()\n",
    "    print(f\"- {species}: {len(species_data)} esempi, ratings: {ratings}\")\n",
    "\n",
    "\n",
    "# Identificazione delle classi con solo rating molto bassi (1-2)\n",
    "print(\"\\n=== Analisi delle Classi con SOLO Rating Molto Bassi (1-2) ===\")\n",
    "\n",
    "# Funzione per verificare se una specie ha esclusivamente rating tra 1-2\n",
    "def has_only_low_ratings(species_ratings):\n",
    "    valid_ratings = [r for r in species_ratings if pd.notna(r) and r != 0]  # Escludi NaN e rating=0\n",
    "    if not valid_ratings:  # Se non ci sono rating validi\n",
    "        return False\n",
    "    return all(1 <= r <= 2 for r in valid_ratings)  # Modificato: ora solo 1-2\n",
    "\n",
    "# Raggruppa per specie e analizza\n",
    "low_rating_species = []\n",
    "for species, group in train_df.groupby('primary_label'):\n",
    "    ratings = group['rating'].tolist()\n",
    "    if has_only_low_ratings(ratings):\n",
    "        low_rating_species.append({\n",
    "            'species': species,\n",
    "            'count': len(ratings),\n",
    "            'avg_rating': np.nanmean([r for r in ratings if pd.notna(r) and r != 0]),\n",
    "            'ratings': sorted([r for r in ratings if pd.notna(r) and r != 0])\n",
    "        })\n",
    "\n",
    "# Crea DataFrame e visualizza risultati\n",
    "if low_rating_species:\n",
    "    low_rating_df = pd.DataFrame(low_rating_species).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Trovate {len(low_rating_species)} classi con SOLO rating molto bassi (1-2):\")\n",
    "    \n",
    "    # Visualizzazione\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(low_rating_df['species'], low_rating_df['avg_rating'], alpha=0.7, color='tomato')\n",
    "    plt.axhline(y=1.5, color='r', linestyle='--', label='Media teorica = 1.5')\n",
    "    plt.title('Classi con Esclusivamente Rating Molto Bassi (1-2)')\n",
    "    plt.xlabel('Specie')\n",
    "    plt.ylabel('Rating Medio')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostra dettagli delle prime 10 classi\n",
    "    print(\"\\nDettagli delle prime 10 classi con solo rating molto bassi:\")\n",
    "    for i, row in low_rating_df.head(10).iterrows():\n",
    "        print(f\"- {row['species']}: {row['count']} clip, rating medio: {row['avg_rating']:.2f}, ratings: {row['ratings']}\")\n",
    "    \n",
    "    # Cerca sovrapposizione con classi rare\n",
    "    overlap = [s for s in low_rating_df['species'] if s in rare_species]\n",
    "    print(f\"\\nSovrapposizione con classi rare (<{RARE_CLASS_THRESHOLD} esempi): {len(overlap)} classi\")\n",
    "    if overlap:\n",
    "        print(f\"Le classi rare che hanno solo rating molto bassi: {overlap[:10]}{'...' if len(overlap) > 10 else ''}\")\n",
    "else:\n",
    "    print(\"Nessuna classe ha esclusivamente rating da 1 a 2.\")\n",
    "\n",
    "print(\"\\n=== Conclusioni dall'Analisi delle Classi Rare ===\")\n",
    "print(f\"1. Abbiamo {len(rare_species)} classi rare (<{RARE_CLASS_THRESHOLD} esempi)\")\n",
    "print(f\"2. Di queste, {len(very_rare_species)} hanno 5 o meno esempi\")\n",
    "print(\"3. La qualità delle registrazioni (rating) è un fattore critico per le classi rare\")\n",
    "print(\"4. Le classi estremamente rare richiedono tecniche speciali (data augmentation, few-shot learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ae8c3",
   "metadata": {
    "papermill": {
     "duration": 0.018439,
     "end_time": "2025-05-23T08:17:31.167560",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.149121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation: Implementazione delle Tecniche dei Vincitori\n",
    "\n",
    "In questa sezione implementiamo le tre tecniche di data augmentation che hanno contribuito significativamente alle performance dei vincitori:\n",
    "1. **Random Segment Selection** - Estrae segmenti casuali dalle registrazioni audio\n",
    "2. **XY Masking** - Applica maschere casuali sugli assi tempo e frequenza degli spettrogrammi Mel\n",
    "3. **Horizontal CutMix** - Combina parti di spettrogrammi da diverse registrazioni\n",
    "\n",
    "La classe `AudioAugmentations` gestisce tutte queste trasformazioni in modo unificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064f2c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.372056Z",
     "iopub.status.busy": "2025-06-10T11:16:52.371872Z",
     "iopub.status.idle": "2025-06-10T11:16:52.393405Z",
     "shell.execute_reply": "2025-06-10T11:16:52.392691Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.372023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SEDAugmentations:\n",
    "    \"\"\"Augmentations specifiche per SED utilizzate dai vincitori\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 p_mixup=0.5,\n",
    "                 p_background_noise=0.3,\n",
    "                 p_random_filtering=0.4,\n",
    "                 p_spec_freq=0.3,\n",
    "                 p_spec_time=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p_mixup: Probabilità di applicare Mixup\n",
    "            p_background_noise: Probabilità di aggiungere rumore di background\n",
    "            p_random_filtering: Probabilità di applicare random filtering\n",
    "            p_spec_freq: Probabilità di frequency masking\n",
    "            p_spec_time: Probabilità di time masking\n",
    "        \"\"\"\n",
    "        self.p_mixup = p_mixup\n",
    "        self.p_background_noise = p_background_noise\n",
    "        self.p_random_filtering = p_random_filtering\n",
    "        self.p_spec_freq = p_spec_freq\n",
    "        self.p_spec_time = p_spec_time\n",
    "        \n",
    "        # Parametri per SpecAug (come nei tuoi esempi)\n",
    "        self.freq_max_length = 10\n",
    "        self.freq_max_lines = 3\n",
    "        self.time_max_length = 20\n",
    "        self.time_max_lines = 3\n",
    "    \n",
    "    def apply_mixup_audio(self, audio1, audio2, alpha=0.2):\n",
    "        \"\"\"Applica Mixup direttamente sull'audio\"\"\"\n",
    "        if np.random.random() > self.p_mixup:\n",
    "            return audio1, 1.0  # Nessun mixup, peso = 1.0\n",
    "        \n",
    "        # Genera lambda dalla distribuzione Beta\n",
    "        lam = np.random.beta(alpha, alpha) if alpha > 0 else 0.5\n",
    "        \n",
    "        # Assicurati che entrambi gli audio abbiano la stessa lunghezza\n",
    "        min_len = min(len(audio1), len(audio2))\n",
    "        audio1 = audio1[:min_len]\n",
    "        audio2 = audio2[:min_len]\n",
    "        \n",
    "        # Mix degli audio\n",
    "        mixed_audio = lam * audio1 + (1 - lam) * audio2\n",
    "        \n",
    "        return mixed_audio, lam  # Ritorna anche il peso per le etichette\n",
    "    \n",
    "    def add_background_noise(self, audio, noise_factor=0.005):\n",
    "        \"\"\"Aggiunge rumore di background (simula Zenodo nocall)\"\"\"\n",
    "        if np.random.random() > self.p_background_noise:\n",
    "            return audio\n",
    "        \n",
    "        # Genera rumore gaussiano\n",
    "        noise = np.random.normal(0, noise_factor, len(audio))\n",
    "        \n",
    "        # Simula anche rumore ambientale a bassa frequenza\n",
    "        if np.random.random() < 0.5:\n",
    "            # Rumore a bassa frequenza (vento, etc.)\n",
    "            t = np.linspace(0, len(audio) / config.SR, len(audio))\n",
    "            low_freq_noise = noise_factor * 0.5 * np.sin(2 * np.pi * np.random.uniform(1, 10) * t)\n",
    "            noise += low_freq_noise\n",
    "        \n",
    "        return audio + noise\n",
    "    \n",
    "    def apply_random_filtering(self, audio, sr=config.SR):\n",
    "        \"\"\"Random Filtering - equalizzatore casuale semplificato\"\"\"\n",
    "        if np.random.random() > self.p_random_filtering:\n",
    "            return audio\n",
    "        \n",
    "        # Scegli tipo di filtro casualmente\n",
    "        filter_type = np.random.choice(['highpass', 'lowpass', 'bandpass'])\n",
    "        \n",
    "        if filter_type == 'highpass':\n",
    "            # High-pass filter (rimuove basse frequenze)\n",
    "            cutoff = np.random.uniform(100, 1000)  # Hz\n",
    "            sos = signal.butter(4, cutoff, btype='highpass', fs=sr, output='sos')\n",
    "            \n",
    "        elif filter_type == 'lowpass':\n",
    "            # Low-pass filter (rimuove alte frequenze)\n",
    "            cutoff = np.random.uniform(8000, 15000)  # Hz\n",
    "            sos = signal.butter(4, cutoff, btype='lowpass', fs=sr, output='sos')\n",
    "            \n",
    "        else:  # bandpass\n",
    "            # Band-pass filter\n",
    "            low = np.random.uniform(200, 1000)\n",
    "            high = np.random.uniform(8000, 14000)\n",
    "            sos = signal.butter(4, [low, high], btype='bandpass', fs=sr, output='sos')\n",
    "        \n",
    "        try:\n",
    "            filtered_audio = signal.sosfilt(sos, audio)\n",
    "            return filtered_audio.astype(np.float32)\n",
    "        except:\n",
    "            return audio  # Se il filtro fallisce, ritorna audio originale\n",
    "    \n",
    "    def apply_spec_augment(self, spectrogram):\n",
    "        \"\"\"Applica SpecAugment con i parametri che hai specificato\"\"\"\n",
    "        spec = spectrogram.clone()\n",
    "        \n",
    "        if len(spec.shape) == 3:  # [C, H, W]\n",
    "            C, H, W = spec.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Forma spettrogramma non supportata: {spec.shape}\")\n",
    "        \n",
    "        # Frequency Masking\n",
    "        if np.random.random() < self.p_spec_freq:\n",
    "            for _ in range(np.random.randint(1, self.freq_max_lines + 1)):\n",
    "                mask_height = np.random.randint(1, min(self.freq_max_length + 1, H // 4))\n",
    "                mask_start = np.random.randint(0, max(1, H - mask_height))\n",
    "                spec[:, mask_start:mask_start + mask_height, :] = 0\n",
    "        \n",
    "        # Time Masking\n",
    "        if np.random.random() < self.p_spec_time:\n",
    "            for _ in range(np.random.randint(1, self.time_max_lines + 1)):\n",
    "                mask_width = np.random.randint(1, min(self.time_max_length + 1, W // 4))\n",
    "                mask_start = np.random.randint(0, max(1, W - mask_width))\n",
    "                spec[:, :, mask_start:mask_start + mask_width] = 0\n",
    "        \n",
    "        return spec\n",
    "    \n",
    "    def apply_all_audio_augs(self, audio, other_audio=None, sr=config.SR):\n",
    "        \"\"\"Applica tutte le augmentations audio in sequenza\"\"\"\n",
    "        # 1. Mixup (se disponibile altro audio)\n",
    "        mix_weight = 1.0\n",
    "        if other_audio is not None:\n",
    "            audio, mix_weight = self.apply_mixup_audio(audio, other_audio)\n",
    "        \n",
    "        # 2. Background Noise\n",
    "        audio = self.add_background_noise(audio)\n",
    "        \n",
    "        # 3. Random Filtering\n",
    "        audio = self.apply_random_filtering(audio, sr)\n",
    "        \n",
    "        return audio, mix_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40c4ed",
   "metadata": {
    "papermill": {
     "duration": 0.016747,
     "end_time": "2025-05-23T08:17:31.416102",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.399355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Dataset PyTorch per Dati Audio\n",
    "\n",
    "## Dataset PyTorch con Supporto Integrato per Data Augmentation\n",
    "\n",
    "Implementiamo due classi di dataset:\n",
    "- `RandomSegmentBirdDataset` \n",
    "  - Estrae segmenti randomici di 15 secondi in base alla durata delle registrazioni\n",
    "  - Applica le tecniche di data augmentation durante il caricamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20b040",
   "metadata": {
    "papermill": {
     "duration": 0.015093,
     "end_time": "2025-05-23T08:17:31.493206",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.478113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RandomSegmentBirdDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251d116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.434793Z",
     "iopub.status.busy": "2025-06-10T11:16:52.434512Z",
     "iopub.status.idle": "2025-06-10T11:16:52.454438Z",
     "shell.execute_reply": "2025-06-10T11:16:52.453785Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.434771Z"
    },
    "papermill": {
     "duration": 0.03675,
     "end_time": "2025-05-23T08:17:31.545532",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.508782",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomSegmentBirdDatasetSED(Dataset):\n",
    "    \"\"\"Dataset SED che passa audio crudo al modello\"\"\"\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, \n",
    "                 sed_augmentations=None, spec_augmentations=None):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.sed_augmentations = sed_augmentations\n",
    "        # Nota: spec_augmentations non serve qui, viene gestito nel modello\n",
    "        \n",
    "        print(\"✅ Dataset SED con audio crudo inizializzato...\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ File non trovato: {file_path}\")\n",
    "            # Ritorna audio crudo dummy, non spettrogramma\n",
    "            dummy_audio = torch.zeros(config.SR * config.DURATION, dtype=torch.float32)\n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return dummy_audio, dummy_label\n",
    "        \n",
    "        try:\n",
    "            # Carica audio con torchaudio\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            \n",
    "            # Converti a mono se necessario\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform[0:1]\n",
    "            \n",
    "            # Ricampiona se necessario\n",
    "            if sr != config.SR:\n",
    "                resampler = T.Resample(sr, config.SR)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # Converti in numpy per augmentations\n",
    "            audio_data = waveform.squeeze().numpy()\n",
    "            target_samples = int(config.SR * config.DURATION)\n",
    "            \n",
    "            # Gestione file corti\n",
    "            if len(audio_data) < target_samples:\n",
    "                audio_data = handle_short_audio(audio_data, target_samples)\n",
    "            \n",
    "            # Estrazione randomica\n",
    "            if len(audio_data) > target_samples:\n",
    "                max_start_idx = len(audio_data) - target_samples\n",
    "                start_idx = np.random.randint(0, max_start_idx)\n",
    "                audio_data = audio_data[start_idx:start_idx + target_samples]\n",
    "            else:\n",
    "                audio_data = audio_data[:target_samples]\n",
    "            \n",
    "            # Applica augmentations SED sull'audio crudo\n",
    "            mix_weight = 1.0\n",
    "            if self.sed_augmentations is not None and len(self.df) > 1:\n",
    "                # Mixup con altro audio (stesso codice di prima)\n",
    "                other_idx = np.random.randint(0, len(self.df))\n",
    "                if other_idx != idx:\n",
    "                    other_row = self.df.iloc[other_idx]\n",
    "                    other_file_path = os.path.join(self.audio_dir, other_row['filename'])\n",
    "                    \n",
    "                    if os.path.exists(other_file_path):\n",
    "                        try:\n",
    "                            other_waveform, other_sr = torchaudio.load(other_file_path)\n",
    "                            if other_waveform.shape[0] > 1:\n",
    "                                other_waveform = other_waveform[0:1]\n",
    "                            if other_sr != config.SR:\n",
    "                                other_resampler = T.Resample(other_sr, config.SR)\n",
    "                                other_waveform = other_resampler(other_waveform)\n",
    "                            \n",
    "                            other_audio_data = other_waveform.squeeze().numpy()\n",
    "                            if len(other_audio_data) < target_samples:\n",
    "                                other_audio_data = handle_short_audio(other_audio_data, target_samples)\n",
    "                            \n",
    "                            if len(other_audio_data) > target_samples:\n",
    "                                other_start_idx = np.random.randint(0, len(other_audio_data) - target_samples)\n",
    "                                other_audio_data = other_audio_data[other_start_idx:other_start_idx + target_samples]\n",
    "                            else:\n",
    "                                other_audio_data = other_audio_data[:target_samples]\n",
    "                            \n",
    "                            audio_data, mix_weight = self.sed_augmentations.apply_all_audio_augs(\n",
    "                                audio_data, other_audio_data\n",
    "                            )\n",
    "                        except:\n",
    "                            audio_data, mix_weight = self.sed_augmentations.apply_all_audio_augs(audio_data)\n",
    "                    else:\n",
    "                        audio_data, mix_weight = self.sed_augmentations.apply_all_audio_augs(audio_data)\n",
    "                else:\n",
    "                    audio_data, mix_weight = self.sed_augmentations.apply_all_audio_augs(audio_data)\n",
    "            \n",
    "            # CRUCIALE: Ritorna audio crudo 1D, non spettrogramma\n",
    "            audio_tensor = torch.tensor(audio_data, dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore nel caricamento di {file_path}: {e}\")\n",
    "            audio_tensor = torch.zeros(config.SR * config.DURATION, dtype=torch.float32)\n",
    "            mix_weight = 1.0\n",
    "        \n",
    "        # Gestione etichette con mixup\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if mix_weight < 1.0 and 'other_idx' in locals() and other_idx != idx:\n",
    "            try:\n",
    "                other_label_tensor = torch.tensor(self.labels[other_idx], dtype=torch.float32)\n",
    "                label_tensor = mix_weight * label_tensor + (1 - mix_weight) * other_label_tensor\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return audio_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bb5aa",
   "metadata": {
    "papermill": {
     "duration": 0.015303,
     "end_time": "2025-05-23T08:17:31.577368",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.562065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creazione dataset e dataloader con RandomSegmentBirdDataset\n",
    "\n",
    "### Creazione dei DataLoader con Augmentation\n",
    "\n",
    "In questa sezione:\n",
    "1. Applichiamo il bilanciamento strategico al dataset di training\n",
    "2. Creiamo l'istanza di AudioAugmentations con le probabilità ottimali\n",
    "3. Configuriamo i dataloader con le funzioni di collate personalizzate\n",
    "4. Analizziamo la distribuzione dei segmenti nel dataset risultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930d066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.456116Z",
     "iopub.status.busy": "2025-06-10T11:16:52.455324Z",
     "iopub.status.idle": "2025-06-10T11:16:52.701517Z",
     "shell.execute_reply": "2025-06-10T11:16:52.700870Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.456093Z"
    },
    "papermill": {
     "duration": 2847.998728,
     "end_time": "2025-05-23T09:04:59.592725",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.593997",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Applica bilanciamento\n",
    "# Applica bilanciamento\n",
    "print(\"\\n🔄 Bilanciamento del dataset di training...\")\n",
    "X_train_df_balanced, y_train_one_hot_balanced = create_balanced_dataset_df(\n",
    "    X_train_df, \n",
    "    y_train_one_hot,\n",
    "    abundant_class_threshold=100,\n",
    "    remove_percentage=0.5\n",
    ")\n",
    "\n",
    "# ⚠️ CREA LE NUOVE AUGMENTATIONS SED DEI VINCITORI\n",
    "print(\"🔄 Inizializzazione augmentations SED dei vincitori...\")\n",
    "\n",
    "sed_augmentations = SEDAugmentations(\n",
    "    p_mixup=0.6,           # Come specificato dagli utenti\n",
    "    p_background_noise=0.4,\n",
    "    p_random_filtering=0.5,\n",
    "    p_spec_freq=0.4,       # Come specificato\n",
    "    p_spec_time=0.4        # Come specificato\n",
    ")\n",
    "\n",
    "# SpecAugment separato per il modello\n",
    "spec_augmentations = SEDAugmentations(\n",
    "    p_mixup=0.0,           # Già fatto nel dataset\n",
    "    p_background_noise=0.0,\n",
    "    p_random_filtering=0.0,\n",
    "    p_spec_freq=0.3,       # Solo SpecAug nel modello\n",
    "    p_spec_time=0.3\n",
    ")\n",
    "\n",
    "# Validation senza augmentations aggressive\n",
    "val_sed_augmentations = SEDAugmentations(\n",
    "    p_mixup=0.0,\n",
    "    p_background_noise=0.1,  # Solo un po' di rumore\n",
    "    p_random_filtering=0.0,\n",
    "    p_spec_freq=0.0,\n",
    "    p_spec_time=0.0\n",
    ")\n",
    "\n",
    "# ⚠️ DATASET SED CON AUGMENTATIONS DEI VINCITORI\n",
    "print(\"🔄 Creazione dataset SED con augmentations...\")\n",
    "\n",
    "train_dataset = RandomSegmentBirdDatasetSED(\n",
    "    X_train_df_balanced, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_train_one_hot_balanced,\n",
    "    sed_augmentations=sed_augmentations,\n",
    "    spec_augmentations=spec_augmentations\n",
    ")\n",
    "\n",
    "val_dataset = RandomSegmentBirdDatasetSED(\n",
    "    X_val_df, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_val_one_hot,\n",
    "    sed_augmentations=val_sed_augmentations,\n",
    "    spec_augmentations=None  # No SpecAug in validation\n",
    ")\n",
    "\n",
    "# DataLoader semplificati per SED\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    "    # NO collate_fn - SED gestisce tutto internamente\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Training batches per epoca: {len(train_loader)}\")\n",
    "print(f\"✅ Validation batches per epoca: {len(val_loader)}\")\n",
    "print(\"🎯 Dataset SED con augmentations dei vincitori configurato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73cddf",
   "metadata": {
    "papermill": {
     "duration": 0.015869,
     "end_time": "2025-05-23T09:04:59.625046",
     "exception": false,
     "start_time": "2025-05-23T09:04:59.609177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Definizione del Modello CNN\n",
    "\n",
    "## Modello EfficientNet con Head Personalizzata\n",
    "\n",
    "Implementiamo un modello basato su EfficientNet-B0 preaddestrato, con:\n",
    "- Supporto per input a singolo canale (spettrogrammi Mel)\n",
    "- Testa di classificazione personalizzata con dropout e normalizzazione batch\n",
    "- Parametri differenziati per l'ottimizzazione\n",
    "- Gestione automatica dei checkpoint e dei pesi preaddestrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Inizializza i layer come in Bird25\"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"Inizializza batch normalization come in Bird25\"\"\"\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    \"\"\"Attention Block V2 - IDENTICO a Bird25\"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22ddec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.702431Z",
     "iopub.status.busy": "2025-06-10T11:16:52.702219Z",
     "iopub.status.idle": "2025-06-10T11:16:52.711839Z",
     "shell.execute_reply": "2025-06-10T11:16:52.711274Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.702406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WeightedFocalBCELoss(nn.Module):\n",
    "    def __init__(self, class_counts, gamma=2, alpha_rare=3.0, alpha_common=0.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # MIGLIORAMENTO: Calcolo pesi più aggressivo per sbilanciamento estremo\n",
    "        total_samples = sum(class_counts.values())\n",
    "        self.class_weights = {}\n",
    "        \n",
    "        for species, count in class_counts.items():\n",
    "            frequency = count / total_samples\n",
    "            \n",
    "            # CORRETTO: Soglie più aggressive per sbilanciamento estremo\n",
    "            if frequency < 0.002:  # < 0.2% → peso molto alto (classi estremamente rare)\n",
    "                weight = alpha_rare * 3  # 9.0 invece di 3.0\n",
    "            elif frequency < 0.005:  # < 0.5% → peso molto alto\n",
    "                weight = alpha_rare * 2  # 6.0 invece di 3.0\n",
    "            elif frequency < 0.01:   # < 1% → peso alto  \n",
    "                weight = alpha_rare      # 3.0\n",
    "            elif frequency > 0.05:   # > 5% → peso basso\n",
    "                weight = alpha_common    # 0.5\n",
    "            elif frequency > 0.1:    # > 10% → peso molto basso\n",
    "                weight = alpha_common * 0.3  # 0.15 invece di 0.25\n",
    "            else:\n",
    "                # Peso inversamente proporzionale con smoothing migliorato\n",
    "                weight = min(1.0 / (frequency + 0.0005), 4.0)  # Cap più alto per classi rare\n",
    "        \n",
    "            self.class_weights[species] = weight\n",
    "        \n",
    "        # Converte in tensore per GPU con ordine corretto delle specie\n",
    "        weight_tensor = torch.tensor([self.class_weights.get(species, 1.0) \n",
    "                                    for species in sorted(class_counts.keys())], dtype=torch.float32)\n",
    "        self.register_buffer('alpha_weights', weight_tensor)\n",
    "        \n",
    "        # DEBUG: Stampa statistiche sui pesi\n",
    "        weights_stats = torch.tensor(list(self.class_weights.values()))\n",
    "        print(f\"Pesi calcolati - Min: {weights_stats.min():.2f}, Max: {weights_stats.max():.2f}, Mean: {weights_stats.mean():.2f}\")\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # BCE Loss base\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Probabilità predette (applica sigmoid per ottenere pt corretto)\n",
    "        pt = torch.sigmoid(inputs)\n",
    "        # Calcola pt effettivo: pt per target=1, (1-pt) per target=0\n",
    "        pt = targets * pt + (1 - targets) * (1 - pt)\n",
    "        \n",
    "        # Componente Focal\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Applica pesi per classe\n",
    "        alpha_weights = self.alpha_weights.to(device).unsqueeze(0)\n",
    "        weighted_focal_loss = alpha_weights * focal_weight * bce_loss\n",
    "        \n",
    "        # MIGLIORAMENTO: Media pesata con gestione migliorata degli sbilanciamenti\n",
    "        # Peso alto per esempi positivi delle classi rare, peso basso per negativi\n",
    "        positive_weights = targets * alpha_weights\n",
    "        negative_weights = (1 - targets) * 0.05  # Peso molto basso per negativi\n",
    "        \n",
    "        sample_weights = positive_weights + negative_weights\n",
    "        total_weight = sample_weights.sum()\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            return (weighted_focal_loss * sample_weights).sum() / total_weight\n",
    "        else:\n",
    "            return weighted_focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb93042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.712871Z",
     "iopub.status.busy": "2025-06-10T11:16:52.712634Z",
     "iopub.status.idle": "2025-06-10T11:16:53.361610Z",
     "shell.execute_reply": "2025-06-10T11:16:53.360932Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.712850Z"
    },
    "papermill": {
     "duration": 64.446572,
     "end_time": "2025-05-23T09:06:04.087781",
     "exception": false,
     "start_time": "2025-05-23T09:04:59.641209",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFSEDModel(nn.Module):\n",
    "    \"\"\"Modello SED IDENTICO a Bird25\"\"\"\n",
    "    def __init__(self, num_classes, model_name='efficientnet_b0', pretrained=True, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Configura parametri come Bird25\n",
    "        self.cfg = {\n",
    "            'SR': config.SR,\n",
    "            'hop_length': config.HOP_LENGTH,\n",
    "            'n_mels': config.N_MELS,\n",
    "            'f_min': config.FMIN,\n",
    "            'f_max': config.FMAX,\n",
    "            'n_fft': config.N_FFT,\n",
    "            'normal': 80,\n",
    "            'infer_duration': config.DURATION,\n",
    "            'duration_train': config.DURATION\n",
    "        }\n",
    "        \n",
    "        # Batch normalization per input (come Bird25)\n",
    "        self.bn0 = nn.BatchNorm2d(config.N_MELS)\n",
    "        \n",
    "        # ✅ CORREZIONE: Backbone con 3 canali di input come Bird25\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=3,  # ✅ SEMPRE 3 CANALI come Bird25\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "        \n",
    "        # Rimuovi gli ultimi 2 layer (come Bird25)\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Per EfficientNet sempre così\n",
    "        backbone_out = self.backbone.classifier.in_features\n",
    "        \n",
    "        # FC layer (come Bird25)\n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        \n",
    "        # Attention block (come Bird25)\n",
    "        self.att_block = AttBlockV2(backbone_out, num_classes, activation=\"sigmoid\")\n",
    "        \n",
    "        # Trasformazione Mel (come Bird25)\n",
    "        self.melspec_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        \n",
    "        # Trasformazione a dB (come Bird25)\n",
    "        self.db_transform = T.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        \"\"\"Inizializza i pesi\"\"\"\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        \"\"\"Estrae features come Bird25\"\"\"\n",
    "        # x: (batch_size, channels, n_mels, time_frames)\n",
    "        x = x.permute((0, 1, 3, 2))  # → (batch_size, channels, time_frames, n_mels)\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)  # → (batch_size, n_mels, time_frames, channels)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)  # → (batch_size, channels, time_frames, n_mels)\n",
    "        \n",
    "        x = x.transpose(2, 3)  # → (batch_size, channels, n_mels, time_frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # Channel smoothing come Bird25\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        return x, frames_num\n",
    "\n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "        \"\"\"Trasforma audio in spettrogramma Mel\"\"\"\n",
    "        audio = audio.float()\n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "        \n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"✅ CORREZIONE: Forward pass con conversione a 3 canali\"\"\"\n",
    "        # x è audio crudo (batch_size, audio_samples)\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Trasforma in spettrogramma Mel\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)  # (batch_size, n_mels, time_frames)\n",
    "        \n",
    "        # ✅ CORREZIONE CRITICA: Converti 1 canale in 3 canali\n",
    "        if len(x.shape) == 3:  # (batch_size, n_mels, time_frames)\n",
    "            x = x.unsqueeze(1)  # (batch_size, 1, n_mels, time_frames)\n",
    "        \n",
    "        # ✅ CONVERSIONE DA 1 A 3 CANALI (come Bird25)\n",
    "        if x.shape[1] == 1:  # Se è single channel\n",
    "            x = x.repeat(1, 3, 1, 1)  # Replica su 3 canali: (batch_size, 3, n_mels, time_frames)\n",
    "        \n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        \n",
    "        # Calcola logit come Bird25\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        \"\"\"Inference con TTA come Bird25\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        \n",
    "        # ✅ CONVERSIONE DA 1 A 3 CANALI anche per inference\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        x, _ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        \n",
    "        start = 0\n",
    "        end = feat_time\n",
    "        pred = self.attention_infer(start, end, x, time_att)\n",
    "        \n",
    "        if feat_time > tta_delta * 2:\n",
    "            start_minus = max(0, start - tta_delta)\n",
    "            end_minus = end - tta_delta\n",
    "            pred_minus = self.attention_infer(start_minus, end_minus, x, time_att)\n",
    "            \n",
    "            start_plus = start + tta_delta\n",
    "            end_plus = min(feat_time, end + tta_delta)\n",
    "            pred_plus = self.attention_infer(start_plus, end_plus, x, time_att)\n",
    "            \n",
    "            pred = 0.5 * pred + 0.25 * pred_minus + 0.25 * pred_plus\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    def attention_infer(self, start, end, x, time_att):\n",
    "        \"\"\"Helper per inference con attention come Bird25\"\"\"\n",
    "        feat = x[:, :, start:end]\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        return framewise_pred_max\n",
    "\n",
    "# Controlla checkpoint esistenti\n",
    "has_previous_checkpoint = False\n",
    "if config.environment == 'kaggle':\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(latest_checkpoint)\n",
    "elif config.environment == 'colab':\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(drive_checkpoint)\n",
    "else:\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    has_previous_checkpoint = os.path.exists(local_checkpoint)\n",
    "\n",
    "# ⚠️ INIZIALIZZA IL NUOVO MODELLO MIGLIORATO\n",
    "print(\"🚀 Inizializzazione del nuovo modello EfficientNet SED...\")\n",
    "model = BirdCLEFSEDModel(\n",
    "    num_classes=config.N_CLASSES, \n",
    "    model_name='efficientnet_b0',\n",
    "    pretrained=not has_previous_checkpoint,\n",
    "    in_channels=3  # ✅ SEMPRE 3 canali per EfficientNet\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Aggiungi SpecAug al modello\n",
    "model.spec_augmentations = spec_augmentations\n",
    "\n",
    "print(\"✅ Modello SED corretto e pronto per il training!\")\n",
    "\n",
    "# Conta parametri\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✅ Modello SED caricato su {config.DEVICE}\")\n",
    "print(f\"📊 Parametri totali: {total_params:,}\")\n",
    "print(f\"🎯 Parametri trainabili: {trainable_params:,}\")\n",
    "\n",
    "# Ottimizzatore per SED\n",
    "def get_sed_optimizer(model, lr_backbone=5e-5, lr_head=1e-3):\n",
    "    \"\"\"Ottimizzatore per SED con LR differenziati\"\"\"\n",
    "    backbone_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder' in name or 'backbone' in name:\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    head_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not ('encoder' in name or 'backbone' in name):\n",
    "            head_params.append(param)\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': lr_backbone, 'weight_decay': 1e-4},\n",
    "        {'params': head_params, 'lr': lr_head, 'weight_decay': 1e-3}\n",
    "    ])\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Calcola frequenze delle classi per la loss\n",
    "primary_species_count = train_df['primary_label'].value_counts().to_dict()\n",
    "\n",
    "# ⚠️ NUOVA LOSS WeightedFocalBCELoss con miglioramenti\n",
    "criterion = WeightedFocalBCELoss(\n",
    "    class_counts=primary_species_count,\n",
    "    gamma=1.5,\n",
    "    alpha_rare=2.5,\n",
    "    alpha_common=0.6\n",
    ")\n",
    "\n",
    "# Ottimizzatore e scheduler\n",
    "optimizer = get_sed_optimizer(model)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.EPOCHS,\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "print(\"🎯 Setup completo! Pronto per il training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffabba6",
   "metadata": {
    "papermill": {
     "duration": 0.016226,
     "end_time": "2025-05-23T09:06:04.121756",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.105530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Addestramento e Validazione del Modello\n",
    "\n",
    "## Funzione di Training con Supporto per Checkpoint\n",
    "\n",
    "La funzione di training implementa:\n",
    "- Caricamento automatico dei checkpoint precedenti\n",
    "- Early stopping basato sulle performance di validation\n",
    "- Salvataggio periodico dei checkpoint e del miglior modello\n",
    "- Supporto per scheduler di learning rate (CosineAnnealingLR)\n",
    "- Visualizzazione delle curve di loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7244c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:53.362828Z",
     "iopub.status.busy": "2025-06-10T11:16:53.362540Z",
     "iopub.status.idle": "2025-06-10T11:16:53.389769Z",
     "shell.execute_reply": "2025-06-10T11:16:53.388983Z",
     "shell.execute_reply.started": "2025-06-10T11:16:53.362804Z"
    },
    "papermill": {
     "duration": 0.058751,
     "end_time": "2025-05-23T09:06:04.196779",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.138028",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                model_save_path=None, model_load_path=None, patience=7,\n",
    "                resume_training=True, scheduler=None):\n",
    "    \"\"\"\n",
    "    Addestra il modello e valuta su validation set con supporto per checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da addestrare\n",
    "        train_loader: DataLoader per dati di training\n",
    "        val_loader: DataLoader per dati di validation\n",
    "        criterion: Funzione di loss\n",
    "        optimizer: Ottimizzatore\n",
    "        epochs: Numero di epoche di training\n",
    "        device: Device per l'addestramento ('cuda' o 'cpu')\n",
    "        model_save_path: Path dove salvare il modello addestrato\n",
    "        model_load_path: Path da cui caricare un modello pre-addestrato\n",
    "        patience: Numero di epoche senza miglioramento prima di terminare l'addestramento\n",
    "        resume_training: Se True, riprende il training da un checkpoint (se disponibile)\n",
    "        scheduler: Learning rate scheduler\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, total_training_time)\n",
    "    \"\"\"\n",
    "    # Directory per i checkpoint in base all'ambiente\n",
    "    checkpoint_dir = None\n",
    "    drive_mounted = False\n",
    "    \n",
    "    # Configura la directory per i checkpoint a seconda dell'ambiente\n",
    "    if config.environment == 'colab':\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            # Controlla se il drive è già montato\n",
    "            if not os.path.exists('/content/drive'):\n",
    "                print(\"Montaggio di Google Drive...\")\n",
    "                drive.mount('/content/drive')\n",
    "                print(\"Google Drive montato con successo.\")\n",
    "            \n",
    "            # Crea directory per i checkpoint se non esiste\n",
    "            checkpoint_dir = '/content/drive/MyDrive/birdclef_checkpoints'\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            print(f\"Directory per i checkpoint creata su Google Drive: {checkpoint_dir}\")\n",
    "            \n",
    "            # Aggiorna il percorso di salvataggio per usare Google Drive\n",
    "            if model_save_path:\n",
    "                filename = os.path.basename(model_save_path)\n",
    "                model_save_path = os.path.join(checkpoint_dir, filename)\n",
    "                print(f\"Il modello sarà salvato in: {model_save_path}\")\n",
    "            \n",
    "            drive_mounted = True\n",
    "        except ImportError:\n",
    "            print(\"Errore: Non riesco ad accedere a Google Drive. Continuo senza persistenza.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
    "            print(\"Continuo senza persistenza su Drive.\")\n",
    "    elif config.environment == 'kaggle':\n",
    "        # In Kaggle, usa la directory di working\n",
    "        checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in Kaggle: {checkpoint_dir}\")\n",
    "    else:\n",
    "        # In locale, usa la directory 'checkpoints' nell'OUTPUT_DIR\n",
    "        checkpoint_dir = os.path.join(config.OUTPUT_DIR, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in locale: {checkpoint_dir}\")\n",
    "    \n",
    "    # Inizializzazione variabili\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    total_training_time = 0\n",
    "    start_epoch = 0\n",
    "    needs_training = True\n",
    "    checkpoint_exists = False\n",
    "    model_loaded = False\n",
    "    \n",
    "    # Verifica se esiste un modello pre-addestrato da caricare\n",
    "    if model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Modello trovato in {model_load_path}. Tentativo di caricamento...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            \n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            print(\"Modello caricato con successo.\")\n",
    "            model_loaded = True\n",
    "            needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "            print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            needs_training = True\n",
    "    else:\n",
    "        print(f\"Modello non trovato in {model_load_path}.\")\n",
    "    \n",
    "    # Cerca un checkpoint SOLO se il caricamento del modello è fallito E resume_training è True\n",
    "    if needs_training and resume_training and checkpoint_dir and not model_loaded:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "        if os.path.exists(latest_checkpoint):\n",
    "            print(f\"Trovato checkpoint in {latest_checkpoint}. Tentativo di caricamento...\")\n",
    "            try:\n",
    "                checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "                \n",
    "                # Verifica che sia un checkpoint compatibile prima di caricarlo\n",
    "                if isinstance(checkpoint, dict) and 'epoch' in checkpoint:\n",
    "                    try:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        start_epoch = checkpoint['epoch'] + 1\n",
    "                        train_losses = checkpoint['train_losses']\n",
    "                        val_losses = checkpoint['val_losses']\n",
    "                        best_val_loss = checkpoint['best_val_loss']\n",
    "                        epochs_without_improvement = checkpoint['epochs_without_improvement']\n",
    "                        total_training_time = checkpoint.get('total_training_time', 0)\n",
    "                        \n",
    "                        # Ricrea lo scheduler con lo stato salvato se presente\n",
    "                        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "                            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                        \n",
    "                        print(f\"Checkpoint caricato con successo (epoca {start_epoch-1})\")\n",
    "                        print(f\"Si riparte dall'epoca {start_epoch}/{epochs}\")\n",
    "                        \n",
    "                        if start_epoch >= epochs:\n",
    "                            needs_training = False\n",
    "                        \n",
    "                        checkpoint_exists = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Il checkpoint non è compatibile con il modello attuale: {e}\")\n",
    "                        print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il caricamento del checkpoint: {e}\")\n",
    "                print(\"Si procederà con il training da zero.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Loop di training sulle epoche (inizia da start_epoch)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            # Aggiornamento scheduler - modificato per CosineAnnealingLR\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_val_loss)  # Per ReduceLROnPlateau\n",
    "                else:\n",
    "                    scheduler.step()  # Per CosineAnnealingLR è step() senza parametri\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            total_training_time += epoch_duration\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Duration: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            # Salvataggio checkpoint per ogni epoca (in qualsiasi ambiente)\n",
    "            if checkpoint_dir:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"birdclef_epoch_{epoch+1}.pth\")\n",
    "                \n",
    "                # Salva checkpoint completo con tutte le informazioni di stato\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_without_improvement': epochs_without_improvement,\n",
    "                    'total_training_time': total_training_time\n",
    "                }\n",
    "                \n",
    "                # Salva anche lo stato dello scheduler se esiste\n",
    "                if scheduler is not None:\n",
    "                    checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                print(f\"Checkpoint completo salvato in {checkpoint_path}\")\n",
    "                \n",
    "                # Aggiorna anche il checkpoint più recente (sovrascrive)\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "            \n",
    "            # Early stopping\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                # Salva il miglior modello separatamente\n",
    "                if model_save_path:\n",
    "                    best_path = model_save_path.replace('.pth', '_best.pth')\n",
    "                    \n",
    "                    # Salva checkpoint completo\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_losses': train_losses,\n",
    "                        'val_losses': val_losses,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }\n",
    "                    \n",
    "                    # Salva anche lo stato dello scheduler\n",
    "                    if scheduler is not None:\n",
    "                        best_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                    \n",
    "                    torch.save(best_checkpoint, best_path)\n",
    "                    print(f\"Salvato miglior modello in {best_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping attivato! Nessun miglioramento per {patience} epoche consecutive.\")\n",
    "                break\n",
    "        \n",
    "        end_time_total = time.time()\n",
    "        if checkpoint_exists:\n",
    "            total_training_time += (end_time_total - start_time_total)\n",
    "        else:\n",
    "            total_training_time = end_time_total - start_time_total\n",
    "            \n",
    "        print(f\"\\nTraining terminato in {total_training_time/60:.2f} minuti totali\")\n",
    "        \n",
    "        # Salva il modello finale\n",
    "        if model_save_path:\n",
    "            final_checkpoint = {\n",
    "                'epoch': epochs-1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'total_training_time': total_training_time\n",
    "            }\n",
    "            \n",
    "            # Salva anche lo stato dello scheduler\n",
    "            if scheduler is not None:\n",
    "                final_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "            torch.save(final_checkpoint, model_save_path)\n",
    "            print(f\"Modello finale salvato in {model_save_path}\")\n",
    "    else:\n",
    "        print(\"Training non necessario: modello già caricato o training ripreso e completato.\")\n",
    "    \n",
    "    # Visualizza le curve di loss\n",
    "    if train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Curve di Loss di Training e Validation')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Salva il grafico\n",
    "        if checkpoint_dir:\n",
    "            plt_path = os.path.join(checkpoint_dir, 'loss_curves.png')\n",
    "            plt.savefig(plt_path)\n",
    "            print(f\"Grafico delle curve di loss salvato in {plt_path}\")\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e87bd",
   "metadata": {
    "papermill": {
     "duration": 0.016154,
     "end_time": "2025-05-23T09:06:04.229449",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.213295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configurazione e Avvio del Training\n",
    "\n",
    "Configuriamo e avviamo il training del modello:\n",
    "- Individuazione automatica dei checkpoint precedenti\n",
    "- Inizializzazione dell'ottimizzatore e dello scheduler\n",
    "- Avvio del training con i parametri ottimizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dabb8",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z",
     "iopub.execute_input": "2025-06-10T11:16:53.391196Z",
     "iopub.status.busy": "2025-06-10T11:16:53.390689Z"
    },
    "papermill": {
     "duration": 0.847329,
     "end_time": "2025-05-23T09:06:05.092857",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.245528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    # Directory per i checkpoint in Kaggle\n",
    "    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Verifica se esiste un checkpoint precedente\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        model_load_path = latest_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente in {latest_checkpoint}\")\n",
    "    else:\n",
    "        # Usa un modello base precaricato se disponibile\n",
    "        model_load_path = \"/kaggle/input/efficientnet_data_aug/pytorch/default/1/birdclef_efficientNET_dataAug_timm_best.pth\"\n",
    "        \n",
    "    # Imposta il percorso di salvataggio\n",
    "    model_save_path = \"/kaggle/working/birdclef_efficientNET_SED.pth\"\n",
    "    \n",
    "elif config.environment == 'colab':\n",
    "    # Per Colab, verifica se esiste un checkpoint su Drive\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(drive_checkpoint):\n",
    "        model_load_path = drive_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente su Drive: {drive_checkpoint}\")\n",
    "    else:\n",
    "        # Altrimenti usa un modello base se disponibile\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "    \n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    if os.path.exists(local_checkpoint):\n",
    "        model_load_path = local_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente: {local_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Addestra il modello con CosineAnnealingLR scheduler\n",
    "train_losses, val_losses, training_time = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,  # Passa lo scheduler\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path,\n",
    "    resume_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035fd7",
   "metadata": {
    "papermill": {
     "duration": 0.017078,
     "end_time": "2025-05-23T09:06:05.129361",
     "exception": false,
     "start_time": "2025-05-23T09:06:05.112283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Generazione della Submission\n",
    "\n",
    "## Generazione della Submission\n",
    "\n",
    "Implementiamo la funzione per generare le predizioni sui file di test:\n",
    "- Funzione di temporal smoothing\n",
    "- Caricamento e segmentazione delle soundscape di test\n",
    "- Estrazione di spettrogrammi Mel da ciascun segmento\n",
    "- Generazione delle predizioni con il modello addestrato\n",
    "- Creazione del file di submission nel formato richiesto dalla competizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_power_to_low_ranked_cols_improved(predictions_df, top_k=30, exponent=2):\n",
    "    \"\"\"\n",
    "    ✅ MIGLIORATA: Applica trasformazione power come Bird25\n",
    "    \"\"\"\n",
    "    print(f\"Applicando power adjustment migliorato (top_k={top_k}, exponent={exponent})...\")\n",
    "    \n",
    "    # Salva row_id per dopo\n",
    "    row_ids = predictions_df['row_id'].values\n",
    "    \n",
    "    # Estrai solo le colonne delle specie\n",
    "    species_cols = [col for col in predictions_df.columns if col != 'row_id']\n",
    "    species_data = predictions_df[species_cols].values\n",
    "    \n",
    "    # ✅ MIGLIORAMENTO: Identifica colonne con ranking più basso per max value\n",
    "    max_values_per_col = np.max(species_data, axis=0)\n",
    "    tail_col_indices = np.argsort(-max_values_per_col)[top_k:]  # Indici delle colonne \"tail\"\n",
    "    \n",
    "    # ✅ MIGLIORAMENTO: Applica trasformazione power solo dove necessario\n",
    "    species_data[:, tail_col_indices] = np.power(\n",
    "        np.clip(species_data[:, tail_col_indices], 0.001, 1.0),  # Evita valori 0\n",
    "        exponent\n",
    "    )\n",
    "    \n",
    "    # Ricostruisci il DataFrame\n",
    "    result_df = pd.DataFrame(species_data, columns=species_cols)\n",
    "    result_df.insert(0, 'row_id', row_ids)\n",
    "    \n",
    "    print(f\"Power adjustment applicato a {len(tail_col_indices)} specie con ranking basso\")\n",
    "    return result_df\n",
    "\n",
    "def enhanced_temporal_smoothing_v2(predictions_df, weights=[0.1, 0.8, 0.1]):\n",
    "    \"\"\"\n",
    "    ✅ MIGLIORATA: Smoothing temporale più robusto\n",
    "    \"\"\"\n",
    "    print(\"Applicando enhanced temporal smoothing v2...\")\n",
    "    \n",
    "    result_df = predictions_df.copy()\n",
    "    cols = [col for col in result_df.columns if col != 'row_id']\n",
    "    \n",
    "    # ✅ MIGLIORAMENTO: Raggruppa per file più robusto\n",
    "    groups = result_df['row_id'].str.rsplit('_', n=1).str[0]\n",
    "    \n",
    "    for group in np.unique(groups):\n",
    "        mask = groups == group\n",
    "        sub_group = result_df[mask].copy()\n",
    "        \n",
    "        if len(sub_group) <= 1:\n",
    "            continue\n",
    "            \n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        \n",
    "        # ✅ MIGLIORAMENTO: Smoothing adattivo\n",
    "        if len(predictions) >= 3:\n",
    "            # Smoothing standard per segmenti interni\n",
    "            for i in range(1, len(predictions) - 1):\n",
    "                new_predictions[i] = (predictions[i-1] * weights[0] + \n",
    "                                    predictions[i] * weights[1] + \n",
    "                                    predictions[i+1] * weights[2])\n",
    "        \n",
    "        # ✅ MIGLIORAMENTO: Gestione bordi più sofisticata\n",
    "        if len(predictions) > 1:\n",
    "            # Primo segmento\n",
    "            new_predictions[0] = predictions[0] * 0.85 + predictions[1] * 0.15\n",
    "            # Ultimo segmento\n",
    "            new_predictions[-1] = predictions[-1] * 0.85 + predictions[-2] * 0.15\n",
    "        \n",
    "        result_df.loc[mask, cols] = new_predictions\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee4056",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z"
    },
    "papermill": {
     "duration": 0.085292,
     "end_time": "2025-05-23T09:06:05.231290",
     "exception": false,
     "start_time": "2025-05-23T09:06:05.145998",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_submission_sed_corrected(model, device=config.DEVICE, overlap=2.5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    ✅ CORRETTA: Genera submission usando SOLO il modello SED con pipeline corretta\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Set seed per riproducibilità\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Percorso dei test soundscapes\n",
    "    test_soundscape_path = config.TEST_SOUNDSCAPES_DIR\n",
    "    test_soundscapes = [os.path.join(test_soundscape_path, afile) \n",
    "                        for afile in sorted(os.listdir(test_soundscape_path)) \n",
    "                        if afile.endswith('.ogg')]\n",
    "    \n",
    "    print(f\"Elaborazione di {len(test_soundscapes)} file soundscape con overlap di {overlap}s...\")\n",
    "    \n",
    "    # Dizionario per accumulare predizioni per ogni target timestamp\n",
    "    accumulated_predictions = {}\n",
    "    \n",
    "    for soundscape in tqdm(test_soundscapes, desc=\"Elaborazione soundscapes\"):\n",
    "        # ✅ USA TORCHAUDIO COME NEL TRAINING\n",
    "        waveform, sr = torchaudio.load(soundscape)\n",
    "        \n",
    "        # Converti a mono se necessario\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1]\n",
    "        \n",
    "        # Ricampiona se necessario\n",
    "        if sr != config.SR:\n",
    "            resampler = T.Resample(sr, config.SR)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        file_name = os.path.basename(soundscape).split('.')[0]\n",
    "        audio_duration = waveform.shape[1] / config.SR\n",
    "        \n",
    "        # ✅ CORREZIONE: Genera timestamp ogni 5 secondi per tutto il file\n",
    "        target_timestamps = list(range(5, int(audio_duration) + 1, 5))\n",
    "        print(f\"File {file_name}: durata {audio_duration:.1f}s, {len(target_timestamps)} timestamp da predire\")\n",
    "        \n",
    "        # Parametri per sliding window di predizione\n",
    "        window_size = int(config.SR * config.DURATION)  # 5 secondi di finestra\n",
    "        step_size = int(config.SR * overlap)  # Passo di 2.5 secondi\n",
    "        \n",
    "        # Per ogni timestamp target (ogni 5 secondi)\n",
    "        for target_time in target_timestamps:\n",
    "            row_id = f\"{file_name}_{target_time}\"\n",
    "            \n",
    "            # Raccogli tutte le predizioni che coprono questo timestamp\n",
    "            predictions_for_timestamp = []\n",
    "            weights_for_timestamp = []\n",
    "            \n",
    "            # Trova tutte le finestre che coprono questo timestamp\n",
    "            for start_idx in range(0, waveform.shape[1], step_size):\n",
    "                segment_start_time = start_idx / config.SR\n",
    "                segment_end_time = segment_start_time + config.DURATION\n",
    "                \n",
    "                # Se questo timestamp cade dentro questa finestra\n",
    "                if segment_start_time <= target_time <= segment_end_time:\n",
    "                    # Estrai segmento di 5 secondi\n",
    "                    segment = waveform[:, start_idx:start_idx + window_size]\n",
    "                    \n",
    "                    # Se il segmento è troppo corto, padda\n",
    "                    if segment.shape[1] < window_size:\n",
    "                        padding = window_size - segment.shape[1]\n",
    "                        segment = torch.nn.functional.pad(segment, (0, padding))\n",
    "                    \n",
    "                    # ✅ PASSA AUDIO CRUDO DIRETTAMENTE AL MODELLO SED\n",
    "                    audio_tensor = torch.tensor(segment.squeeze().numpy(), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # ✅ USA IL METODO INFER DEL MODELLO SED\n",
    "                        output = model.infer(audio_tensor)  # Il modello gestisce internamente tutto\n",
    "                        scores = output.cpu().numpy()\n",
    "                    \n",
    "                    # Calcola il peso in base alla posizione del timestamp nella finestra\n",
    "                    relative_position = (target_time - segment_start_time) / config.DURATION\n",
    "                    # Peso massimo al centro (0.5), minimo ai bordi (0, 1)\n",
    "                    distance_from_center = abs(relative_position - 0.5)\n",
    "                    weight = 1.0 - distance_from_center * (1.0 - alpha)\n",
    "                    \n",
    "                    predictions_for_timestamp.append(scores)\n",
    "                    weights_for_timestamp.append(weight)\n",
    "            \n",
    "            # Combina le predizioni pesate per questo timestamp\n",
    "            if predictions_for_timestamp:\n",
    "                predictions_array = np.array(predictions_for_timestamp)\n",
    "                weights_array = np.array(weights_for_timestamp)\n",
    "                \n",
    "                # Media pesata\n",
    "                weighted_prediction = np.average(predictions_array, axis=0, weights=weights_array)\n",
    "                accumulated_predictions[row_id] = weighted_prediction\n",
    "            else:\n",
    "                # Se nessuna finestra copre questo timestamp, usa predizioni zero\n",
    "                accumulated_predictions[row_id] = np.zeros(len(all_species))\n",
    "    \n",
    "    # Crea il DataFrame di submission dalle predizioni accumulate\n",
    "    predictions_list = []\n",
    "    for row_id, scores in accumulated_predictions.items():\n",
    "        predictions_list.append([row_id] + list(scores))\n",
    "    \n",
    "    predictions = pd.DataFrame(predictions_list, columns=['row_id'] + all_species)\n",
    "    \n",
    "    # Ordina per row_id per avere un output coerente\n",
    "    predictions = predictions.sort_values('row_id').reset_index(drop=True)\n",
    "    \n",
    "    # Applica lo smoothing temporale\n",
    "    print(\"Applicazione smoothing temporale alle predizioni...\")\n",
    "    predictions = enhanced_temporal_smoothing_v2(predictions)\n",
    "    \n",
    "    # Clip dei valori tra 0 e 1 per sicurezza\n",
    "    for col in predictions.columns:\n",
    "        if col != 'row_id':\n",
    "            predictions[col] = predictions[col].clip(0, 1)\n",
    "\n",
    "    # Salva la submission come CSV\n",
    "    predictions.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"Submission salvata con {len(predictions)} predizioni.\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8151aac",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.environment == 'kaggle':\n",
    "    print(\"\\n🚀 Generazione submission SED corretta...\")\n",
    "    \n",
    "    # ✅ USA LA FUNZIONE CORRETTA\n",
    "    submission_df = generate_submission_sed_corrected(model, overlap=2.5, alpha=0.5)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\n🔄 Applicando post-processing migliorato...\")\n",
    "        \n",
    "        # 1. Power adjustment migliorato\n",
    "        submission_df = apply_power_to_low_ranked_cols_improved(\n",
    "            submission_df, \n",
    "            top_k=30,      \n",
    "            exponent=2     \n",
    "        )\n",
    "        \n",
    "        # 2. Enhanced temporal smoothing v2\n",
    "        submission_df = enhanced_temporal_smoothing_v2(\n",
    "            submission_df,\n",
    "            weights=[0.1, 0.8, 0.1]  \n",
    "        )\n",
    "        \n",
    "        # 3. Final clipping e validazione\n",
    "        species_cols = [col for col in submission_df.columns if col != 'row_id']\n",
    "        submission_df[species_cols] = submission_df[species_cols].clip(0, 1)\n",
    "        \n",
    "        # 4. Salva submission finale\n",
    "        submission_df.to_csv(\"submission.csv\", index=False)\n",
    "        print(f\"✅ Submission SED corretta salvata con {len(submission_df)} predizioni\")\n",
    "        \n",
    "        print(\"\\n📊 Anteprima submission finale:\")\n",
    "        print(submission_df.head())\n",
    "        \n",
    "        # Statistiche di validazione\n",
    "        print(f\"\\n📈 Statistiche submission:\")\n",
    "        print(f\"- Numero predizioni: {len(submission_df)}\")\n",
    "        print(f\"- Range valori: [{submission_df[species_cols].min().min():.4f}, {submission_df[species_cols].max().max():.4f}]\")\n",
    "        print(f\"- Media predizioni per timestamp: {submission_df[species_cols].mean().mean():.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n⏭️ Salto la generazione della submission perché non siamo su Kaggle.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2958.165303,
   "end_time": "2025-05-23T09:06:08.504478",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-23T08:16:50.339175",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "019fbbcbfdba4ddcb43fb56b89d6f838": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "0e6983f29cc74e13aca0f83533f7b614": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "115e833a1e2c4af39e3114e86b5a6ab3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1676e31d4c4f47c9baf5ec751d18c852": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4776353c32654130b576919e03430fba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4cfad99b490345cd96beb56b9f0b1be5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_019fbbcbfdba4ddcb43fb56b89d6f838",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1676e31d4c4f47c9baf5ec751d18c852",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "4eccaf67e74748dfa07fc2012bab7b38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_776ce6e2cdec43fc8cabbde8fd761761",
       "placeholder": "​",
       "style": "IPY_MODEL_d8f43c7e3c4740c5bc38eaddb2b5d58f",
       "tabbable": null,
       "tooltip": null,
       "value": "Preparazione dataset adattivo: 100%"
      }
     },
     "516a0dbb64754d7a890a368ff27fc31d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59621d93883346c6a2c056ccfc81c7a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0e6983f29cc74e13aca0f83533f7b614",
       "placeholder": "​",
       "style": "IPY_MODEL_c887db35485246d68c555172293290f7",
       "tabbable": null,
       "tooltip": null,
       "value": " 22144/22144 [47:27&lt;00:00,  8.19it/s]"
      }
     },
     "776ce6e2cdec43fc8cabbde8fd761761": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c436e69cc1f402fbc34a644d082d965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "827b45a7e0fa4bf2b3ccb8bfc15e7015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8f4a5b09e08c49048c3f3ff6e8148381",
        "IPY_MODEL_4cfad99b490345cd96beb56b9f0b1be5",
        "IPY_MODEL_a0376a34ebb84e7b868ce7afe6b27ec5"
       ],
       "layout": "IPY_MODEL_f5e68b30d7704b3b94c35dfa95547613",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8f4a5b09e08c49048c3f3ff6e8148381": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c436e69cc1f402fbc34a644d082d965",
       "placeholder": "​",
       "style": "IPY_MODEL_c2ab18fa36cf46acbf93d02c43a59673",
       "tabbable": null,
       "tooltip": null,
       "value": "Elaborazione soundscapes: "
      }
     },
     "9ec1ed79d57e4dc78c7896e41c727c86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a0376a34ebb84e7b868ce7afe6b27ec5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_115e833a1e2c4af39e3114e86b5a6ab3",
       "placeholder": "​",
       "style": "IPY_MODEL_516a0dbb64754d7a890a368ff27fc31d",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "a186aabdde28484ba9084312371f3193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e0a8185c24844db298d59005e31c1e41",
       "max": 22144,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ec1ed79d57e4dc78c7896e41c727c86",
       "tabbable": null,
       "tooltip": null,
       "value": 22144
      }
     },
     "c2ab18fa36cf46acbf93d02c43a59673": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c887db35485246d68c555172293290f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c8e92d36ad3f48ccac59f575ebe68759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4eccaf67e74748dfa07fc2012bab7b38",
        "IPY_MODEL_a186aabdde28484ba9084312371f3193",
        "IPY_MODEL_59621d93883346c6a2c056ccfc81c7a4"
       ],
       "layout": "IPY_MODEL_4776353c32654130b576919e03430fba",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d8f43c7e3c4740c5bc38eaddb2b5d58f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0a8185c24844db298d59005e31c1e41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5e68b30d7704b3b94c35dfa95547613": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
