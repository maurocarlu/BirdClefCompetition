{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdfb9f2",
   "metadata": {
    "papermill": {
     "duration": 0.010235,
     "end_time": "2025-05-23T08:16:57.646225",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.635990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con CNN\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza un'architettura CNN per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5518a6b",
   "metadata": {
    "papermill": {
     "duration": 0.008058,
     "end_time": "2025-05-23T08:16:57.663760",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.655702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Importazione delle Librerie Necessarie\n",
    "\n",
    "Importiamo tutte le librerie necessarie per l'elaborazione audio, deep learning e visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542df0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:35.694932Z",
     "iopub.status.busy": "2025-06-10T11:16:35.694677Z",
     "iopub.status.idle": "2025-06-10T11:16:48.484917Z",
     "shell.execute_reply": "2025-06-10T11:16:48.484096Z",
     "shell.execute_reply.started": "2025-06-10T11:16:35.694912Z"
    },
    "papermill": {
     "duration": 28.02249,
     "end_time": "2025-05-23T08:17:25.694495",
     "exception": false,
     "start_time": "2025-05-23T08:16:57.672005",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# Sostituisci le importazioni di Transformers con timm\n",
    "import timm\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"timm versione: {timm.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68fd08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.486723Z",
     "iopub.status.busy": "2025-06-10T11:16:48.486462Z",
     "iopub.status.idle": "2025-06-10T11:16:48.493061Z",
     "shell.execute_reply": "2025-06-10T11:16:48.492214Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.486705Z"
    },
    "papermill": {
     "duration": 0.022231,
     "end_time": "2025-05-23T08:17:25.727295",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.705064",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Imposta questo a True per abilitare la cancellazione\n",
    "clear_working_dir = True\n",
    "\n",
    "working_dir = '/kaggle/working/'\n",
    "\n",
    "if clear_working_dir:\n",
    "    for filename in os.listdir(working_dir):\n",
    "        file_path = os.path.join(working_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # elimina file o link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # elimina directory\n",
    "        except Exception as e:\n",
    "            print(f'Errore durante la rimozione di {file_path}: {e}')\n",
    "    print(f\"Tutti i file in {working_dir} sono stati rimossi.\")\n",
    "else:\n",
    "    print(\"Pulizia disabilitata (clear_working_dir = False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ca1b2",
   "metadata": {
    "papermill": {
     "duration": 0.009235,
     "end_time": "2025-05-23T08:17:25.745603",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.736368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configurazione dell'Ambiente di Esecuzione\n",
    "\n",
    "In questa sezione configuriamo l'ambiente di esecuzione in modo che il notebook funzioni sia su Kaggle, che su Google Colab, che in locale. Il codice rileverà automaticamente l'ambiente e configurerà i percorsi di conseguenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253f9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.494032Z",
     "iopub.status.busy": "2025-06-10T11:16:48.493746Z",
     "iopub.status.idle": "2025-06-10T11:16:48.550324Z",
     "shell.execute_reply": "2025-06-10T11:16:48.549592Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.494004Z"
    },
    "papermill": {
     "duration": 0.022269,
     "end_time": "2025-05-23T08:17:25.776636",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.754367",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbe73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.552327Z",
     "iopub.status.busy": "2025-06-10T11:16:48.552079Z",
     "iopub.status.idle": "2025-06-10T11:16:48.571116Z",
     "shell.execute_reply": "2025-06-10T11:16:48.570453Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.552310Z"
    },
    "papermill": {
     "duration": 0.029321,
     "end_time": "2025-05-23T08:17:25.814763",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.785442",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            # quindi non impostiamo ancora i percorsi derivati\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio - già allineati con vincitori\n",
    "        self.SR = 32000      # Sample rate\n",
    "        self.DURATION = 5    # Durata dei clip in secondi\n",
    "        self.N_MELS = 128    # Numero di bande Mel\n",
    "        self.N_FFT = 1024    # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 512  # Hop length per STFT\n",
    "        self.FMIN = 20       # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = 16000    # Frequenza massima\n",
    "        self.POWER = 2       # Esponente per calcolo spettrogramma\n",
    "            \n",
    "        # Parametri per il training - aggiornati secondo i vincitori\n",
    "        self.BATCH_SIZE = 96  # Aumentato da 32 a 96 come dai vincitori\n",
    "        self.EPOCHS = 15     # Numero di epoche per il training\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 4  # Aumentato per migliorare il data loading\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d552f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.571998Z",
     "iopub.status.busy": "2025-06-10T11:16:48.571749Z",
     "iopub.status.idle": "2025-06-10T11:16:48.652611Z",
     "shell.execute_reply": "2025-06-10T11:16:48.651889Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.571972Z"
    },
    "papermill": {
     "duration": 0.033917,
     "end_time": "2025-05-23T08:17:25.857854",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.823937",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/simplecnn/PyTorch/default/1\"\n",
    "    cache_model_file = os.path.join(cache_model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(cache_model_file)\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/simplecnn/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    model_file = os.path.join(model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {model_file}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b77a54",
   "metadata": {},
   "source": [
    "### Normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ca1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.653594Z",
     "iopub.status.busy": "2025-06-10T11:16:48.653337Z",
     "iopub.status.idle": "2025-06-10T11:16:48.761445Z",
     "shell.execute_reply": "2025-06-10T11:16:48.760655Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.653567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crea una singola istanza della trasformazione MelSpectrogram da riutilizzare\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=config.SR,\n",
    "    n_fft=config.N_FFT,\n",
    "    win_length=None,\n",
    "    hop_length=config.HOP_LENGTH,\n",
    "    f_min=config.FMIN,\n",
    "    f_max=config.FMAX,\n",
    "    n_mels=config.N_MELS,\n",
    "    window_fn=torch.hann_window,\n",
    "    power=config.POWER,\n",
    "    normalized=False,\n",
    "    onesided=True,\n",
    "    norm=\"slaney\",\n",
    "    mel_scale=\"slaney\"\n",
    ")\n",
    "\n",
    "# Funzione di conversione a dB e normalizzazione\n",
    "def amplitude_to_db(spectrogram):\n",
    "    \"\"\"\n",
    "    Converti spettrogramma in scala dB e applica normalizzazione Z-score (come il top performer)\n",
    "    \"\"\"\n",
    "    # Converti in dB (manteniamo questa parte)\n",
    "    spectrogram_db = 10.0 * torch.log10(torch.clamp(spectrogram, min=1e-10))\n",
    "    \n",
    "    # Normalizzazione Z-score invece di min-max\n",
    "    mean = torch.mean(spectrogram_db)\n",
    "    std = torch.std(spectrogram_db)\n",
    "    eps = 1e-6  # Epsilon per evitare divisione per zero\n",
    "    \n",
    "    # Se std=0, sottrai solo la media, altrimenti applica completa standardizzazione\n",
    "    return torch.where(std == 0, \n",
    "                       spectrogram_db - mean, \n",
    "                       (spectrogram_db - mean) / (std + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8881d",
   "metadata": {
    "papermill": {
     "duration": 0.010495,
     "end_time": "2025-05-23T08:17:25.878875",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.868380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Configurazione del Modello e Parametri\n",
    "\n",
    "Definiamo i parametri di configurazione per il preprocessamento audio, la creazione dello spettrogramma Mel e l'addestramento della CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3039383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.762648Z",
     "iopub.status.busy": "2025-06-10T11:16:48.762356Z",
     "iopub.status.idle": "2025-06-10T11:16:48.771503Z",
     "shell.execute_reply": "2025-06-10T11:16:48.770689Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.762620Z"
    },
    "papermill": {
     "duration": 0.029142,
     "end_time": "2025-05-23T08:17:25.918990",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.889848",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I parametri principali sono già definiti nella classe Config\n",
    "# Verifichiamo l'esistenza delle directory e creiamo quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1420b9",
   "metadata": {
    "papermill": {
     "duration": 0.009036,
     "end_time": "2025-05-23T08:17:25.937146",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.928110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Caricamento e Preprocessing dei Dati\n",
    "\n",
    "In questa sezione carichiamo i metadati dal file CSV di training, creiamo codifiche one-hot per le etichette delle specie e implementiamo funzioni per il caricamento e preprocessamento dei file audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc24a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:48.773148Z",
     "iopub.status.busy": "2025-06-10T11:16:48.772425Z",
     "iopub.status.idle": "2025-06-10T11:16:49.917820Z",
     "shell.execute_reply": "2025-06-10T11:16:49.917150Z",
     "shell.execute_reply.started": "2025-06-10T11:16:48.773128Z"
    },
    "papermill": {
     "duration": 1.647048,
     "end_time": "2025-05-23T08:17:27.594652",
     "exception": false,
     "start_time": "2025-05-23T08:17:25.947604",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "    \n",
    "# Per Kaggle, dovremo creare un dataset speciale per le soundscapes di test\n",
    "# Questo verrà utilizzato direttamente nella fase di generazione della submission\n",
    "# Non creiamo X_test_df e test_dataset per ora\n",
    "X_test_df = None\n",
    "y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95eca4",
   "metadata": {
    "papermill": {
     "duration": 0.008471,
     "end_time": "2025-05-23T08:17:27.612056",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.603585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Funzione di bilanciamento del dataset - Cancella una percentuale di esempi dalle classi molto numerose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b82d0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:49.918767Z",
     "iopub.status.busy": "2025-06-10T11:16:49.918505Z",
     "iopub.status.idle": "2025-06-10T11:16:49.927078Z",
     "shell.execute_reply": "2025-06-10T11:16:49.926322Z",
     "shell.execute_reply.started": "2025-06-10T11:16:49.918749Z"
    },
    "papermill": {
     "duration": 0.029146,
     "end_time": "2025-05-23T08:17:27.649859",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.620713",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataset_df(train_df, labels_one_hot, abundant_class_threshold=200, remove_percentage=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame bilanciato rimuovendo parte degli esempi con rating bassi dalle classi abbondanti.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame originale\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        abundant_class_threshold: Soglia per definire una classe come \"abbondante\"\n",
    "        remove_percentage: Percentuale di esempi con rating 1-3 da rimuovere dalle classi abbondanti\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame bilanciato, etichette one-hot bilanciate)\n",
    "    \"\"\"\n",
    "    # Conta esempi per ogni classe\n",
    "    class_counts = train_df['primary_label'].value_counts()\n",
    "    \n",
    "    # Identifica classi abbondanti\n",
    "    abundant_classes = class_counts[class_counts > abundant_class_threshold].index.tolist()\n",
    "    print(f\"Classi identificate come abbondanti (>{abundant_class_threshold} esempi): {len(abundant_classes)}\")\n",
    "    \n",
    "    # Copia il DataFrame originale\n",
    "    balanced_df = train_df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Contatori per statistiche\n",
    "    total_removed = 0\n",
    "    removed_by_class = {}\n",
    "    \n",
    "    # Per ogni classe abbondante\n",
    "    for cls in abundant_classes:\n",
    "        # Filtra esempi con rating 1-3 per questa classe\n",
    "        low_quality_mask = (balanced_df['primary_label'] == cls) & (balanced_df['rating'].isin([1, 2, 3]))\n",
    "        low_quality_indices = balanced_df[low_quality_mask].index.tolist()\n",
    "        \n",
    "        # Numero di esempi da rimuovere\n",
    "        n_to_remove = int(len(low_quality_indices) * remove_percentage)\n",
    "        \n",
    "        # Seleziona casualmente gli indici da rimuovere\n",
    "        np.random.seed(random_state)\n",
    "        if n_to_remove > 0:\n",
    "            indices_to_remove = np.random.choice(low_quality_indices, size=n_to_remove, replace=False)\n",
    "            \n",
    "            # Memorizza gli indici da rimuovere\n",
    "            rows_to_drop.extend(indices_to_remove)\n",
    "            \n",
    "            # Aggiorna statistiche\n",
    "            removed_by_class[cls] = n_to_remove\n",
    "            total_removed += n_to_remove\n",
    "    \n",
    "    # Rimuovi le righe selezionate\n",
    "    if rows_to_drop:\n",
    "        balanced_df = balanced_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Aggiorna anche le etichette one-hot rimuovendo gli stessi indici\n",
    "        mask = np.ones(len(train_df), dtype=bool)\n",
    "        mask[rows_to_drop] = False\n",
    "        balanced_labels = labels_one_hot[mask]\n",
    "    else:\n",
    "        balanced_labels = labels_one_hot\n",
    "    \n",
    "    # Statistiche finali\n",
    "    print(f\"Totale esempi rimossi: {total_removed} ({total_removed/len(train_df):.1%} del dataset originale)\")\n",
    "    print(f\"Dimensione dataset originale: {len(train_df)}\")\n",
    "    print(f\"Dimensione dataset bilanciato: {len(balanced_df)}\")\n",
    "    \n",
    "    # Visualizza le prime 5 classi con maggiori rimozioni\n",
    "    if removed_by_class:\n",
    "        top_removed = sorted(removed_by_class.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nClassi con maggior numero di esempi rimossi:\")\n",
    "        for cls, count in top_removed:\n",
    "            original = class_counts[cls]\n",
    "            remaining = original - count\n",
    "            print(f\"- {cls}: {count} rimossi, {remaining}/{original} rimanenti ({remaining/original:.1%})\")\n",
    "    else:\n",
    "        print(\"Nessun esempio rimosso.\")\n",
    "    \n",
    "    return balanced_df, balanced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573968a",
   "metadata": {
    "papermill": {
     "duration": 0.008725,
     "end_time": "2025-05-23T08:17:27.668000",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.659275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.5 Analisi Esplorativa dei Dati (EDA)\n",
    "\n",
    "In questa sezione esploreremo le caratteristiche del dataset per comprendere meglio la distribuzione delle specie, le proprietà audio e identificare eventuali pattern nei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:49.929612Z",
     "iopub.status.busy": "2025-06-10T11:16:49.929407Z",
     "iopub.status.idle": "2025-06-10T11:16:52.362879Z",
     "shell.execute_reply": "2025-06-10T11:16:52.361969Z",
     "shell.execute_reply.started": "2025-06-10T11:16:49.929596Z"
    },
    "papermill": {
     "duration": 3.453267,
     "end_time": "2025-05-23T08:17:31.130595",
     "exception": false,
     "start_time": "2025-05-23T08:17:27.677328",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configurazione stile visualizzazioni\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"=== Statistiche di base del dataset ===\")\n",
    "print(f\"Numero totale di registrazioni: {len(train_df)}\")\n",
    "print(f\"Numero di specie uniche nel dataset: {len(all_species)}\")\n",
    "print(f\"Campi disponibili nei metadati: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Verifichiamo i dati mancanti\n",
    "missing_data = train_df.isnull().sum()\n",
    "print(\"\\n=== Valori mancanti ===\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# 1. Distribuzione delle specie nel dataset (visualizzazione migliorata)\n",
    "print(\"\\n=== Analisi delle Specie ===\")\n",
    "primary_species_count = train_df['primary_label'].value_counts()\n",
    "\n",
    "# Plot combinato: distribuzione delle specie con evidenza delle classi rare\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# A sinistra: top 20 specie più rappresentate\n",
    "sns.barplot(x=primary_species_count.head(20).index, y=primary_species_count.head(20).values, ax=ax1)\n",
    "ax1.set_title('Top 20 Specie per Numero di Registrazioni')\n",
    "ax1.set_xlabel('Specie')\n",
    "ax1.set_ylabel('Numero di Registrazioni')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# A destra: distribuzione del numero di esempi per specie\n",
    "sns.histplot(primary_species_count, bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribuzione del Numero di Registrazioni per Specie')\n",
    "ax2.set_xlabel('Numero di Registrazioni')\n",
    "ax2.set_ylabel('Conteggio Specie')\n",
    "ax2.axvline(x=primary_species_count.median(), color='r', linestyle='--', \n",
    "            label=f'Mediana: {primary_species_count.median()}')\n",
    "ax2.axvline(x=primary_species_count.mean(), color='g', linestyle='--', \n",
    "            label=f'Media: {primary_species_count.mean():.1f}')\n",
    "ax2.axvline(x=50, color='orange', linestyle=':', label='Soglia classi rare (50)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcolo dell'indice di Gini per misurare lo sbilanciamento\n",
    "def gini_coefficient(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n+1)\n",
    "    return (np.sum((2*index - n - 1) * x)) / (n * np.sum(x))\n",
    "\n",
    "gini = gini_coefficient(primary_species_count.values)\n",
    "print(f\"\\nIndice di Gini per la distribuzione delle specie: {gini:.4f}\")\n",
    "print(f\"Questo indica {'un alto' if gini > 0.6 else 'un moderato' if gini > 0.3 else 'un basso'} livello di sbilanciamento nel dataset.\")\n",
    "\n",
    "# 2. NUOVA ANALISI: Rating per le classi con pochi esempi\n",
    "# Definisco la soglia per le classi rare (< 50 esempi)\n",
    "RARE_CLASS_THRESHOLD = 50\n",
    "rare_species = primary_species_count[primary_species_count < RARE_CLASS_THRESHOLD].index.tolist()\n",
    "print(f\"\\n=== Analisi dei Rating per Classi Rare (<{RARE_CLASS_THRESHOLD} esempi) ===\")\n",
    "print(f\"Numero di classi rare: {len(rare_species)} su {len(all_species)} totali ({len(rare_species)/len(all_species):.1%})\")\n",
    "\n",
    "# Raccolgo i dati sui rating per le classi rare\n",
    "rare_class_ratings = []\n",
    "for species in rare_species:\n",
    "    species_df = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_df['rating'].fillna(0).tolist()  # Sostituisco NaN con 0 (nessun rating)\n",
    "    \n",
    "    # Statistiche per questa specie\n",
    "    rare_class_ratings.append({\n",
    "        'species': species,\n",
    "        'count': len(species_df),\n",
    "        'avg_rating': np.mean(ratings),\n",
    "        'ratings': ratings,\n",
    "        'rating_counts': {r: ratings.count(r) for r in set(ratings)}\n",
    "    })\n",
    "\n",
    "# Creo DataFrame per analisi\n",
    "rare_ratings_df = pd.DataFrame(rare_class_ratings)\n",
    "rare_ratings_df = rare_ratings_df.sort_values('count')\n",
    "\n",
    "# Visualizzazione: Rating medi vs Conteggio per le classi rare\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Heatmap: distribuzione dei rating per classi rare\n",
    "n_rare_to_show = min(30, len(rare_ratings_df))  # Mostra max 30 classi per leggibilità\n",
    "rare_sample = rare_ratings_df.head(n_rare_to_show)\n",
    "\n",
    "# Preparo i dati per la heatmap\n",
    "heatmap_data = []\n",
    "rating_values = [0, 1, 2, 3, 4, 5]  # Tutti i possibili rating\n",
    "for _, row in rare_sample.iterrows():\n",
    "    species_data = [row['species'], row['count']]\n",
    "    for rating in rating_values:\n",
    "        species_data.append(row['rating_counts'].get(rating, 0))\n",
    "    heatmap_data.append(species_data)\n",
    "\n",
    "# Creo DataFrame per la heatmap\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data, \n",
    "    columns=['species', 'count'] + [f'rating_{r}' for r in rating_values]\n",
    ")\n",
    "\n",
    "# Plot combinato con scatter plot e heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Scatter plot: rating medio vs numero esempi\n",
    "sns.scatterplot(\n",
    "    x='count', \n",
    "    y='avg_rating', \n",
    "    data=rare_ratings_df, \n",
    "    ax=ax1, \n",
    "    alpha=0.7,\n",
    "    hue='count',\n",
    "    palette='viridis',\n",
    "    size='count',\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "ax1.set_title('Rating Medio vs Numero di Esempi per Classi Rare')\n",
    "ax1.set_xlabel('Numero di Esempi')\n",
    "ax1.set_ylabel('Rating Medio')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mostra statistiche\n",
    "avg_rating_rare = rare_ratings_df['avg_rating'].mean()\n",
    "ax1.axhline(y=avg_rating_rare, color='r', linestyle='--', \n",
    "           label=f'Rating medio classi rare: {avg_rating_rare:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Heatmap: distribuzione dei rating per le classi più rare\n",
    "pivot_data = pd.DataFrame({\n",
    "    'species': heatmap_df['species'],\n",
    "    'Rating 0': heatmap_df['rating_0'],\n",
    "    'Rating 1': heatmap_df['rating_1'],\n",
    "    'Rating 2': heatmap_df['rating_2'],\n",
    "    'Rating 3': heatmap_df['rating_3'],\n",
    "    'Rating 4': heatmap_df['rating_4'],\n",
    "    'Rating 5': heatmap_df['rating_5'],\n",
    "}).set_index('species')\n",
    "\n",
    "sns.heatmap(pivot_data, cmap=\"YlGnBu\", annot=True, fmt='g', ax=ax2)\n",
    "ax2.set_title(f'Distribuzione dei Rating nelle {n_rare_to_show} Classi più Rare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche aggregate sui rating per le classi rare\n",
    "print(\"\\nStatistiche sui rating per le classi rare:\")\n",
    "print(f\"- Rating medio complessivo: {rare_ratings_df['avg_rating'].mean():.2f}\")\n",
    "print(f\"- Percentuale di registrazioni senza rating (0): {sum(r['rating_counts'].get(0, 0) for r in rare_class_ratings) / sum(r['count'] for r in rare_class_ratings):.1%}\")\n",
    "\n",
    "# Analisi delle classi estremamente rare (≤ 5 esempi)\n",
    "very_rare_species = primary_species_count[primary_species_count <= 5].index.tolist()\n",
    "print(f\"\\nClassi estremamente rare (≤ 5 esempi): {len(very_rare_species)}\")\n",
    "\n",
    "very_rare_df = train_df[train_df['primary_label'].isin(very_rare_species)]\n",
    "print(\"Dettaglio delle registrazioni per le classi estremamente rare:\")\n",
    "for species in very_rare_species:\n",
    "    species_data = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_data['rating'].fillna(0).tolist()\n",
    "    print(f\"- {species}: {len(species_data)} esempi, ratings: {ratings}\")\n",
    "\n",
    "\n",
    "# Identificazione delle classi con solo rating molto bassi (1-2)\n",
    "print(\"\\n=== Analisi delle Classi con SOLO Rating Molto Bassi (1-2) ===\")\n",
    "\n",
    "# Funzione per verificare se una specie ha esclusivamente rating tra 1-2\n",
    "def has_only_low_ratings(species_ratings):\n",
    "    valid_ratings = [r for r in species_ratings if pd.notna(r) and r != 0]  # Escludi NaN e rating=0\n",
    "    if not valid_ratings:  # Se non ci sono rating validi\n",
    "        return False\n",
    "    return all(1 <= r <= 2 for r in valid_ratings)  # Modificato: ora solo 1-2\n",
    "\n",
    "# Raggruppa per specie e analizza\n",
    "low_rating_species = []\n",
    "for species, group in train_df.groupby('primary_label'):\n",
    "    ratings = group['rating'].tolist()\n",
    "    if has_only_low_ratings(ratings):\n",
    "        low_rating_species.append({\n",
    "            'species': species,\n",
    "            'count': len(ratings),\n",
    "            'avg_rating': np.nanmean([r for r in ratings if pd.notna(r) and r != 0]),\n",
    "            'ratings': sorted([r for r in ratings if pd.notna(r) and r != 0])\n",
    "        })\n",
    "\n",
    "# Crea DataFrame e visualizza risultati\n",
    "if low_rating_species:\n",
    "    low_rating_df = pd.DataFrame(low_rating_species).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Trovate {len(low_rating_species)} classi con SOLO rating molto bassi (1-2):\")\n",
    "    \n",
    "    # Visualizzazione\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(low_rating_df['species'], low_rating_df['avg_rating'], alpha=0.7, color='tomato')\n",
    "    plt.axhline(y=1.5, color='r', linestyle='--', label='Media teorica = 1.5')\n",
    "    plt.title('Classi con Esclusivamente Rating Molto Bassi (1-2)')\n",
    "    plt.xlabel('Specie')\n",
    "    plt.ylabel('Rating Medio')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostra dettagli delle prime 10 classi\n",
    "    print(\"\\nDettagli delle prime 10 classi con solo rating molto bassi:\")\n",
    "    for i, row in low_rating_df.head(10).iterrows():\n",
    "        print(f\"- {row['species']}: {row['count']} clip, rating medio: {row['avg_rating']:.2f}, ratings: {row['ratings']}\")\n",
    "    \n",
    "    # Cerca sovrapposizione con classi rare\n",
    "    overlap = [s for s in low_rating_df['species'] if s in rare_species]\n",
    "    print(f\"\\nSovrapposizione con classi rare (<{RARE_CLASS_THRESHOLD} esempi): {len(overlap)} classi\")\n",
    "    if overlap:\n",
    "        print(f\"Le classi rare che hanno solo rating molto bassi: {overlap[:10]}{'...' if len(overlap) > 10 else ''}\")\n",
    "else:\n",
    "    print(\"Nessuna classe ha esclusivamente rating da 1 a 2.\")\n",
    "\n",
    "print(\"\\n=== Conclusioni dall'Analisi delle Classi Rare ===\")\n",
    "print(f\"1. Abbiamo {len(rare_species)} classi rare (<{RARE_CLASS_THRESHOLD} esempi)\")\n",
    "print(f\"2. Di queste, {len(very_rare_species)} hanno 5 o meno esempi\")\n",
    "print(\"3. La qualità delle registrazioni (rating) è un fattore critico per le classi rare\")\n",
    "print(\"4. Le classi estremamente rare richiedono tecniche speciali (data augmentation, few-shot learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ae8c3",
   "metadata": {
    "papermill": {
     "duration": 0.018439,
     "end_time": "2025-05-23T08:17:31.167560",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.149121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation: Implementazione delle Tecniche dei Vincitori\n",
    "\n",
    "In questa sezione implementiamo le tre tecniche di data augmentation che hanno contribuito significativamente alle performance dei vincitori:\n",
    "1. **Random Segment Selection** - Estrae segmenti casuali dalle registrazioni audio\n",
    "2. **XY Masking** - Applica maschere casuali sugli assi tempo e frequenza degli spettrogrammi Mel\n",
    "3. **Horizontal CutMix** - Combina parti di spettrogrammi da diverse registrazioni\n",
    "\n",
    "La classe `AudioAugmentations` gestisce tutte queste trasformazioni in modo unificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108b0ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.363951Z",
     "iopub.status.busy": "2025-06-10T11:16:52.363686Z",
     "iopub.status.idle": "2025-06-10T11:16:52.371267Z",
     "shell.execute_reply": "2025-06-10T11:16:52.370543Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.363933Z"
    },
    "papermill": {
     "duration": 0.032882,
     "end_time": "2025-05-23T08:17:31.217582",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.184700",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioAugmentations:\n",
    "    def __init__(self, p_random_segment=0.5, p_xy_mask=0.5, p_horizontal_cutmix=0.25):\n",
    "        \"\"\"\n",
    "        Inizializza le trasformazioni per data augmentation audio.\n",
    "        \n",
    "        Args:\n",
    "            p_random_segment: Probabilità di utilizzare un segmento casuale\n",
    "            p_xy_mask: Probabilità di applicare il mascheramento XY\n",
    "            p_horizontal_cutmix: Probabilità di applicare horizontal cutmix\n",
    "        \"\"\"\n",
    "        self.p_random_segment = p_random_segment\n",
    "        self.p_xy_mask = p_xy_mask\n",
    "        self.p_horizontal_cutmix = p_horizontal_cutmix\n",
    "    \n",
    "    def apply_xy_masking(self, spec):\n",
    "        \"\"\"Applica maschere casuali sull'asse X (tempo) e Y (frequenza) allo spettrogramma\"\"\"\n",
    "        mask = spec.clone()\n",
    "        \n",
    "        # Determina la dimensionalità del tensore\n",
    "        if len(mask.shape) == 3:  # [channels, height, width]\n",
    "            channels, height, width = mask.shape\n",
    "        elif len(mask.shape) == 4:  # [batch, channels, height, width]\n",
    "            _, channels, height, width = mask.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Forma dello spettrogramma non supportata: {mask.shape}\")\n",
    "        \n",
    "        # Masking temporale (asse X)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_width = int(width * np.random.uniform(0.1, 0.2))  # 10-20% width\n",
    "            mask_start = np.random.randint(0, width - mask_width)\n",
    "            mask[..., mask_start:mask_start+mask_width] = 0\n",
    "        \n",
    "        # Masking frequenziale (asse Y)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_height = int(height * np.random.uniform(0.1, 0.2))  # 10-20% height\n",
    "            mask_start = np.random.randint(0, height - mask_height)\n",
    "            \n",
    "            # Adatta l'indicizzazione in base alla dimensionalità\n",
    "            if len(mask.shape) == 3:  # [channels, height, width]\n",
    "                mask[:, mask_start:mask_start+mask_height, :] = 0\n",
    "            else:  # [batch, channels, height, width]\n",
    "                mask[:, :, mask_start:mask_start+mask_height, :] = 0\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064f2c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.372056Z",
     "iopub.status.busy": "2025-06-10T11:16:52.371872Z",
     "iopub.status.idle": "2025-06-10T11:16:52.393405Z",
     "shell.execute_reply": "2025-06-10T11:16:52.392691Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.372023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SpectrogramImageAugmentations:\n",
    "    def __init__(self, p_random_erasing=0.3, p_time_mask=0.5, p_freq_mask=0.5):\n",
    "        \"\"\"\n",
    "        Augmentations che trattano gli spettrogrammi come immagini.\n",
    "        \n",
    "        Args:\n",
    "            p_random_erasing: Probabilità di applicare Random Erasing\n",
    "            p_time_mask: Probabilità di applicare Time Masking\n",
    "            p_freq_mask: Probabilità di applicare Frequency Masking\n",
    "        \"\"\"\n",
    "        self.p_random_erasing = p_random_erasing\n",
    "        self.p_time_mask = p_time_mask\n",
    "        self.p_freq_mask = p_freq_mask\n",
    "    \n",
    "    def random_erasing(self, spec, scale=(0.02, 0.1), ratio=(0.3, 3.3)):\n",
    "        \"\"\"\n",
    "        Applica Random Erasing come nelle immagini.\n",
    "        Rimuove rettangoli casuali dallo spettrogramma.\n",
    "        \"\"\"\n",
    "        if np.random.random() > self.p_random_erasing:\n",
    "            return spec\n",
    "            \n",
    "        spec = spec.clone()\n",
    "        \n",
    "        # Ottieni dimensioni [C, H, W]\n",
    "        if len(spec.shape) == 3:\n",
    "            C, H, W = spec.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Forma spettrogramma non supportata: {spec.shape}\")\n",
    "        \n",
    "        # Calcola area del rettangolo da cancellare\n",
    "        area = H * W\n",
    "        target_area = np.random.uniform(scale[0], scale[1]) * area\n",
    "        aspect_ratio = np.random.uniform(ratio[0], ratio[1])\n",
    "        \n",
    "        # Calcola dimensioni del rettangolo\n",
    "        h = int(round(np.sqrt(target_area * aspect_ratio)))\n",
    "        w = int(round(np.sqrt(target_area / aspect_ratio)))\n",
    "        \n",
    "        # Assicurati che le dimensioni siano valide\n",
    "        if w < W and h < H:\n",
    "            x1 = np.random.randint(0, W - w)\n",
    "            y1 = np.random.randint(0, H - h)\n",
    "            \n",
    "            # Cancella il rettangolo (imposta a 0 o valore medio)\n",
    "            spec[:, y1:y1+h, x1:x1+w] = 0\n",
    "            \n",
    "        return spec\n",
    "    \n",
    "    def time_frequency_masking(self, spec):\n",
    "        \"\"\"\n",
    "        Applica Time e Frequency Masking separatamente.\n",
    "        Più aggressivo del semplice XY masking.\n",
    "        \"\"\"\n",
    "        spec = spec.clone()\n",
    "        \n",
    "        if len(spec.shape) == 3:\n",
    "            C, H, W = spec.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Forma spettrogramma non supportata: {spec.shape}\")\n",
    "        \n",
    "        # Time Masking (asse temporale)\n",
    "        if np.random.random() < self.p_time_mask:\n",
    "            # Maschera 10-25% dell'asse temporale\n",
    "            mask_width = int(W * np.random.uniform(0.1, 0.25))\n",
    "            mask_start = np.random.randint(0, max(1, W - mask_width))\n",
    "            spec[:, :, mask_start:mask_start + mask_width] = 0\n",
    "        \n",
    "        # Frequency Masking (asse frequenziale)\n",
    "        if np.random.random() < self.p_freq_mask:\n",
    "            # Maschera 10-20% dell'asse frequenziale\n",
    "            mask_height = int(H * np.random.uniform(0.1, 0.2))\n",
    "            mask_start = np.random.randint(0, max(1, H - mask_height))\n",
    "            spec[:, mask_start:mask_start + mask_height, :] = 0\n",
    "            \n",
    "        return spec\n",
    "    \n",
    "    def apply_all(self, spec):\n",
    "        \"\"\"Applica tutte le augmentations in sequenza\"\"\"\n",
    "        # Prima Random Erasing\n",
    "        spec = self.random_erasing(spec)\n",
    "        \n",
    "        # Poi Time/Frequency Masking\n",
    "        spec = self.time_frequency_masking(spec)\n",
    "        \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5870e",
   "metadata": {
    "papermill": {
     "duration": 0.016381,
     "end_time": "2025-05-23T08:17:31.250882",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.234501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Horizontal CutMix: Implementazione della Funzione di Collate\n",
    "\n",
    "Questa funzione personalizzata viene utilizzata nel DataLoader per implementare l'Horizontal CutMix,\n",
    "che combina sezioni temporali di spettrogrammi diversi all'interno dello stesso batch.\n",
    "Le etichette vengono miscelate proporzionalmente alla quantità di dati combinati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f22fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.394703Z",
     "iopub.status.busy": "2025-06-10T11:16:52.394232Z",
     "iopub.status.idle": "2025-06-10T11:16:52.413231Z",
     "shell.execute_reply": "2025-06-10T11:16:52.412525Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.394685Z"
    },
    "papermill": {
     "duration": 0.029684,
     "end_time": "2025-05-23T08:17:31.297426",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.267742",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mixup_cutmix_collate(batch, p_mixup=1.0, p_cutmix=0.25, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Collate function che applica SEMPRE Mixup (p=1.0) e occasionalmente CutMix.\n",
    "    \n",
    "    Args:\n",
    "        batch: Lista di tuple (input, target)\n",
    "        p_mixup: Probabilità di applicare mixup (1.0 = sempre)\n",
    "        p_cutmix: Probabilità di applicare cutmix invece di mixup\n",
    "        alpha: Parametro per la distribuzione Beta\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (inputs_batch, targets_batch)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Estrae input e target dal batch\n",
    "    for input_tensor, target_tensor in batch:\n",
    "        inputs.append(input_tensor)\n",
    "        targets.append(target_tensor)\n",
    "    \n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if batch_size > 1 and np.random.rand() < p_mixup:\n",
    "        # Decide se fare CutMix o Mixup normale\n",
    "        if np.random.rand() < p_cutmix:\n",
    "            # === CUTMIX (come prima) ===\n",
    "            for i in range(batch_size):\n",
    "                j = np.random.randint(0, batch_size)\n",
    "                if i != j:\n",
    "                    width = inputs.shape[3]\n",
    "                    cut_point = np.random.randint(int(width * 0.25), int(width * 0.75))\n",
    "                    mix_ratio = cut_point / width\n",
    "                    inputs[i, :, :, :cut_point] = inputs[j, :, :, :cut_point]\n",
    "                    targets[i] = targets[i] * (1 - mix_ratio) + targets[j] * mix_ratio\n",
    "        else:\n",
    "            # === MIXUP NORMALE ===\n",
    "            # Genera lambda dalla distribuzione Beta\n",
    "            lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "            \n",
    "            # Permuta gli indici per il mixing\n",
    "            indices = torch.randperm(batch_size)\n",
    "            \n",
    "            # Mix degli input\n",
    "            inputs = lam * inputs + (1 - lam) * inputs[indices]\n",
    "            \n",
    "            # Mix dei target\n",
    "            targets = lam * targets + (1 - lam) * targets[indices]\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eab71e",
   "metadata": {
    "papermill": {
     "duration": 0.01671,
     "end_time": "2025-05-23T08:17:31.330311",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.313601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Caricamento e pre-processing\n",
    "\n",
    "## Preprocessing Audio con Supporto per Data Augmentation\n",
    "\n",
    "La funzione di caricamento audio è stata modificata per supportare due modalità di estrazione:\n",
    "- **Posizionale** - Estrazione da punti specifici nella registrazione ('start', 'center', 'end')\n",
    "- **Casuale** - Estrazione di un segmento casuale quando `random_segment=True`\n",
    "\n",
    "Questa implementazione permette di applicare la tecnica di Random Segment Selection come parte della data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f1129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.414152Z",
     "iopub.status.busy": "2025-06-10T11:16:52.413918Z",
     "iopub.status.idle": "2025-06-10T11:16:52.433709Z",
     "shell.execute_reply": "2025-06-10T11:16:52.433098Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.414127Z"
    },
    "papermill": {
     "duration": 0.035135,
     "end_time": "2025-05-23T08:17:31.382511",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.347376",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio_torch(file_path, target_sr=config.SR, duration=config.DURATION, \n",
    "                                    segment_position='center', random_segment=False):\n",
    "    \"\"\"\n",
    "    Carica un file audio, estrae un segmento e lo converte in spettrogramma Mel.\n",
    "    Ottimizzata per segmenti lunghi (15s).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carica il file audio con torchaudio\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        \n",
    "        # Converti a mono se necessario\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1]\n",
    "        \n",
    "        # Ricampiona se necessario\n",
    "        if sr != target_sr:\n",
    "            resampler = T.Resample(sr, target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Calcola la lunghezza target in campioni\n",
    "        target_len = int(target_sr * duration)\n",
    "        total_len = waveform.shape[1]\n",
    "        \n",
    "        # Gestisci clip troppo corte - strategia migliorata\n",
    "        if total_len < target_len:\n",
    "            if total_len < target_len / 3:\n",
    "                # Per file molto corti, ripeti l'audio\n",
    "                n_repeat = (target_len // total_len) + 1\n",
    "                waveform = waveform.repeat(1, n_repeat)\n",
    "            else:\n",
    "                # Per file moderatamente corti, usa padding con silenzio\n",
    "                padding = target_len - total_len\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding), mode='constant', value=0)\n",
    "            total_len = waveform.shape[1]\n",
    "        \n",
    "        # Seleziona il segmento - ora sempre casuale\n",
    "        if random_segment or segment_position is None:\n",
    "            # Estrai segmento casuale\n",
    "            max_start_idx = total_len - target_len\n",
    "            if max_start_idx <= 0:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = torch.randint(0, max_start_idx, (1,)).item()\n",
    "        else:\n",
    "            # Supporta ancora le posizioni predefinite per compatibilità\n",
    "            if segment_position == 'start':\n",
    "                start_idx = int(total_len * 0.1)  # 10% dall'inizio\n",
    "            elif segment_position == 'end':\n",
    "                start_idx = max(0, total_len - target_len - int(total_len * 0.1))  # 10% dalla fine\n",
    "            else:  # 'center' (default)\n",
    "                start_idx = max(0, int(total_len / 2 - target_len / 2))\n",
    "        \n",
    "        # Estrai il segmento\n",
    "        waveform = waveform[:, start_idx:start_idx + target_len]\n",
    "        \n",
    "        # Padda se necessario (nel caso di start_idx + target_len > total_len)\n",
    "        if waveform.shape[1] < target_len:\n",
    "            padding = target_len - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        # Applica la trasformazione Mel\n",
    "        mel_spectrogram = mel_transform(waveform)\n",
    "        \n",
    "        # Converti in scala dB e normalizza\n",
    "        log_mel_spec = amplitude_to_db(mel_spectrogram)\n",
    "        \n",
    "        return log_mel_spec\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "        # Crea uno spettrogramma vuoto\n",
    "        time_steps = int(target_sr * duration / config.HOP_LENGTH) + 1\n",
    "        return torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40c4ed",
   "metadata": {
    "papermill": {
     "duration": 0.016747,
     "end_time": "2025-05-23T08:17:31.416102",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.399355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Dataset PyTorch per Dati Audio\n",
    "\n",
    "## Dataset PyTorch con Supporto Integrato per Data Augmentation\n",
    "\n",
    "Implementiamo due classi di dataset:\n",
    "- `RandomSegmentBirdDataset` \n",
    "  - Estrae segmenti randomici di 15 secondi in base alla durata delle registrazioni\n",
    "  - Applica le tecniche di data augmentation durante il caricamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20b040",
   "metadata": {
    "papermill": {
     "duration": 0.015093,
     "end_time": "2025-05-23T08:17:31.493206",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.478113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RandomSegmentBirdDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251d116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.434793Z",
     "iopub.status.busy": "2025-06-10T11:16:52.434512Z",
     "iopub.status.idle": "2025-06-10T11:16:52.454438Z",
     "shell.execute_reply": "2025-06-10T11:16:52.453785Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.434771Z"
    },
    "papermill": {
     "duration": 0.03675,
     "end_time": "2025-05-23T08:17:31.545532",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.508782",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomSegmentBirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, transform=None, \n",
    "                 augmentations=None, image_augmentations=None):\n",
    "        \"\"\"Dataset con caricamento LAZY degli spettrogrammi\"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        self.image_augmentations = image_augmentations\n",
    "        \n",
    "        print(\"Dataset con LAZY loading inizializzato...\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Ottieni il record dal DataFrame\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Attenzione: File non trovato in {file_path}.\")\n",
    "            time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "            dummy_spec = torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)\n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return dummy_spec, dummy_label\n",
    "        \n",
    "        # ⚠️ CARICA SOLO AL MOMENTO DELL'USO - NON PRIMA!\n",
    "        try:\n",
    "            # Carica il file audio con torchaudio direttamente qui\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            \n",
    "            # Converti a mono se necessario\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform[0:1]\n",
    "            \n",
    "            # Ricampiona se necessario\n",
    "            if sr != config.SR:\n",
    "                resampler = T.Resample(sr, config.SR)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # Calcola la lunghezza target in campioni\n",
    "            target_len = int(config.SR * config.DURATION)\n",
    "            total_len = waveform.shape[1]\n",
    "            \n",
    "            # Estrai segmento casuale\n",
    "            if total_len > target_len:\n",
    "                max_start_idx = total_len - target_len\n",
    "                start_idx = torch.randint(0, max_start_idx, (1,)).item()\n",
    "                waveform = waveform[:, start_idx:start_idx + target_len]\n",
    "            elif total_len < target_len:\n",
    "                # Padda se necessario\n",
    "                padding = target_len - total_len\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            \n",
    "            # Calcola spettrogramma MEL\n",
    "            mel_spectrogram = mel_transform(waveform)\n",
    "            log_mel_spec = amplitude_to_db(mel_spectrogram)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento di {file_path}: {e}\")\n",
    "            time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "            log_mel_spec = torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)\n",
    "        \n",
    "        # Applica augmentations solo se richieste (e molto leggere)\n",
    "        if self.augmentations and np.random.random() < 0.1:  # Solo 10%\n",
    "            log_mel_spec = self.augmentations.apply_xy_masking(log_mel_spec)\n",
    "        \n",
    "        if self.image_augmentations:\n",
    "            log_mel_spec = self.image_augmentations.apply_all(log_mel_spec)\n",
    "\n",
    "        # Ottieni le etichette\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return log_mel_spec, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bb5aa",
   "metadata": {
    "papermill": {
     "duration": 0.015303,
     "end_time": "2025-05-23T08:17:31.577368",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.562065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creazione dataset e dataloader con RandomSegmentBirdDataset\n",
    "\n",
    "### Creazione dei DataLoader con Augmentation\n",
    "\n",
    "In questa sezione:\n",
    "1. Applichiamo il bilanciamento strategico al dataset di training\n",
    "2. Creiamo l'istanza di AudioAugmentations con le probabilità ottimali\n",
    "3. Configuriamo i dataloader con le funzioni di collate personalizzate\n",
    "4. Analizziamo la distribuzione dei segmenti nel dataset risultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930d066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.456116Z",
     "iopub.status.busy": "2025-06-10T11:16:52.455324Z",
     "iopub.status.idle": "2025-06-10T11:16:52.701517Z",
     "shell.execute_reply": "2025-06-10T11:16:52.700870Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.456093Z"
    },
    "papermill": {
     "duration": 2847.998728,
     "end_time": "2025-05-23T09:04:59.592725",
     "exception": false,
     "start_time": "2025-05-23T08:17:31.593997",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Applica il bilanciamento strategico solo al dataset di training\n",
    "print(\"\\n=== Bilanciamento Strategico del Dataset di Training ===\")\n",
    "X_train_df_balanced, y_train_one_hot_balanced = create_balanced_dataset_df(\n",
    "    X_train_df, \n",
    "    y_train_one_hot,\n",
    "    abundant_class_threshold=150,\n",
    "    remove_percentage=0.4\n",
    ")\n",
    "\n",
    "# === NUOVE AUGMENTATIONS ===\n",
    "# Crea augmentations audio originali\n",
    "audio_augmentations = AudioAugmentations(\n",
    "    p_random_segment=1.0,  # Sempre segmenti casuali\n",
    "    p_xy_mask=0.1,         # Ridotto perché abbiamo anche le image augmentations\n",
    "    p_horizontal_cutmix=0.0  # Disabilitato qui, lo facciamo nel collate\n",
    ")\n",
    "\n",
    "# Crea augmentations come immagini (NUOVO!)\n",
    "image_augmentations = SpectrogramImageAugmentations(\n",
    "    p_random_erasing=0.3,   # Random Erasing al 30%\n",
    "    p_time_mask=0.5,        # Time Masking al 50%\n",
    "    p_freq_mask=0.5         # Frequency Masking al 50%\n",
    ")\n",
    "\n",
    "# Per validation: solo segmenti casuali, nessuna augmentation\n",
    "val_augmentations = AudioAugmentations(\n",
    "    p_random_segment=1.0,\n",
    "    p_xy_mask=0.0,\n",
    "    p_horizontal_cutmix=0.0\n",
    ")\n",
    "\n",
    "# Creiamo i dataset con le NUOVE augmentations\n",
    "print(\"Creazione dataset di training con augmentations come immagini...\")\n",
    "train_dataset = RandomSegmentBirdDataset(\n",
    "    X_train_df_balanced, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_train_one_hot_balanced,\n",
    "    augmentations=audio_augmentations,\n",
    "    image_augmentations=image_augmentations  # NUOVO!\n",
    ")\n",
    "\n",
    "print(\"Creazione dataset di validation...\")\n",
    "val_dataset = RandomSegmentBirdDataset(\n",
    "    X_val_df, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_val_one_hot,\n",
    "    augmentations=val_augmentations,\n",
    "    image_augmentations=None  # Nessuna augmentation per validation\n",
    ")\n",
    "\n",
    "# === DATALOADER CON MIXUP SEMPRE ATTIVO ===\n",
    "# Training con Mixup + CutMix\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: mixup_cutmix_collate(\n",
    "        batch, \n",
    "        p_mixup=1.0,      # SEMPRE Mixup (come richiesto dai vincitori)\n",
    "        p_cutmix=0.25,    # 25% delle volte CutMix invece di Mixup normale\n",
    "        alpha=0.2         # Parametro Beta per Mixup\n",
    "    )\n",
    ")\n",
    "\n",
    "# Validation senza augmentations\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")\n",
    "print(\"=== AUGMENTATIONS ATTIVE ===\")\n",
    "print(\"✅ Random Segment Selection (100%)\")\n",
    "print(\"✅ Random Erasing (30%)\")\n",
    "print(\"✅ Time/Frequency Masking (50%)\")\n",
    "print(\"✅ Mixup (100% - sempre attivo)\")\n",
    "print(\"✅ CutMix (25% delle volte invece di Mixup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73cddf",
   "metadata": {
    "papermill": {
     "duration": 0.015869,
     "end_time": "2025-05-23T09:04:59.625046",
     "exception": false,
     "start_time": "2025-05-23T09:04:59.609177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Definizione del Modello CNN\n",
    "\n",
    "## Modello EfficientNet con Head Personalizzata\n",
    "\n",
    "Implementiamo un modello basato su EfficientNet-B0 preaddestrato, con:\n",
    "- Supporto per input a singolo canale (spettrogrammi Mel)\n",
    "- Testa di classificazione personalizzata con dropout e normalizzazione batch\n",
    "- Parametri differenziati per l'ottimizzazione\n",
    "- Gestione automatica dei checkpoint e dei pesi preaddestrati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22ddec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.702431Z",
     "iopub.status.busy": "2025-06-10T11:16:52.702219Z",
     "iopub.status.idle": "2025-06-10T11:16:52.711839Z",
     "shell.execute_reply": "2025-06-10T11:16:52.711274Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.702406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WeightedFocalBCELoss(nn.Module):\n",
    "    def __init__(self, class_counts, gamma=2, alpha_rare=3.0, alpha_common=0.5):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # MIGLIORAMENTO: Calcolo pesi più aggressivo per sbilanciamento estremo\n",
    "        total_samples = sum(class_counts.values())\n",
    "        self.class_weights = {}\n",
    "        \n",
    "        for species, count in class_counts.items():\n",
    "            frequency = count / total_samples\n",
    "            \n",
    "            # CORRETTO: Soglie più aggressive per sbilanciamento estremo\n",
    "            if frequency < 0.002:  # < 0.2% → peso molto alto (classi estremamente rare)\n",
    "                weight = alpha_rare * 3  # 9.0 invece di 3.0\n",
    "            elif frequency < 0.005:  # < 0.5% → peso molto alto\n",
    "                weight = alpha_rare * 2  # 6.0 invece di 3.0\n",
    "            elif frequency < 0.01:   # < 1% → peso alto  \n",
    "                weight = alpha_rare      # 3.0\n",
    "            elif frequency > 0.05:   # > 5% → peso basso\n",
    "                weight = alpha_common    # 0.5\n",
    "            elif frequency > 0.1:    # > 10% → peso molto basso\n",
    "                weight = alpha_common * 0.3  # 0.15 invece di 0.25\n",
    "            else:\n",
    "                # Peso inversamente proporzionale con smoothing migliorato\n",
    "                weight = min(1.0 / (frequency + 0.0005), 4.0)  # Cap più alto per classi rare\n",
    "        \n",
    "            self.class_weights[species] = weight\n",
    "        \n",
    "        # Converte in tensore per GPU con ordine corretto delle specie\n",
    "        weight_tensor = torch.tensor([self.class_weights.get(species, 1.0) \n",
    "                                    for species in sorted(class_counts.keys())], dtype=torch.float32)\n",
    "        self.register_buffer('alpha_weights', weight_tensor)\n",
    "        \n",
    "        # DEBUG: Stampa statistiche sui pesi\n",
    "        weights_stats = torch.tensor(list(self.class_weights.values()))\n",
    "        print(f\"Pesi calcolati - Min: {weights_stats.min():.2f}, Max: {weights_stats.max():.2f}, Mean: {weights_stats.mean():.2f}\")\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # BCE Loss base\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Probabilità predette (applica sigmoid per ottenere pt corretto)\n",
    "        pt = torch.sigmoid(inputs)\n",
    "        # Calcola pt effettivo: pt per target=1, (1-pt) per target=0\n",
    "        pt = targets * pt + (1 - targets) * (1 - pt)\n",
    "        \n",
    "        # Componente Focal\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Applica pesi per classe\n",
    "        alpha_weights = self.alpha_weights.to(device).unsqueeze(0)\n",
    "        weighted_focal_loss = alpha_weights * focal_weight * bce_loss\n",
    "        \n",
    "        # MIGLIORAMENTO: Media pesata con gestione migliorata degli sbilanciamenti\n",
    "        # Peso alto per esempi positivi delle classi rare, peso basso per negativi\n",
    "        positive_weights = targets * alpha_weights\n",
    "        negative_weights = (1 - targets) * 0.05  # Peso molto basso per negativi\n",
    "        \n",
    "        sample_weights = positive_weights + negative_weights\n",
    "        total_weight = sample_weights.sum()\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            return (weighted_focal_loss * sample_weights).sum() / total_weight\n",
    "        else:\n",
    "            return weighted_focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb93042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:52.712871Z",
     "iopub.status.busy": "2025-06-10T11:16:52.712634Z",
     "iopub.status.idle": "2025-06-10T11:16:53.361610Z",
     "shell.execute_reply": "2025-06-10T11:16:53.360932Z",
     "shell.execute_reply.started": "2025-06-10T11:16:52.712850Z"
    },
    "papermill": {
     "duration": 64.446572,
     "end_time": "2025-05-23T09:06:04.087781",
     "exception": false,
     "start_time": "2025-05-23T09:04:59.641209",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNetBirdClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=config.N_CLASSES, pretrained=True, model_name='efficientnet_b0'):\n",
    "        super(EfficientNetBirdClassifier, self).__init__()\n",
    "        \n",
    "        # Tenta di caricare il modello con pesi pre-addestrati, gestendo fallimenti di connessione\n",
    "        try:\n",
    "            if pretrained:\n",
    "                print(f\"Tentativo di caricare {model_name} con pesi pre-addestrati...\")\n",
    "                self.efficientnet = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=True,\n",
    "                    num_classes=0  # Rimuovi il classificatore originale\n",
    "                )\n",
    "                print(\"Modello caricato con successo con pesi pre-addestrati.\")\n",
    "            else:\n",
    "                # Se pretrained=False, non tentare di scaricare i pesi\n",
    "                self.efficientnet = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=False,  # CORRETTO: ora è False\n",
    "                    num_classes=0  # Rimuovi il classificatore originale\n",
    "                )\n",
    "                print(\"Modello inizializzato senza pesi pre-addestrati (modalità offline).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento dei pesi pre-addestrati: {e}\")\n",
    "            print(\"Inizializzazione del modello senza pesi pre-addestrati...\")\n",
    "            self.efficientnet = timm.create_model(\n",
    "                model_name,\n",
    "                pretrained=False,\n",
    "                num_classes=0  # Rimuovi il classificatore originale\n",
    "            )\n",
    "        \n",
    "        # Ottieni la dimensione dell'output del feature extractor\n",
    "        if hasattr(self.efficientnet, 'num_features'):\n",
    "            classifier_in_features = self.efficientnet.num_features\n",
    "        elif hasattr(self.efficientnet, 'classifier'):\n",
    "            classifier_in_features = self.efficientnet.classifier.in_features\n",
    "        else:\n",
    "            # Valore predefinito per EfficientNet-B0\n",
    "            classifier_in_features = 1280\n",
    "        \n",
    "         # Sostituisci il classificatore semplice con una MLP con dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),  # Primo dropout significativo\n",
    "            nn.Linear(classifier_in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),  # Secondo dropout più leggero\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Se l'input è un'immagine a 1 canale, replicala su 3 canali\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Passa l'input attraverso il backbone per ottenere le features\n",
    "        features = self.efficientnet(x)\n",
    "        \n",
    "        # Passa le feature attraverso il classificatore\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Controlla se esiste un checkpoint precedente\n",
    "has_previous_checkpoint = False\n",
    "if config.environment == 'kaggle':\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(latest_checkpoint)\n",
    "elif config.environment == 'colab':\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(drive_checkpoint)\n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    has_previous_checkpoint = os.path.exists(local_checkpoint)\n",
    "\n",
    "# Inizializza il modello - usa pretrained=False se hai già un checkpoint\n",
    "model = EfficientNetBirdClassifier(\n",
    "    num_classes=config.N_CLASSES, \n",
    "    pretrained=not has_previous_checkpoint  # Scarica i pesi solo se non c'è già un checkpoint\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Definiamo ottimizzatore con learning rate differenziati\n",
    "def get_optimizer(model, lr_base=5e-4, lr_head=1e-3):  # Ridotti significativamente\n",
    "    # Parametri del corpo (pre-addestrati)\n",
    "    backbone_params = [p for name, p in model.named_parameters() \n",
    "                      if 'classifier' not in name]\n",
    "    \n",
    "    # Parametri della testa (da addestrare da zero)\n",
    "    classifier_params = [p for name, p in model.named_parameters() \n",
    "                      if 'classifier' in name]\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': lr_base},  # Ridotto da 1e-3 a 5e-4\n",
    "        {'params': classifier_params, 'lr': lr_head}  # Ridotto da 3e-3 a 1e-3\n",
    "    ], weight_decay=3e-4)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Dopo aver caricato train_df, calcola le frequenze delle classi\n",
    "primary_species_count = train_df['primary_label'].value_counts().to_dict()\n",
    "\n",
    "# Sostituisci la loss function\n",
    "criterion = WeightedFocalBCELoss(\n",
    "    class_counts=primary_species_count,\n",
    "    gamma=2,\n",
    "    alpha_rare=3.0,    # Peso 3x per classi rare\n",
    "    alpha_common=0.5   # Peso 0.5x per classi comuni\n",
    ")\n",
    "\n",
    "# Ottimizzatore con learning rate differenziati\n",
    "optimizer = get_optimizer(model)\n",
    "\n",
    "# Learning rate scheduler - aggiornato a CosineAnnealingLR come usato dai vincitori\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.EPOCHS,  # Numero totale di epoche\n",
    "    eta_min=1e-6  # Learning rate minimo\n",
    ")\n",
    "\n",
    "print(f\"Modello EfficientNet-B0 (timm) caricato su {config.DEVICE}\")\n",
    "print(f\"Numero di classi: {config.N_CLASSES}\")\n",
    "print(f\"Parametri totali: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffabba6",
   "metadata": {
    "papermill": {
     "duration": 0.016226,
     "end_time": "2025-05-23T09:06:04.121756",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.105530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Addestramento e Validazione del Modello\n",
    "\n",
    "## Funzione di Training con Supporto per Checkpoint\n",
    "\n",
    "La funzione di training implementa:\n",
    "- Caricamento automatico dei checkpoint precedenti\n",
    "- Early stopping basato sulle performance di validation\n",
    "- Salvataggio periodico dei checkpoint e del miglior modello\n",
    "- Supporto per scheduler di learning rate (CosineAnnealingLR)\n",
    "- Visualizzazione delle curve di loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7244c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:16:53.362828Z",
     "iopub.status.busy": "2025-06-10T11:16:53.362540Z",
     "iopub.status.idle": "2025-06-10T11:16:53.389769Z",
     "shell.execute_reply": "2025-06-10T11:16:53.388983Z",
     "shell.execute_reply.started": "2025-06-10T11:16:53.362804Z"
    },
    "papermill": {
     "duration": 0.058751,
     "end_time": "2025-05-23T09:06:04.196779",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.138028",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                model_save_path=None, model_load_path=None, patience=3,\n",
    "                resume_training=True, scheduler=None):\n",
    "    \"\"\"\n",
    "    Addestra il modello e valuta su validation set con supporto per checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da addestrare\n",
    "        train_loader: DataLoader per dati di training\n",
    "        val_loader: DataLoader per dati di validation\n",
    "        criterion: Funzione di loss\n",
    "        optimizer: Ottimizzatore\n",
    "        epochs: Numero di epoche di training\n",
    "        device: Device per l'addestramento ('cuda' o 'cpu')\n",
    "        model_save_path: Path dove salvare il modello addestrato\n",
    "        model_load_path: Path da cui caricare un modello pre-addestrato\n",
    "        patience: Numero di epoche senza miglioramento prima di terminare l'addestramento\n",
    "        resume_training: Se True, riprende il training da un checkpoint (se disponibile)\n",
    "        scheduler: Learning rate scheduler\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, total_training_time)\n",
    "    \"\"\"\n",
    "    # Directory per i checkpoint in base all'ambiente\n",
    "    checkpoint_dir = None\n",
    "    drive_mounted = False\n",
    "    \n",
    "    # Configura la directory per i checkpoint a seconda dell'ambiente\n",
    "    if config.environment == 'colab':\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            # Controlla se il drive è già montato\n",
    "            if not os.path.exists('/content/drive'):\n",
    "                print(\"Montaggio di Google Drive...\")\n",
    "                drive.mount('/content/drive')\n",
    "                print(\"Google Drive montato con successo.\")\n",
    "            \n",
    "            # Crea directory per i checkpoint se non esiste\n",
    "            checkpoint_dir = '/content/drive/MyDrive/birdclef_checkpoints'\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            print(f\"Directory per i checkpoint creata su Google Drive: {checkpoint_dir}\")\n",
    "            \n",
    "            # Aggiorna il percorso di salvataggio per usare Google Drive\n",
    "            if model_save_path:\n",
    "                filename = os.path.basename(model_save_path)\n",
    "                model_save_path = os.path.join(checkpoint_dir, filename)\n",
    "                print(f\"Il modello sarà salvato in: {model_save_path}\")\n",
    "            \n",
    "            drive_mounted = True\n",
    "        except ImportError:\n",
    "            print(\"Errore: Non riesco ad accedere a Google Drive. Continuo senza persistenza.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
    "            print(\"Continuo senza persistenza su Drive.\")\n",
    "    elif config.environment == 'kaggle':\n",
    "        # In Kaggle, usa la directory di working\n",
    "        checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in Kaggle: {checkpoint_dir}\")\n",
    "    else:\n",
    "        # In locale, usa la directory 'checkpoints' nell'OUTPUT_DIR\n",
    "        checkpoint_dir = os.path.join(config.OUTPUT_DIR, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in locale: {checkpoint_dir}\")\n",
    "    \n",
    "    # Inizializzazione variabili\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    total_training_time = 0\n",
    "    start_epoch = 0\n",
    "    needs_training = True\n",
    "    checkpoint_exists = False\n",
    "    model_loaded = False\n",
    "    \n",
    "    # Verifica se esiste un modello pre-addestrato da caricare\n",
    "    if model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Modello trovato in {model_load_path}. Tentativo di caricamento...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            \n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            print(\"Modello caricato con successo.\")\n",
    "            model_loaded = True\n",
    "            needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "            print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            needs_training = True\n",
    "    else:\n",
    "        print(f\"Modello non trovato in {model_load_path}.\")\n",
    "    \n",
    "    # Cerca un checkpoint SOLO se il caricamento del modello è fallito E resume_training è True\n",
    "    if needs_training and resume_training and checkpoint_dir and not model_loaded:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint.pth\")\n",
    "        if os.path.exists(latest_checkpoint):\n",
    "            print(f\"Trovato checkpoint in {latest_checkpoint}. Tentativo di caricamento...\")\n",
    "            try:\n",
    "                checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "                \n",
    "                # Verifica che sia un checkpoint compatibile prima di caricarlo\n",
    "                if isinstance(checkpoint, dict) and 'epoch' in checkpoint:\n",
    "                    try:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        start_epoch = checkpoint['epoch'] + 1\n",
    "                        train_losses = checkpoint['train_losses']\n",
    "                        val_losses = checkpoint['val_losses']\n",
    "                        best_val_loss = checkpoint['best_val_loss']\n",
    "                        epochs_without_improvement = checkpoint['epochs_without_improvement']\n",
    "                        total_training_time = checkpoint.get('total_training_time', 0)\n",
    "                        \n",
    "                        # Ricrea lo scheduler con lo stato salvato se presente\n",
    "                        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "                            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                        \n",
    "                        print(f\"Checkpoint caricato con successo (epoca {start_epoch-1})\")\n",
    "                        print(f\"Si riparte dall'epoca {start_epoch}/{epochs}\")\n",
    "                        \n",
    "                        if start_epoch >= epochs:\n",
    "                            needs_training = False\n",
    "                        \n",
    "                        checkpoint_exists = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Il checkpoint non è compatibile con il modello attuale: {e}\")\n",
    "                        print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il caricamento del checkpoint: {e}\")\n",
    "                print(\"Si procederà con il training da zero.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Loop di training sulle epoche (inizia da start_epoch)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            # Aggiornamento scheduler - modificato per CosineAnnealingLR\n",
    "            if scheduler is not None:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_val_loss)  # Per ReduceLROnPlateau\n",
    "                else:\n",
    "                    scheduler.step()  # Per CosineAnnealingLR è step() senza parametri\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            total_training_time += epoch_duration\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Duration: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            # Salvataggio checkpoint per ogni epoca (in qualsiasi ambiente)\n",
    "            if checkpoint_dir:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"birdclef_epoch_{epoch+1}.pth\")\n",
    "                \n",
    "                # Salva checkpoint completo con tutte le informazioni di stato\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_without_improvement': epochs_without_improvement,\n",
    "                    'total_training_time': total_training_time\n",
    "                }\n",
    "                \n",
    "                # Salva anche lo stato dello scheduler se esiste\n",
    "                if scheduler is not None:\n",
    "                    checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                print(f\"Checkpoint completo salvato in {checkpoint_path}\")\n",
    "                \n",
    "                # Aggiorna anche il checkpoint più recente (sovrascrive)\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_dir, \"latest_checkpoint.pth\"))\n",
    "            \n",
    "            # Early stopping\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                # Salva il miglior modello separatamente\n",
    "                if model_save_path:\n",
    "                    best_path = model_save_path.replace('.pth', '_best.pth')\n",
    "                    \n",
    "                    # Salva checkpoint completo\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_losses': train_losses,\n",
    "                        'val_losses': val_losses,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }\n",
    "                    \n",
    "                    # Salva anche lo stato dello scheduler\n",
    "                    if scheduler is not None:\n",
    "                        best_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                    \n",
    "                    torch.save(best_checkpoint, best_path)\n",
    "                    print(f\"Salvato miglior modello in {best_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping attivato! Nessun miglioramento per {patience} epoche consecutive.\")\n",
    "                break\n",
    "        \n",
    "        end_time_total = time.time()\n",
    "        if checkpoint_exists:\n",
    "            total_training_time += (end_time_total - start_time_total)\n",
    "        else:\n",
    "            total_training_time = end_time_total - start_time_total\n",
    "            \n",
    "        print(f\"\\nTraining terminato in {total_training_time/60:.2f} minuti totali\")\n",
    "        \n",
    "        # Salva il modello finale\n",
    "        if model_save_path:\n",
    "            final_checkpoint = {\n",
    "                'epoch': epochs-1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'total_training_time': total_training_time\n",
    "            }\n",
    "            \n",
    "            # Salva anche lo stato dello scheduler\n",
    "            if scheduler is not None:\n",
    "                final_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "            torch.save(final_checkpoint, model_save_path)\n",
    "            print(f\"Modello finale salvato in {model_save_path}\")\n",
    "    else:\n",
    "        print(\"Training non necessario: modello già caricato o training ripreso e completato.\")\n",
    "    \n",
    "    # Visualizza le curve di loss\n",
    "    if train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Curve di Loss di Training e Validation')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Salva il grafico\n",
    "        if checkpoint_dir:\n",
    "            plt_path = os.path.join(checkpoint_dir, 'loss_curves.png')\n",
    "            plt.savefig(plt_path)\n",
    "            print(f\"Grafico delle curve di loss salvato in {plt_path}\")\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e87bd",
   "metadata": {
    "papermill": {
     "duration": 0.016154,
     "end_time": "2025-05-23T09:06:04.229449",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.213295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configurazione e Avvio del Training\n",
    "\n",
    "Configuriamo e avviamo il training del modello:\n",
    "- Individuazione automatica dei checkpoint precedenti\n",
    "- Inizializzazione dell'ottimizzatore e dello scheduler\n",
    "- Avvio del training con i parametri ottimizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dabb8",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z",
     "iopub.execute_input": "2025-06-10T11:16:53.391196Z",
     "iopub.status.busy": "2025-06-10T11:16:53.390689Z"
    },
    "papermill": {
     "duration": 0.847329,
     "end_time": "2025-05-23T09:06:05.092857",
     "exception": false,
     "start_time": "2025-05-23T09:06:04.245528",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    # Directory per i checkpoint in Kaggle\n",
    "    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Verifica se esiste un checkpoint precedente\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        model_load_path = latest_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente in {latest_checkpoint}\")\n",
    "    else:\n",
    "        # Usa un modello base precaricato se disponibile\n",
    "        model_load_path = \"/kaggle/input/efficientnet_data_aug/pytorch/default/1/birdclef_efficientNET_dataAug_timm_best.pth\"\n",
    "        \n",
    "    # Imposta il percorso di salvataggio\n",
    "    model_save_path = \"/kaggle/working/birdclef_efficientNET_NEwdataAug5SecFBCE.pth\"\n",
    "    \n",
    "elif config.environment == 'colab':\n",
    "    # Per Colab, verifica se esiste un checkpoint su Drive\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(drive_checkpoint):\n",
    "        model_load_path = drive_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente su Drive: {drive_checkpoint}\")\n",
    "    else:\n",
    "        # Altrimenti usa un modello base se disponibile\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "    \n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    if os.path.exists(local_checkpoint):\n",
    "        model_load_path = local_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente: {local_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"baseline_bird_cnn_model_val.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_model_timm_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Addestra il modello con CosineAnnealingLR scheduler\n",
    "train_losses, val_losses, training_time = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,  # Passa lo scheduler\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path,\n",
    "    resume_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035fd7",
   "metadata": {
    "papermill": {
     "duration": 0.017078,
     "end_time": "2025-05-23T09:06:05.129361",
     "exception": false,
     "start_time": "2025-05-23T09:06:05.112283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Generazione della Submission\n",
    "\n",
    "## Generazione della Submission\n",
    "\n",
    "Implementiamo la funzione per generare le predizioni sui file di test:\n",
    "- Funzione di temporal smoothing\n",
    "- Caricamento e segmentazione delle soundscape di test\n",
    "- Estrazione di spettrogrammi Mel da ciascun segmento\n",
    "- Generazione delle predizioni con il modello addestrato\n",
    "- Creazione del file di submission nel formato richiesto dalla competizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07e1ed",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_temporal_smoothing(predictions_df):\n",
    "    \"\"\"\n",
    "    Applica smoothing temporale alle predizioni per migliorare coerenza tra timestep adiacenti.\n",
    "    Esattamente come implementato dal top performer.\n",
    "    \"\"\"\n",
    "    cols = predictions_df.columns[1:]  # Tutte le colonne eccetto row_id\n",
    "    # Estrai il nome del file dal row_id (tutto fino all'ultimo underscore)\n",
    "    groups = predictions_df['row_id'].str.rsplit('_', n=1).str[0]\n",
    "    \n",
    "    # Per ogni file audio\n",
    "    for group in np.unique(groups):\n",
    "        # Filtra le righe di questo file\n",
    "        mask = groups == group\n",
    "        sub_group = predictions_df[mask].copy()\n",
    "        \n",
    "        # Ottieni solo le colonne con i punteggi\n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        \n",
    "        # Applica smoothing per tutti i punti temporali tranne primo e ultimo\n",
    "        for i in range(1, predictions.shape[0]-1):\n",
    "            new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "        \n",
    "        # Gestisci separatamente primo e ultimo punto temporale\n",
    "        if predictions.shape[0] > 1:\n",
    "            new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "            new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "        \n",
    "        # Aggiorna le predizioni\n",
    "        predictions_df.loc[mask, cols] = new_predictions\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee4056",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z"
    },
    "papermill": {
     "duration": 0.085292,
     "end_time": "2025-05-23T09:06:05.231290",
     "exception": false,
     "start_time": "2025-05-23T09:06:05.145998",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_submission_with_overlap(model, device=config.DEVICE, overlap=2.5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Genera submission usando la STESSA pipeline del training.\n",
    "    CORRETTO: Genera predizioni ogni 5 secondi come richiesto.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Set seed per riproducibilità\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Percorso dei test soundscapes\n",
    "    test_soundscape_path = config.TEST_SOUNDSCAPES_DIR\n",
    "    test_soundscapes = [os.path.join(test_soundscape_path, afile) \n",
    "                        for afile in sorted(os.listdir(test_soundscape_path)) \n",
    "                        if afile.endswith('.ogg')]\n",
    "    \n",
    "    print(f\"Elaborazione di {len(test_soundscapes)} file soundscape con overlap di {overlap}s...\")\n",
    "    \n",
    "    # Dizionario per accumulare predizioni per ogni target timestamp\n",
    "    accumulated_predictions = {}\n",
    "    \n",
    "    for soundscape in tqdm(test_soundscapes, desc=\"Elaborazione soundscapes\"):\n",
    "        # USA TORCHAUDIO COME NEL TRAINING\n",
    "        waveform, sr = torchaudio.load(soundscape)\n",
    "        \n",
    "        # Converti a mono se necessario\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform[0:1]\n",
    "        \n",
    "        # Ricampiona se necessario\n",
    "        if sr != config.SR:\n",
    "            resampler = T.Resample(sr, config.SR)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        file_name = os.path.basename(soundscape).split('.')[0]\n",
    "        audio_duration = waveform.shape[1] / config.SR\n",
    "        \n",
    "        # CORREZIONE: Genera timestamp ogni 5 secondi per tutto il file\n",
    "        target_timestamps = list(range(5, int(audio_duration) + 1, 5))\n",
    "        print(f\"File {file_name}: durata {audio_duration:.1f}s, {len(target_timestamps)} timestamp da predire\")\n",
    "        \n",
    "        # Parametri per sliding window di predizione\n",
    "        window_size = int(config.SR * config.DURATION)  # 10 secondi di finestra\n",
    "        step_size = int(config.SR * overlap)  # Passo di 2.5 secondi\n",
    "        \n",
    "        # Per ogni timestamp target (ogni 5 secondi)\n",
    "        for target_time in target_timestamps:\n",
    "            row_id = f\"{file_name}_{target_time}\"\n",
    "            \n",
    "            # Raccogli tutte le predizioni che coprono questo timestamp\n",
    "            predictions_for_timestamp = []\n",
    "            weights_for_timestamp = []\n",
    "            \n",
    "            # Trova tutte le finestre che coprono questo timestamp\n",
    "            for start_idx in range(0, waveform.shape[1], step_size):\n",
    "                segment_start_time = start_idx / config.SR\n",
    "                segment_end_time = segment_start_time + config.DURATION\n",
    "                \n",
    "                # Se questo timestamp cade dentro questa finestra\n",
    "                if segment_start_time <= target_time <= segment_end_time:\n",
    "                    # Estrai segmento di 10 secondi\n",
    "                    segment = waveform[:, start_idx:start_idx + window_size]\n",
    "                    \n",
    "                    # Se il segmento è troppo corto, padda\n",
    "                    if segment.shape[1] < window_size:\n",
    "                        padding = window_size - segment.shape[1]\n",
    "                        segment = torch.nn.functional.pad(segment, (0, padding))\n",
    "                    \n",
    "                    # USA LA STESSA PIPELINE DEL TRAINING\n",
    "                    mel_spectrogram = mel_transform(segment)\n",
    "                    log_mel_spec = amplitude_to_db(mel_spectrogram)\n",
    "                    \n",
    "                    # Aggiungi dimensione batch\n",
    "                    input_tensor = log_mel_spec.unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        output = model(input_tensor)\n",
    "                        scores = torch.sigmoid(output).cpu().numpy()[0]\n",
    "                    \n",
    "                    # Calcola il peso in base alla posizione del timestamp nella finestra\n",
    "                    relative_position = (target_time - segment_start_time) / config.DURATION\n",
    "                    # Peso massimo al centro (0.5), minimo ai bordi (0, 1)\n",
    "                    distance_from_center = abs(relative_position - 0.5)\n",
    "                    weight = 1.0 - distance_from_center * (1.0 - alpha)\n",
    "                    \n",
    "                    predictions_for_timestamp.append(scores)\n",
    "                    weights_for_timestamp.append(weight)\n",
    "            \n",
    "            # Combina le predizioni pesate per questo timestamp\n",
    "            if predictions_for_timestamp:\n",
    "                predictions_array = np.array(predictions_for_timestamp)\n",
    "                weights_array = np.array(weights_for_timestamp)\n",
    "                \n",
    "                # Media pesata\n",
    "                weighted_prediction = np.average(predictions_array, axis=0, weights=weights_array)\n",
    "                accumulated_predictions[row_id] = weighted_prediction\n",
    "            else:\n",
    "                # Se nessuna finestra copre questo timestamp, usa predizioni zero\n",
    "                accumulated_predictions[row_id] = np.zeros(len(all_species))\n",
    "    \n",
    "    # Crea il DataFrame di submission dalle predizioni accumulate\n",
    "    predictions_list = []\n",
    "    for row_id, scores in accumulated_predictions.items():\n",
    "        predictions_list.append([row_id] + list(scores))\n",
    "    \n",
    "    predictions = pd.DataFrame(predictions_list, columns=['row_id'] + all_species)\n",
    "    \n",
    "    # Ordina per row_id per avere un output coerente\n",
    "    predictions = predictions.sort_values('row_id').reset_index(drop=True)\n",
    "    \n",
    "    # Applica lo smoothing temporale\n",
    "    print(\"Applicazione smoothing temporale alle predizioni...\")\n",
    "    predictions = apply_temporal_smoothing(predictions)\n",
    "    \n",
    "    # Clip dei valori tra 0 e 1 per sicurezza\n",
    "    for col in predictions.columns:\n",
    "        if col != 'row_id':\n",
    "            predictions[col] = predictions[col].clip(0, 1)\n",
    "\n",
    "    # Salva la submission come CSV\n",
    "    predictions.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"Submission salvata con {len(predictions)} predizioni.\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8151aac",
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T11:17:28.324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modifica la chiamata per usare la nuova funzione\n",
    "if config.environment == 'kaggle':\n",
    "    print(\"\\nGenerazione del file di submission con overlap e combinazione pesata...\")\n",
    "    submission_df = generate_submission_with_overlap(model, overlap=2.5, alpha=0.5)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\nAnteprima del file di submission:\")\n",
    "        print(submission_df.head())\n",
    "else:\n",
    "    print(\"\\nSalto la generazione della submission perché non siamo su Kaggle.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2958.165303,
   "end_time": "2025-05-23T09:06:08.504478",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-23T08:16:50.339175",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "019fbbcbfdba4ddcb43fb56b89d6f838": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "0e6983f29cc74e13aca0f83533f7b614": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "115e833a1e2c4af39e3114e86b5a6ab3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1676e31d4c4f47c9baf5ec751d18c852": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4776353c32654130b576919e03430fba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4cfad99b490345cd96beb56b9f0b1be5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_019fbbcbfdba4ddcb43fb56b89d6f838",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1676e31d4c4f47c9baf5ec751d18c852",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "4eccaf67e74748dfa07fc2012bab7b38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_776ce6e2cdec43fc8cabbde8fd761761",
       "placeholder": "​",
       "style": "IPY_MODEL_d8f43c7e3c4740c5bc38eaddb2b5d58f",
       "tabbable": null,
       "tooltip": null,
       "value": "Preparazione dataset adattivo: 100%"
      }
     },
     "516a0dbb64754d7a890a368ff27fc31d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59621d93883346c6a2c056ccfc81c7a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0e6983f29cc74e13aca0f83533f7b614",
       "placeholder": "​",
       "style": "IPY_MODEL_c887db35485246d68c555172293290f7",
       "tabbable": null,
       "tooltip": null,
       "value": " 22144/22144 [47:27&lt;00:00,  8.19it/s]"
      }
     },
     "776ce6e2cdec43fc8cabbde8fd761761": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c436e69cc1f402fbc34a644d082d965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "827b45a7e0fa4bf2b3ccb8bfc15e7015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8f4a5b09e08c49048c3f3ff6e8148381",
        "IPY_MODEL_4cfad99b490345cd96beb56b9f0b1be5",
        "IPY_MODEL_a0376a34ebb84e7b868ce7afe6b27ec5"
       ],
       "layout": "IPY_MODEL_f5e68b30d7704b3b94c35dfa95547613",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8f4a5b09e08c49048c3f3ff6e8148381": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7c436e69cc1f402fbc34a644d082d965",
       "placeholder": "​",
       "style": "IPY_MODEL_c2ab18fa36cf46acbf93d02c43a59673",
       "tabbable": null,
       "tooltip": null,
       "value": "Elaborazione soundscapes: "
      }
     },
     "9ec1ed79d57e4dc78c7896e41c727c86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a0376a34ebb84e7b868ce7afe6b27ec5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_115e833a1e2c4af39e3114e86b5a6ab3",
       "placeholder": "​",
       "style": "IPY_MODEL_516a0dbb64754d7a890a368ff27fc31d",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "a186aabdde28484ba9084312371f3193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e0a8185c24844db298d59005e31c1e41",
       "max": 22144,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ec1ed79d57e4dc78c7896e41c727c86",
       "tabbable": null,
       "tooltip": null,
       "value": 22144
      }
     },
     "c2ab18fa36cf46acbf93d02c43a59673": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c887db35485246d68c555172293290f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c8e92d36ad3f48ccac59f575ebe68759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4eccaf67e74748dfa07fc2012bab7b38",
        "IPY_MODEL_a186aabdde28484ba9084312371f3193",
        "IPY_MODEL_59621d93883346c6a2c056ccfc81c7a4"
       ],
       "layout": "IPY_MODEL_4776353c32654130b576919e03430fba",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d8f43c7e3c4740c5bc38eaddb2b5d58f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0a8185c24844db298d59005e31c1e41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5e68b30d7704b3b94c35dfa95547613": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
