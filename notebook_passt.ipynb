{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con CNN\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza un'architettura CNN per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importazione delle Librerie Necessarie\n",
    "\n",
    "Importiamo tutte le librerie necessarie per l'elaborazione audio, deep learning e visualizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:39:56.650248Z",
     "iopub.status.busy": "2025-05-14T17:39:56.649850Z",
     "iopub.status.idle": "2025-05-14T17:40:06.789170Z",
     "shell.execute_reply": "2025-05-14T17:40:06.787940Z",
     "shell.execute_reply.started": "2025-05-14T17:39:56.650210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, AutoConfig\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# Sostituisci le importazioni di Transformers con timm\n",
    "import timm\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"timm versione: {timm.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Imposta questo a True per abilitare la cancellazione\n",
    "clear_working_dir = True\n",
    "\n",
    "working_dir = '/kaggle/working/'\n",
    "\n",
    "if clear_working_dir:\n",
    "    for filename in os.listdir(working_dir):\n",
    "        file_path = os.path.join(working_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # elimina file o link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # elimina directory\n",
    "        except Exception as e:\n",
    "            print(f'Errore durante la rimozione di {file_path}: {e}')\n",
    "    print(f\"Tutti i file in {working_dir} sono stati rimossi.\")\n",
    "else:\n",
    "    print(\"Pulizia disabilitata (clear_working_dir = False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurazione dell'Ambiente di Esecuzione\n",
    "\n",
    "In questa sezione configuriamo l'ambiente di esecuzione in modo che il notebook funzioni sia su Kaggle, che su Google Colab, che in locale. Il codice rileverà automaticamente l'ambiente e configurerà i percorsi di conseguenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.791974Z",
     "iopub.status.busy": "2025-05-14T17:40:06.791352Z",
     "iopub.status.idle": "2025-05-14T17:40:06.801870Z",
     "shell.execute_reply": "2025-05-14T17:40:06.800513Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.791943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.803494Z",
     "iopub.status.busy": "2025-05-14T17:40:06.803146Z",
     "iopub.status.idle": "2025-05-14T17:40:06.830225Z",
     "shell.execute_reply": "2025-05-14T17:40:06.828841Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.803460Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            # quindi non impostiamo ancora i percorsi derivati\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio - già allineati con vincitori\n",
    "        self.SR = 32000      # Sample rate\n",
    "        self.DURATION = 5    # Durata dei clip in secondi\n",
    "        self.N_MELS = 128    # Numero di bande Mel\n",
    "        self.N_FFT = 1024    # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 500  # Hop length per STFT\n",
    "        self.FMIN = 40       # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = 15000    # Frequenza massima\n",
    "        self.POWER = 2       # Esponente per calcolo spettrogramma\n",
    "            \n",
    "        # Parametri per il training - aggiornati secondo i vincitori\n",
    "        self.BATCH_SIZE = 96  # Aumentato da 32 a 96 come dai vincitori\n",
    "        self.EPOCHS = 10     # Numero di epoche per il training\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 4  # Aumentato per migliorare il data loading\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.832040Z",
     "iopub.status.busy": "2025-05-14T17:40:06.831609Z",
     "iopub.status.idle": "2025-05-14T17:40:06.865978Z",
     "shell.execute_reply": "2025-05-14T17:40:06.864801Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.832012Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/simplecnn/PyTorch/default/1\"\n",
    "    cache_model_file = os.path.join(cache_model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(cache_model_file)\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/simplecnn/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    model_file = os.path.join(model_path, \"baseline_bird_cnn_model_val.pth\")\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {model_file}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurazione del Modello e Parametri\n",
    "\n",
    "Definiamo i parametri di configurazione per il preprocessamento audio, la creazione dello spettrogramma Mel e l'addestramento della CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.869228Z",
     "iopub.status.busy": "2025-05-14T17:40:06.868890Z",
     "iopub.status.idle": "2025-05-14T17:40:06.900017Z",
     "shell.execute_reply": "2025-05-14T17:40:06.898591Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.869203Z"
    }
   },
   "outputs": [],
   "source": [
    "# I parametri principali sono già definiti nella classe Config\n",
    "# Verifichiamo l'esistenza delle directory e creiamo quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Caricamento e Preprocessing dei Dati\n",
    "\n",
    "In questa sezione carichiamo i metadati dal file CSV di training, creiamo codifiche one-hot per le etichette delle specie e implementiamo funzioni per il caricamento e preprocessamento dei file audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:06.901809Z",
     "iopub.status.busy": "2025-05-14T17:40:06.901297Z",
     "iopub.status.idle": "2025-05-14T17:40:08.193126Z",
     "shell.execute_reply": "2025-05-14T17:40:08.192038Z",
     "shell.execute_reply.started": "2025-05-14T17:40:06.901725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "    \n",
    "# Per Kaggle, dovremo creare un dataset speciale per le soundscapes di test\n",
    "# Questo verrà utilizzato direttamente nella fase di generazione della submission\n",
    "# Non creiamo X_test_df e test_dataset per ora\n",
    "X_test_df = None\n",
    "y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione di bilanciamento del dataset - Cancella una percentuale di esempi dalle classi molto numerose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset_df(train_df, labels_one_hot, abundant_class_threshold=200, remove_percentage=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame bilanciato rimuovendo parte degli esempi con rating bassi dalle classi abbondanti.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame originale\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        abundant_class_threshold: Soglia per definire una classe come \"abbondante\"\n",
    "        remove_percentage: Percentuale di esempi con rating 1-3 da rimuovere dalle classi abbondanti\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame bilanciato, etichette one-hot bilanciate)\n",
    "    \"\"\"\n",
    "    # Conta esempi per ogni classe\n",
    "    class_counts = train_df['primary_label'].value_counts()\n",
    "    \n",
    "    # Identifica classi abbondanti\n",
    "    abundant_classes = class_counts[class_counts > abundant_class_threshold].index.tolist()\n",
    "    print(f\"Classi identificate come abbondanti (>{abundant_class_threshold} esempi): {len(abundant_classes)}\")\n",
    "    \n",
    "    # Copia il DataFrame originale\n",
    "    balanced_df = train_df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Contatori per statistiche\n",
    "    total_removed = 0\n",
    "    removed_by_class = {}\n",
    "    \n",
    "    # Per ogni classe abbondante\n",
    "    for cls in abundant_classes:\n",
    "        # Filtra esempi con rating 1-3 per questa classe\n",
    "        low_quality_mask = (balanced_df['primary_label'] == cls) & (balanced_df['rating'].isin([1, 2, 3]))\n",
    "        low_quality_indices = balanced_df[low_quality_mask].index.tolist()\n",
    "        \n",
    "        # Numero di esempi da rimuovere\n",
    "        n_to_remove = int(len(low_quality_indices) * remove_percentage)\n",
    "        \n",
    "        # Seleziona casualmente gli indici da rimuovere\n",
    "        np.random.seed(random_state)\n",
    "        if n_to_remove > 0:\n",
    "            indices_to_remove = np.random.choice(low_quality_indices, size=n_to_remove, replace=False)\n",
    "            \n",
    "            # Memorizza gli indici da rimuovere\n",
    "            rows_to_drop.extend(indices_to_remove)\n",
    "            \n",
    "            # Aggiorna statistiche\n",
    "            removed_by_class[cls] = n_to_remove\n",
    "            total_removed += n_to_remove\n",
    "    \n",
    "    # Rimuovi le righe selezionate\n",
    "    if rows_to_drop:\n",
    "        balanced_df = balanced_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Aggiorna anche le etichette one-hot rimuovendo gli stessi indici\n",
    "        mask = np.ones(len(train_df), dtype=bool)\n",
    "        mask[rows_to_drop] = False\n",
    "        balanced_labels = labels_one_hot[mask]\n",
    "    else:\n",
    "        balanced_labels = labels_one_hot\n",
    "    \n",
    "    # Statistiche finali\n",
    "    print(f\"Totale esempi rimossi: {total_removed} ({total_removed/len(train_df):.1%} del dataset originale)\")\n",
    "    print(f\"Dimensione dataset originale: {len(train_df)}\")\n",
    "    print(f\"Dimensione dataset bilanciato: {len(balanced_df)}\")\n",
    "    \n",
    "    # Visualizza le prime 5 classi con maggiori rimozioni\n",
    "    if removed_by_class:\n",
    "        top_removed = sorted(removed_by_class.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nClassi con maggior numero di esempi rimossi:\")\n",
    "        for cls, count in top_removed:\n",
    "            original = class_counts[cls]\n",
    "            remaining = original - count\n",
    "            print(f\"- {cls}: {count} rimossi, {remaining}/{original} rimanenti ({remaining/original:.1%})\")\n",
    "    else:\n",
    "        print(\"Nessun esempio rimosso.\")\n",
    "    \n",
    "    return balanced_df, balanced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Analisi Esplorativa dei Dati (EDA)\n",
    "\n",
    "In questa sezione esploreremo le caratteristiche del dataset per comprendere meglio la distribuzione delle specie, le proprietà audio e identificare eventuali pattern nei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:08.194532Z",
     "iopub.status.busy": "2025-05-14T17:40:08.194196Z",
     "iopub.status.idle": "2025-05-14T17:40:32.789695Z",
     "shell.execute_reply": "2025-05-14T17:40:32.788430Z",
     "shell.execute_reply.started": "2025-05-14T17:40:08.194504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configurazione stile visualizzazioni\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"=== Statistiche di base del dataset ===\")\n",
    "print(f\"Numero totale di registrazioni: {len(train_df)}\")\n",
    "print(f\"Numero di specie uniche nel dataset: {len(all_species)}\")\n",
    "print(f\"Campi disponibili nei metadati: {train_df.columns.tolist()}\")\n",
    "\n",
    "# Verifichiamo i dati mancanti\n",
    "missing_data = train_df.isnull().sum()\n",
    "print(\"\\n=== Valori mancanti ===\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# 1. Distribuzione delle specie nel dataset (visualizzazione migliorata)\n",
    "print(\"\\n=== Analisi delle Specie ===\")\n",
    "primary_species_count = train_df['primary_label'].value_counts()\n",
    "\n",
    "# Plot combinato: distribuzione delle specie con evidenza delle classi rare\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# A sinistra: top 20 specie più rappresentate\n",
    "sns.barplot(x=primary_species_count.head(20).index, y=primary_species_count.head(20).values, ax=ax1)\n",
    "ax1.set_title('Top 20 Specie per Numero di Registrazioni')\n",
    "ax1.set_xlabel('Specie')\n",
    "ax1.set_ylabel('Numero di Registrazioni')\n",
    "ax1.tick_params(axis='x', rotation=90)\n",
    "\n",
    "# A destra: distribuzione del numero di esempi per specie\n",
    "sns.histplot(primary_species_count, bins=30, kde=True, ax=ax2)\n",
    "ax2.set_title('Distribuzione del Numero di Registrazioni per Specie')\n",
    "ax2.set_xlabel('Numero di Registrazioni')\n",
    "ax2.set_ylabel('Conteggio Specie')\n",
    "ax2.axvline(x=primary_species_count.median(), color='r', linestyle='--', \n",
    "            label=f'Mediana: {primary_species_count.median()}')\n",
    "ax2.axvline(x=primary_species_count.mean(), color='g', linestyle='--', \n",
    "            label=f'Media: {primary_species_count.mean():.1f}')\n",
    "ax2.axvline(x=50, color='orange', linestyle=':', label='Soglia classi rare (50)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcolo dell'indice di Gini per misurare lo sbilanciamento\n",
    "def gini_coefficient(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n+1)\n",
    "    return (np.sum((2*index - n - 1) * x)) / (n * np.sum(x))\n",
    "\n",
    "gini = gini_coefficient(primary_species_count.values)\n",
    "print(f\"\\nIndice di Gini per la distribuzione delle specie: {gini:.4f}\")\n",
    "print(f\"Questo indica {'un alto' if gini > 0.6 else 'un moderato' if gini > 0.3 else 'un basso'} livello di sbilanciamento nel dataset.\")\n",
    "\n",
    "# 2. NUOVA ANALISI: Rating per le classi con pochi esempi\n",
    "# Definisco la soglia per le classi rare (< 50 esempi)\n",
    "RARE_CLASS_THRESHOLD = 50\n",
    "rare_species = primary_species_count[primary_species_count < RARE_CLASS_THRESHOLD].index.tolist()\n",
    "print(f\"\\n=== Analisi dei Rating per Classi Rare (<{RARE_CLASS_THRESHOLD} esempi) ===\")\n",
    "print(f\"Numero di classi rare: {len(rare_species)} su {len(all_species)} totali ({len(rare_species)/len(all_species):.1%})\")\n",
    "\n",
    "# Raccolgo i dati sui rating per le classi rare\n",
    "rare_class_ratings = []\n",
    "for species in rare_species:\n",
    "    species_df = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_df['rating'].fillna(0).tolist()  # Sostituisco NaN con 0 (nessun rating)\n",
    "    \n",
    "    # Statistiche per questa specie\n",
    "    rare_class_ratings.append({\n",
    "        'species': species,\n",
    "        'count': len(species_df),\n",
    "        'avg_rating': np.mean(ratings),\n",
    "        'ratings': ratings,\n",
    "        'rating_counts': {r: ratings.count(r) for r in set(ratings)}\n",
    "    })\n",
    "\n",
    "# Creo DataFrame per analisi\n",
    "rare_ratings_df = pd.DataFrame(rare_class_ratings)\n",
    "rare_ratings_df = rare_ratings_df.sort_values('count')\n",
    "\n",
    "# Visualizzazione: Rating medi vs Conteggio per le classi rare\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Heatmap: distribuzione dei rating per classi rare\n",
    "n_rare_to_show = min(30, len(rare_ratings_df))  # Mostra max 30 classi per leggibilità\n",
    "rare_sample = rare_ratings_df.head(n_rare_to_show)\n",
    "\n",
    "# Preparo i dati per la heatmap\n",
    "heatmap_data = []\n",
    "rating_values = [0, 1, 2, 3, 4, 5]  # Tutti i possibili rating\n",
    "for _, row in rare_sample.iterrows():\n",
    "    species_data = [row['species'], row['count']]\n",
    "    for rating in rating_values:\n",
    "        species_data.append(row['rating_counts'].get(rating, 0))\n",
    "    heatmap_data.append(species_data)\n",
    "\n",
    "# Creo DataFrame per la heatmap\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data, \n",
    "    columns=['species', 'count'] + [f'rating_{r}' for r in rating_values]\n",
    ")\n",
    "\n",
    "# Plot combinato con scatter plot e heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Scatter plot: rating medio vs numero esempi\n",
    "sns.scatterplot(\n",
    "    x='count', \n",
    "    y='avg_rating', \n",
    "    data=rare_ratings_df, \n",
    "    ax=ax1, \n",
    "    alpha=0.7,\n",
    "    hue='count',\n",
    "    palette='viridis',\n",
    "    size='count',\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "ax1.set_title('Rating Medio vs Numero di Esempi per Classi Rare')\n",
    "ax1.set_xlabel('Numero di Esempi')\n",
    "ax1.set_ylabel('Rating Medio')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mostra statistiche\n",
    "avg_rating_rare = rare_ratings_df['avg_rating'].mean()\n",
    "ax1.axhline(y=avg_rating_rare, color='r', linestyle='--', \n",
    "           label=f'Rating medio classi rare: {avg_rating_rare:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Heatmap: distribuzione dei rating per le classi più rare\n",
    "pivot_data = pd.DataFrame({\n",
    "    'species': heatmap_df['species'],\n",
    "    'Rating 0': heatmap_df['rating_0'],\n",
    "    'Rating 1': heatmap_df['rating_1'],\n",
    "    'Rating 2': heatmap_df['rating_2'],\n",
    "    'Rating 3': heatmap_df['rating_3'],\n",
    "    'Rating 4': heatmap_df['rating_4'],\n",
    "    'Rating 5': heatmap_df['rating_5'],\n",
    "}).set_index('species')\n",
    "\n",
    "sns.heatmap(pivot_data, cmap=\"YlGnBu\", annot=True, fmt='g', ax=ax2)\n",
    "ax2.set_title(f'Distribuzione dei Rating nelle {n_rare_to_show} Classi più Rare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche aggregate sui rating per le classi rare\n",
    "print(\"\\nStatistiche sui rating per le classi rare:\")\n",
    "print(f\"- Rating medio complessivo: {rare_ratings_df['avg_rating'].mean():.2f}\")\n",
    "print(f\"- Percentuale di registrazioni senza rating (0): {sum(r['rating_counts'].get(0, 0) for r in rare_class_ratings) / sum(r['count'] for r in rare_class_ratings):.1%}\")\n",
    "\n",
    "# Analisi delle classi estremamente rare (≤ 5 esempi)\n",
    "very_rare_species = primary_species_count[primary_species_count <= 5].index.tolist()\n",
    "print(f\"\\nClassi estremamente rare (≤ 5 esempi): {len(very_rare_species)}\")\n",
    "\n",
    "very_rare_df = train_df[train_df['primary_label'].isin(very_rare_species)]\n",
    "print(\"Dettaglio delle registrazioni per le classi estremamente rare:\")\n",
    "for species in very_rare_species:\n",
    "    species_data = train_df[train_df['primary_label'] == species]\n",
    "    ratings = species_data['rating'].fillna(0).tolist()\n",
    "    print(f\"- {species}: {len(species_data)} esempi, ratings: {ratings}\")\n",
    "\n",
    "\n",
    "# Identificazione delle classi con solo rating molto bassi (1-2)\n",
    "print(\"\\n=== Analisi delle Classi con SOLO Rating Molto Bassi (1-2) ===\")\n",
    "\n",
    "# Funzione per verificare se una specie ha esclusivamente rating tra 1-2\n",
    "def has_only_low_ratings(species_ratings):\n",
    "    valid_ratings = [r for r in species_ratings if pd.notna(r) and r != 0]  # Escludi NaN e rating=0\n",
    "    if not valid_ratings:  # Se non ci sono rating validi\n",
    "        return False\n",
    "    return all(1 <= r <= 2 for r in valid_ratings)  # Modificato: ora solo 1-2\n",
    "\n",
    "# Raggruppa per specie e analizza\n",
    "low_rating_species = []\n",
    "for species, group in train_df.groupby('primary_label'):\n",
    "    ratings = group['rating'].tolist()\n",
    "    if has_only_low_ratings(ratings):\n",
    "        low_rating_species.append({\n",
    "            'species': species,\n",
    "            'count': len(ratings),\n",
    "            'avg_rating': np.nanmean([r for r in ratings if pd.notna(r) and r != 0]),\n",
    "            'ratings': sorted([r for r in ratings if pd.notna(r) and r != 0])\n",
    "        })\n",
    "\n",
    "# Crea DataFrame e visualizza risultati\n",
    "if low_rating_species:\n",
    "    low_rating_df = pd.DataFrame(low_rating_species).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Trovate {len(low_rating_species)} classi con SOLO rating molto bassi (1-2):\")\n",
    "    \n",
    "    # Visualizzazione\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(low_rating_df['species'], low_rating_df['avg_rating'], alpha=0.7, color='tomato')\n",
    "    plt.axhline(y=1.5, color='r', linestyle='--', label='Media teorica = 1.5')\n",
    "    plt.title('Classi con Esclusivamente Rating Molto Bassi (1-2)')\n",
    "    plt.xlabel('Specie')\n",
    "    plt.ylabel('Rating Medio')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostra dettagli delle prime 10 classi\n",
    "    print(\"\\nDettagli delle prime 10 classi con solo rating molto bassi:\")\n",
    "    for i, row in low_rating_df.head(10).iterrows():\n",
    "        print(f\"- {row['species']}: {row['count']} clip, rating medio: {row['avg_rating']:.2f}, ratings: {row['ratings']}\")\n",
    "    \n",
    "    # Cerca sovrapposizione con classi rare\n",
    "    overlap = [s for s in low_rating_df['species'] if s in rare_species]\n",
    "    print(f\"\\nSovrapposizione con classi rare (<{RARE_CLASS_THRESHOLD} esempi): {len(overlap)} classi\")\n",
    "    if overlap:\n",
    "        print(f\"Le classi rare che hanno solo rating molto bassi: {overlap[:10]}{'...' if len(overlap) > 10 else ''}\")\n",
    "else:\n",
    "    print(\"Nessuna classe ha esclusivamente rating da 1 a 2.\")\n",
    "\n",
    "print(\"\\n=== Conclusioni dall'Analisi delle Classi Rare ===\")\n",
    "print(f\"1. Abbiamo {len(rare_species)} classi rare (<{RARE_CLASS_THRESHOLD} esempi)\")\n",
    "print(f\"2. Di queste, {len(very_rare_species)} hanno 5 o meno esempi\")\n",
    "print(\"3. La qualità delle registrazioni (rating) è un fattore critico per le classi rare\")\n",
    "print(\"4. Le classi estremamente rare richiedono tecniche speciali (data augmentation, few-shot learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Implementazione delle Tecniche dei Vincitori\n",
    "\n",
    "In questa sezione implementiamo le tre tecniche di data augmentation che hanno contribuito significativamente alle performance dei vincitori:\n",
    "1. **Random Segment Selection** - Estrae segmenti casuali dalle registrazioni audio\n",
    "2. **XY Masking** - Applica maschere casuali sugli assi tempo e frequenza degli spettrogrammi Mel\n",
    "3. **Horizontal CutMix** - Combina parti di spettrogrammi da diverse registrazioni\n",
    "\n",
    "La classe `AudioAugmentations` gestisce tutte queste trasformazioni in modo unificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentations:\n",
    "    def __init__(self, p_random_segment=0.5, p_xy_mask=0.5, p_horizontal_cutmix=0.25):\n",
    "        \"\"\"\n",
    "        Inizializza le trasformazioni per data augmentation audio.\n",
    "        \n",
    "        Args:\n",
    "            p_random_segment: Probabilità di utilizzare un segmento casuale\n",
    "            p_xy_mask: Probabilità di applicare il mascheramento XY\n",
    "            p_horizontal_cutmix: Probabilità di applicare horizontal cutmix\n",
    "        \"\"\"\n",
    "        self.p_random_segment = p_random_segment\n",
    "        self.p_xy_mask = p_xy_mask\n",
    "        self.p_horizontal_cutmix = p_horizontal_cutmix\n",
    "    \n",
    "    def apply_xy_masking(self, spec):\n",
    "        \"\"\"Applica maschere casuali sull'asse X (tempo) e Y (frequenza) allo spettrogramma\"\"\"\n",
    "        mask = spec.clone()\n",
    "        \n",
    "        # Determina la dimensionalità del tensore\n",
    "        if len(mask.shape) == 3:  # [channels, height, width]\n",
    "            channels, height, width = mask.shape\n",
    "        elif len(mask.shape) == 4:  # [batch, channels, height, width]\n",
    "            _, channels, height, width = mask.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Forma dello spettrogramma non supportata: {mask.shape}\")\n",
    "        \n",
    "        # Masking temporale (asse X)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_width = int(width * np.random.uniform(0.1, 0.2))  # 10-20% width\n",
    "            mask_start = np.random.randint(0, width - mask_width)\n",
    "            mask[..., mask_start:mask_start+mask_width] = 0\n",
    "        \n",
    "        # Masking frequenziale (asse Y)\n",
    "        if np.random.random() < self.p_xy_mask:\n",
    "            mask_height = int(height * np.random.uniform(0.1, 0.2))  # 10-20% height\n",
    "            mask_start = np.random.randint(0, height - mask_height)\n",
    "            \n",
    "            # Adatta l'indicizzazione in base alla dimensionalità\n",
    "            if len(mask.shape) == 3:  # [channels, height, width]\n",
    "                mask[:, mask_start:mask_start+mask_height, :] = 0\n",
    "            else:  # [batch, channels, height, width]\n",
    "                mask[:, :, mask_start:mask_start+mask_height, :] = 0\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal CutMix: Implementazione della Funzione di Collate\n",
    "\n",
    "Questa funzione personalizzata viene utilizzata nel DataLoader per implementare l'Horizontal CutMix,\n",
    "che combina sezioni temporali di spettrogrammi diversi all'interno dello stesso batch.\n",
    "Le etichette vengono miscelate proporzionalmente alla quantità di dati combinati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passt_horizontal_cutmix_collate(batch, p_cutmix=0.25):\n",
    "    \"\"\"\n",
    "    Funzione di collate che applica horizontal cutmix tra elementi del batch,\n",
    "    ottimizzata per l'output del feature extractor di PASST.\n",
    "    \n",
    "    Args:\n",
    "        batch: Lista di tuple (inputs, target) dove inputs è un dizionario\n",
    "        p_cutmix: Probabilità di applicare cutmix ad ogni coppia di esempi\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (inputs_batch, targets_batch)\n",
    "    \"\"\"\n",
    "    # Usa prima la funzione collate standard\n",
    "    inputs, targets = passt_collate_fn(batch)\n",
    "    batch_size = len(targets)\n",
    "    \n",
    "    # Se gli input sono un dizionario (output del feature extractor)\n",
    "    if isinstance(inputs, dict) and 'input_values' in inputs:\n",
    "        if batch_size > 1:  # Serve almeno 2 elementi per fare cutmix\n",
    "            for i in range(batch_size):\n",
    "                if np.random.rand() < p_cutmix:\n",
    "                    # Seleziona un altro esempio casuale da mixare\n",
    "                    j = np.random.randint(0, batch_size)\n",
    "                    if i != j:\n",
    "                        # Prendi i valori di input\n",
    "                        input_vals = inputs['input_values']\n",
    "                        \n",
    "                        # Determina il punto di taglio orizzontale (lungo la dimensione temporale)\n",
    "                        # Nota: per PASST, 'input_values' è la forma d'onda (1D)\n",
    "                        # quindi il cutmix sarà sulla lunghezza della sequenza\n",
    "                        length = input_vals.shape[1]  # B x L\n",
    "                        cut_point = np.random.randint(int(length * 0.25), int(length * 0.75))\n",
    "                        \n",
    "                        # Esegue il mixing\n",
    "                        mix_ratio = cut_point / length\n",
    "                        inputs['input_values'][i, :cut_point] = inputs['input_values'][j, :cut_point]\n",
    "                        \n",
    "                        # Mix delle etichette in proporzione al mix degli input\n",
    "                        targets[i] = targets[i] * (1 - mix_ratio) + targets[j] * mix_ratio\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento e pre-processing\n",
    "\n",
    "## Preprocessing Audio con Supporto per Data Augmentation\n",
    "\n",
    "La funzione di caricamento audio è stata modificata per supportare due modalità di estrazione:\n",
    "- **Posizionale** - Estrazione da punti specifici nella registrazione ('start', 'center', 'end')\n",
    "- **Casuale** - Estrazione di un segmento casuale quando `random_segment=True`\n",
    "\n",
    "Questa implementazione permette di applicare la tecnica di Random Segment Selection come parte della data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:32.790961Z",
     "iopub.status.busy": "2025-05-14T17:40:32.790445Z",
     "iopub.status.idle": "2025-05-14T17:40:32.803627Z",
     "shell.execute_reply": "2025-05-14T17:40:32.802015Z",
     "shell.execute_reply.started": "2025-05-14T17:40:32.790937Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_audio_waveform(file_path, target_sr=32000, duration=5, segment_position='center', random_segment=False):\n",
    "    \"\"\"\n",
    "    Carica un file audio e restituisce la forma d'onda grezza per il PASST feature extractor.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Percorso del file audio\n",
    "        target_sr: Sample rate target\n",
    "        duration: Durata target in secondi\n",
    "        segment_position: Posizione del segmento ('start', 'center', 'end')\n",
    "        random_segment: Se True, seleziona un segmento casuale\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Forma d'onda audio normalizzata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carica il file audio\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        # Lunghezza target in campioni\n",
    "        target_len = int(target_sr * duration)\n",
    "        total_len = len(y)\n",
    "        \n",
    "        # Gestisci clip troppo corte\n",
    "        if total_len < target_len:\n",
    "            import math\n",
    "            n_copy = math.ceil(target_len / total_len)\n",
    "            if n_copy > 1:\n",
    "                y = np.tile(y, n_copy)\n",
    "            total_len = len(y)\n",
    "        \n",
    "        # Seleziona il segmento\n",
    "        if random_segment and total_len > target_len:\n",
    "            # Estrai segmento casuale\n",
    "            max_start_idx = total_len - target_len\n",
    "            start_idx = np.random.randint(0, max_start_idx)\n",
    "        else:\n",
    "            # Usa le posizioni predefinite\n",
    "            if segment_position == 'start':\n",
    "                start_idx = int(total_len * 0.2)\n",
    "                if start_idx + target_len > total_len:\n",
    "                    start_idx = max(0, total_len - target_len)\n",
    "            elif segment_position == 'end':\n",
    "                end_point = int(total_len * 0.8)\n",
    "                start_idx = max(0, end_point - target_len)\n",
    "            else:  # 'center' (default)\n",
    "                start_idx = max(0, int(total_len / 2 - target_len / 2))\n",
    "        \n",
    "        # Estrai il segmento\n",
    "        y = y[start_idx:start_idx + target_len]\n",
    "        \n",
    "        # Padda se necessario\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)), mode='constant')\n",
    "            \n",
    "        # Normalizza l'audio tra -1 e 1 (importante per il feature extractor di PASST)\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "        return np.zeros(target_sr * duration, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch per Dati Audio\n",
    "\n",
    "## Dataset PyTorch con Supporto Integrato per Data Augmentation\n",
    "\n",
    "Implementiamo due classi di dataset:\n",
    "- `BirdDataset` - Dataset standard che estrae il segmento centrale delle clip audio\n",
    "- `AdaptiveMultiSegmentBirdDataset` - Dataset avanzato che:\n",
    "  - Estrae automaticamente diversi segmenti in base alla durata delle registrazioni\n",
    "  - Applica le tecniche di data augmentation durante il caricamento\n",
    "  - Supporta l'estrazione di segmenti adattivi (1-3 per clip in base alla lunghezza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASSBirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, feature_extractor=None):\n",
    "        \"\"\"\n",
    "        Dataset che estrae solo il segmento centrale per ogni clip audio e usa il feature extractor di PASST.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Ottieni il sample rate richiesto dal feature extractor\n",
    "        self.target_sr = 16000  # Sample rate richiesto dal modello PASST\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Attenzione: File non trovato in {file_path}.\")\n",
    "            dummy_audio = np.zeros(self.target_sr * config.DURATION, dtype=np.float32)\n",
    "            if self.feature_extractor:\n",
    "                inputs = self.feature_extractor(\n",
    "                    dummy_audio, \n",
    "                    sampling_rate=self.target_sr, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "            else:\n",
    "                inputs = torch.tensor(dummy_audio, dtype=torch.float32)\n",
    "            \n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return inputs, dummy_label\n",
    "            \n",
    "        # Carica l'audio come waveform con il sample rate di config\n",
    "        audio = load_audio_waveform(file_path, target_sr=config.SR, duration=config.DURATION)\n",
    "        \n",
    "        # Ricampiona l'audio al sample rate richiesto dal feature extractor\n",
    "        if self.feature_extractor and config.SR != self.target_sr:\n",
    "            audio_resampled = librosa.resample(\n",
    "                y=audio, \n",
    "                orig_sr=config.SR, \n",
    "                target_sr=self.target_sr\n",
    "            )\n",
    "        else:\n",
    "            audio_resampled = audio\n",
    "        \n",
    "        # Se abbiamo un feature extractor, usalo per processare l'audio\n",
    "        if self.feature_extractor:\n",
    "            inputs = self.feature_extractor(\n",
    "                audio_resampled, \n",
    "                sampling_rate=self.target_sr,  # Usa il sample rate richiesto dal modello\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # Rimuovi la dimensione batch aggiunta dal feature extractor\n",
    "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs = torch.tensor(audio, dtype=torch.float32)\n",
    "            \n",
    "        # Ottieni le etichette\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            \n",
    "        return inputs, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BirdDataset con l'utilizzo di adaptive clip in base alla lunghezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveMultiSegmentPASSBirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, feature_extractor=None, augmentations=None):\n",
    "        \"\"\"\n",
    "        Dataset che estrae un numero appropriato di segmenti in base alla lunghezza di ogni clip,\n",
    "        ottimizzato per l'uso con il modello PASST.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.augmentations = augmentations\n",
    "        # Ottieni il sample rate richiesto dal feature extractor\n",
    "        self.target_sr = 16000  # Sample rate richiesto dal modello PASST\n",
    "        \n",
    "        # Costruisci la mappa di segmenti per file\n",
    "        self.segments_to_use = self._build_segment_map()\n",
    "    \n",
    "    def _build_segment_map(self):\n",
    "        \"\"\"\n",
    "        Costruisce una mappa di segmenti da estrarre per ciascun file.\n",
    "        Per ogni file audio, determiniamo quanti e quali segmenti utilizzare.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Per ogni esempio nel dataset\n",
    "        for idx in range(len(self.df)):\n",
    "            # Per ora, aggiungi solo un segmento centrale per ogni file\n",
    "            # Nella versione completa, dovresti determinare il numero di segmenti\n",
    "            # in base alla durata del file audio\n",
    "            segments.append((idx, 'center'))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segments_to_use)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df_idx, segment_position = self.segments_to_use[idx]\n",
    "        \n",
    "        row = self.df.iloc[df_idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        # Determina se applicare random segment in base all'oggetto augmentations\n",
    "        use_random_segment = False  # Valore predefinito\n",
    "        if self.augmentations is not None:\n",
    "            use_random_segment = np.random.random() < self.augmentations.p_random_segment\n",
    "        \n",
    "        # Carica l'audio come waveform\n",
    "        audio = load_audio_waveform(\n",
    "            file_path, \n",
    "            target_sr=config.SR, \n",
    "            duration=config.DURATION,\n",
    "            segment_position=segment_position,\n",
    "            random_segment=use_random_segment\n",
    "        )\n",
    "        \n",
    "        # Ricampiona l'audio al sample rate richiesto dal feature extractor\n",
    "        if self.feature_extractor and config.SR != self.target_sr:\n",
    "            audio_resampled = librosa.resample(\n",
    "                y=audio, \n",
    "                orig_sr=config.SR, \n",
    "                target_sr=self.target_sr\n",
    "            )\n",
    "        else:\n",
    "            audio_resampled = audio\n",
    "        \n",
    "        # Se abbiamo un feature extractor, usalo per processare l'audio\n",
    "        if self.feature_extractor:\n",
    "            inputs = self.feature_extractor(\n",
    "                audio_resampled, \n",
    "                sampling_rate=self.target_sr,  # Usa il sample rate richiesto dal modello\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # Rimuovi la dimensione batch aggiunta dal feature extractor\n",
    "            inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        else:\n",
    "            inputs = torch.tensor(audio, dtype=torch.float32)\n",
    "            \n",
    "        # Ottieni le etichette\n",
    "        label_tensor = torch.tensor(self.labels[df_idx], dtype=torch.float32)\n",
    "            \n",
    "        return inputs, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passt_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Funzione di collate per gestire l'output del feature extractor del PASST.\n",
    "    \n",
    "    Args:\n",
    "        batch: Lista di tuple (inputs, target) dove inputs è un dizionario\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (inputs_batch, targets_batch)\n",
    "    \"\"\"\n",
    "    # Separa input e target\n",
    "    inputs = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    # Crea un batch combinando gli input del feature extractor\n",
    "    if isinstance(inputs[0], dict):\n",
    "        # Estrai le chiavi dai dizionari di input\n",
    "        keys = inputs[0].keys()\n",
    "        \n",
    "        # Crea un nuovo dizionario combinando i tensor per ogni chiave\n",
    "        batch_inputs = {}\n",
    "        for key in keys:\n",
    "            batch_inputs[key] = torch.stack([item[key] for item in inputs])\n",
    "    else:\n",
    "        # Se gli input non sono dizionari, semplicemente stacka i tensors\n",
    "        batch_inputs = torch.stack(inputs)\n",
    "        \n",
    "    # Stacka i target\n",
    "    batch_targets = torch.stack(targets)\n",
    "    \n",
    "    return batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_passt_model(num_labels=config.N_CLASSES, model_name=\"MIT/ast-finetuned-audioset-10-10-0.4593\"):\n",
    "    \"\"\"\n",
    "    Configura il modello PASST per la classificazione multi-etichetta audio.\n",
    "    \n",
    "    Args:\n",
    "        num_labels: Numero di classi/etichette\n",
    "        model_name: Nome o percorso del modello pre-addestrato\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, feature_extractor)\n",
    "    \"\"\"\n",
    "    print(f\"Configurazione modello PASST: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Carica il feature extractor\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        \n",
    "        # Crea una configurazione per il multi-label\n",
    "        config_model = AutoConfig.from_pretrained(model_name)\n",
    "        config_model.num_labels = num_labels\n",
    "        config_model.problem_type = \"multi_label_classification\"\n",
    "        \n",
    "        # Carica il modello direttamente con la configurazione corretta\n",
    "        # e ignora le dimensioni incompatibili\n",
    "        model = AutoModelForAudioClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config_model,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Modello PASST caricato con successo con {num_labels} classi\")\n",
    "        return model, feature_extractor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel caricamento del modello PASST: {e}\")\n",
    "        raise RuntimeError(f\"Impossibile caricare il modello PASST: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione dataset e dataloader con AdaptiveMultiSegment\n",
    "\n",
    "### Creazione dei DataLoader con Augmentation\n",
    "\n",
    "In questa sezione:\n",
    "1. Applichiamo il bilanciamento strategico al dataset di training\n",
    "2. Creiamo l'istanza di AudioAugmentations con le probabilità ottimali\n",
    "3. Configuriamo i dataloader con le funzioni di collate personalizzate\n",
    "4. Analizziamo la distribuzione dei segmenti nel dataset risultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sostituisci questa sezione per utilizzare i dataset PASST\n",
    "\n",
    "# Crea il modello e il feature extractor\n",
    "passt_model, passt_feature_extractor = setup_passt_model(num_labels=config.N_CLASSES)\n",
    "\n",
    "# Applica il bilanciamento strategico solo al dataset di training\n",
    "print(\"\\n=== Bilanciamento Strategico del Dataset di Training ===\")\n",
    "X_train_df_balanced, y_train_one_hot_balanced = create_balanced_dataset_df(\n",
    "    X_train_df, \n",
    "    y_train_one_hot,\n",
    "    abundant_class_threshold=150,\n",
    "    remove_percentage=0.4\n",
    ")\n",
    "\n",
    "# Crea un'istanza delle augmentations audio (supporterà solo random segment)\n",
    "audio_augmentations = AudioAugmentations(p_random_segment=0.5, p_xy_mask=0, p_horizontal_cutmix=0)\n",
    "\n",
    "# Creiamo i dataset utilizzando i dataset PASST\n",
    "print(\"Creazione dataset di training con approccio multi-segmento adattivo per PASST...\")\n",
    "train_dataset = AdaptiveMultiSegmentPASSBirdDataset(\n",
    "    X_train_df_balanced, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_train_one_hot_balanced,\n",
    "    feature_extractor=passt_feature_extractor,\n",
    "    augmentations=audio_augmentations\n",
    ")\n",
    "\n",
    "# Per validation, non usiamo augmentation\n",
    "print(\"Creazione dataset di validation per PASST...\")\n",
    "val_dataset = PASSBirdDataset(\n",
    "    X_val_df, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_val_one_hot,\n",
    "    feature_extractor=passt_feature_extractor\n",
    ")\n",
    "\n",
    "# Creiamo i dataloader con collate function personalizzata\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    collate_fn=passt_collate_fn  # Non usiamo cutmix per ora, è più complesso con PASST\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    collate_fn=passt_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo l'ottimizzatore con learning rate differenziati\n",
    "def get_optimizer(model, lr_base=2e-5):\n",
    "    # Per i modelli transformer, di solito un single learning rate è sufficiente\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr_base,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "# Loss function - BCE per multi-label\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Ottimizzatore \n",
    "optimizer = get_optimizer(passt_model)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.EPOCHS,\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Addestramento e Validazione del Modello\n",
    "\n",
    "## Funzione di Training con Supporto per Checkpoint\n",
    "\n",
    "La funzione di training implementa:\n",
    "- Caricamento automatico dei checkpoint precedenti\n",
    "- Early stopping basato sulle performance di validation\n",
    "- Salvataggio periodico dei checkpoint e del miglior modello\n",
    "- Supporto per scheduler di learning rate (CosineAnnealingLR)\n",
    "- Visualizzazione delle curve di loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:36.480613Z",
     "iopub.status.busy": "2025-05-14T17:40:36.479785Z",
     "iopub.status.idle": "2025-05-14T17:40:36.553261Z",
     "shell.execute_reply": "2025-05-14T17:40:36.552366Z",
     "shell.execute_reply.started": "2025-05-14T17:40:36.480579Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_passt_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                     model_save_path=None, model_load_path=None, patience=3,\n",
    "                     resume_training=True, scheduler=None):\n",
    "    \"\"\"\n",
    "    Addestra il modello PASST e valuta su validation set con supporto per checkpoint.\n",
    "    \"\"\"\n",
    "    # Inizializza liste per tracciare l'andamento\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    total_training_time = 0\n",
    "    start_epoch = 0\n",
    "    checkpoint_info = {}\n",
    "    needs_training = True\n",
    "    \n",
    "    # Gestione checkpoint precedente\n",
    "    if resume_training and model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Caricamento checkpoint: {model_load_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            \n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            train_losses = checkpoint.get('train_losses', [])\n",
    "            val_losses = checkpoint.get('val_losses', [])\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            epochs_without_improvement = checkpoint.get('epochs_without_improvement', 0)\n",
    "            total_training_time = checkpoint.get('total_training_time', 0)\n",
    "            \n",
    "            print(f\"Riprendo training dall'epoca {start_epoch}/{epochs}\")\n",
    "            print(f\"Best validation loss finora: {best_val_loss:.4f}\")\n",
    "            \n",
    "            # Controlla se il training è già terminato\n",
    "            if start_epoch >= epochs:\n",
    "                print(\"Il training è già completato. Caricamento modello finale.\")\n",
    "                needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del checkpoint: {e}\")\n",
    "            print(\"Inizializzando il training da zero.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Loop di training sulle epoche (inizia da start_epoch)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                # Per PASST, inputs può essere un dizionario\n",
    "                if isinstance(inputs, dict):\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                    \n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Output del modello PASST\n",
    "                outputs = model(**inputs) if isinstance(inputs, dict) else model(inputs)\n",
    "                \n",
    "                # Per i modelli HuggingFace, l'output è spesso un oggetto con vari attributi\n",
    "                if hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                \n",
    "                # Calcolo della loss\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    # Per PASST, inputs può essere un dizionario\n",
    "                    if isinstance(val_inputs, dict):\n",
    "                        val_inputs = {k: v.to(device) for k, v in val_inputs.items()}\n",
    "                    else:\n",
    "                        val_inputs = val_inputs.to(device)\n",
    "                        \n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    # Output del modello PASST\n",
    "                    val_outputs = model(**val_inputs) if isinstance(val_inputs, dict) else model(val_inputs)\n",
    "                    \n",
    "                    # Per i modelli HuggingFace, l'output è spesso un oggetto con vari attributi\n",
    "                    if hasattr(val_outputs, 'logits'):\n",
    "                        val_logits = val_outputs.logits\n",
    "                    else:\n",
    "                        val_logits = val_outputs\n",
    "                    \n",
    "                    # Calcolo della loss\n",
    "                    val_loss = criterion(val_logits, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            # Aggiorna lo scheduler se è presente\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "            # Calcola tempo trascorso\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            total_training_time += epoch_time\n",
    "            \n",
    "            # Salva il checkpoint\n",
    "            checkpoint_info = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_without_improvement': epochs_without_improvement,\n",
    "                'total_training_time': total_training_time\n",
    "            }\n",
    "            \n",
    "            if scheduler:\n",
    "                checkpoint_info['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "            # Salva solo se abbiamo un percorso di salvataggio\n",
    "            if model_save_path:\n",
    "                # Salva sempre l'ultimo checkpoint per ripartire\n",
    "                checkpoint_path = os.path.join(os.path.dirname(model_save_path), \"latest_checkpoint.pth\")\n",
    "                torch.save(checkpoint_info, checkpoint_path)\n",
    "                \n",
    "                # Salva il modello se abbiamo un miglioramento nella val loss\n",
    "                if epoch_val_loss < best_val_loss:\n",
    "                    print(f\"Validation loss migliorata da {best_val_loss:.4f} a {epoch_val_loss:.4f}. Salvataggio modello...\")\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    epochs_without_improvement = 0\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "                    \n",
    "            # Controllo early stopping\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping dopo {patience} epoche senza miglioramenti.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"Epoca {epoch+1}/{epochs} completata in {epoch_time:.1f}s - \"\n",
    "                  f\"Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "        # Training completato\n",
    "        total_training_time += (time.time() - start_time_total)\n",
    "        print(f\"Training completato in {total_training_time:.1f}s\")\n",
    "        \n",
    "        # Carica i pesi migliori per l'inferenza\n",
    "        if model_save_path and os.path.exists(model_save_path):\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "            print(f\"Caricati i pesi del miglior modello da {model_save_path}\")\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurazione e Avvio del Training\n",
    "\n",
    "Configuriamo e avviamo il training del modello:\n",
    "- Individuazione automatica dei checkpoint precedenti\n",
    "- Inizializzazione dell'ottimizzatore e dello scheduler\n",
    "- Avvio del training con i parametri ottimizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    # Directory per i checkpoint in Kaggle\n",
    "    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Verifica se esiste un checkpoint precedente\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        model_load_path = latest_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente in {latest_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = None\n",
    "        \n",
    "    # Imposta il percorso di salvataggio\n",
    "    model_save_path = \"/kaggle/working/birdclef_passt_model.pth\"\n",
    "    \n",
    "elif config.environment == 'colab':\n",
    "    # Per Colab, verifica se esiste un checkpoint su Drive\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    if os.path.exists(drive_checkpoint):\n",
    "        model_load_path = drive_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente su Drive: {drive_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = None\n",
    "    \n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, f\"birdclef_passt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "    \n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    if os.path.exists(local_checkpoint):\n",
    "        model_load_path = local_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente: {local_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = None\n",
    "    \n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_passt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Addestra il modello PASST\n",
    "train_losses, val_losses, training_time = train_passt_model(\n",
    "    model=passt_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path,\n",
    "    resume_training=True\n",
    ")\n",
    "\n",
    "def plot_training_history(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Visualizza le curve di loss dell'addestramento.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Curve di Loss durante l\\'addestramento')\n",
    "    plt.xlabel('Epoche')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dirs['visualizations'], 'loss_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Dopo l'addestramento\n",
    "if train_losses and val_losses:\n",
    "    plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generazione della Submission\n",
    "\n",
    "## Generazione della Submission\n",
    "\n",
    "Implementiamo la funzione per generare le predizioni sui file di test:\n",
    "- Caricamento e segmentazione delle soundscape di test\n",
    "- Estrazione di spettrogrammi Mel da ciascun segmento\n",
    "- Generazione delle predizioni con il modello addestrato\n",
    "- Creazione del file di submission nel formato richiesto dalla competizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T17:40:36.574352Z",
     "iopub.status.busy": "2025-05-14T17:40:36.573965Z",
     "iopub.status.idle": "2025-05-14T17:40:36.639233Z",
     "shell.execute_reply": "2025-05-14T17:40:36.638203Z",
     "shell.execute_reply.started": "2025-05-14T17:40:36.574325Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_passt_submission(model, feature_extractor, device=config.DEVICE):\n",
    "    \"\"\"\n",
    "    Genera un file di submission usando il modello PASST.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PASST addestrato\n",
    "        feature_extractor: Feature extractor PASST\n",
    "        device: Device per inferenza ('cuda' o 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame di submission\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Seed per riproducibilità\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Percorso dei test soundscapes\n",
    "    test_soundscape_path = config.TEST_SOUNDSCAPES_DIR\n",
    "    test_soundscapes = [os.path.join(test_soundscape_path, afile) \n",
    "                        for afile in sorted(os.listdir(test_soundscape_path)) \n",
    "                        if afile.endswith('.ogg')]\n",
    "    \n",
    "    print(f\"Elaborazione di {len(test_soundscapes)} file soundscape...\")\n",
    "    \n",
    "    # Crea DataFrame per le predizioni\n",
    "    predictions = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "    \n",
    "    for soundscape in tqdm(test_soundscapes, desc=\"Elaborazione soundscapes\"):\n",
    "        # Carica audio\n",
    "        sig, rate = librosa.load(path=soundscape, sr=config.SR)\n",
    "        \n",
    "        # Split in segmenti da 5 secondi\n",
    "        segment_length = rate * config.TEST_CLIP_DURATION\n",
    "        chunks = []\n",
    "        for i in range(0, len(sig), segment_length):\n",
    "            chunk = sig[i:i+segment_length]\n",
    "            # Padda se necessario\n",
    "            if len(chunk) < segment_length:\n",
    "                chunk = np.pad(chunk, (0, segment_length - len(chunk)), mode='constant')\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Genera predizioni per ogni segmento\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Calcola row_id (nome file + tempo finale del segmento in secondi)\n",
    "            file_name = os.path.basename(soundscape).split('.')[0]\n",
    "            row_id = f\"{file_name}_{i * config.TEST_CLIP_DURATION + config.TEST_CLIP_DURATION}\"\n",
    "            \n",
    "            # Normalizza l'audio\n",
    "            if np.max(np.abs(chunk)) > 0:\n",
    "                chunk = chunk / np.max(np.abs(chunk))\n",
    "                \n",
    "            # Usa il feature extractor per preparare l'input\n",
    "            inputs = feature_extractor(chunk, sampling_rate=config.SR, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Effettua predizione\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                if hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                scores = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "            \n",
    "            # Aggiungi riga al DataFrame di predizioni\n",
    "            new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + all_species)\n",
    "            predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Salva la submission come CSV\n",
    "    predictions.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Genera submission\n",
    "if config.environment == 'kaggle':\n",
    "    print(\"\\nGenerazione del file di submission con PASST...\")\n",
    "    submission_df = generate_passt_submission(passt_model, passt_feature_extractor)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\nAnteprima del file di submission:\")\n",
    "        print(submission_df.head())\n",
    "else:\n",
    "    print(\"\\nSalto la generazione della submission perché non siamo su Kaggle.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
