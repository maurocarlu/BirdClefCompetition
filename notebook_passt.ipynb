{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progetto Machine Learning: Riconoscimento di Specie di Uccelli con PASST\n",
    "\n",
    "Questo notebook implementa un sistema di riconoscimento di specie di uccelli attraverso l'analisi di registrazioni audio della competizione BirdClef 2025. Il progetto utilizza l'architettura PASST (Patchout Audio Spectrogram Transformer) per classificare gli audio convertiti in spettrogrammi Mel e include anche un sistema di configurazione automatica dell'ambiente per eseguire il codice su Kaggle, Google Colab o in locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie di sistema e utilità\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import IPython.display as ipd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Librerie per data science e manipolazione dati\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerie per elaborazione audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import timm\n",
    "\n",
    "# Per il modello PASST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Visualizzazione\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ignoriamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurazione del logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('BirdClef-PASST')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n",
    "print(f\"PyTorch versione: {torch.__version__}\")\n",
    "print(f\"timm versione: {timm.__version__}\")\n",
    "print(f\"Python versione: {platform.python_version()}\")\n",
    "print(f\"Sistema operativo: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulizia directory di lavoro (utile per Kaggle)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Imposta questo a True per abilitare la cancellazione\n",
    "clear_working_dir = False  # Disabilitato di default per sicurezza\n",
    "\n",
    "working_dir = '/kaggle/working/'\n",
    "\n",
    "if clear_working_dir and os.path.exists(working_dir):\n",
    "    for filename in os.listdir(working_dir):\n",
    "        file_path = os.path.join(working_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # elimina file o link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # elimina directory\n",
    "        except Exception as e:\n",
    "            print(f'Errore durante la rimozione di {file_path}: {e}')\n",
    "    print(f\"Tutti i file in {working_dir} sono stati rimossi.\")\n",
    "else:\n",
    "    print(\"Pulizia disabilitata (clear_working_dir = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabile per impostare manualmente l'ambiente\n",
    "# Modifica questa variabile in base all'ambiente in uso:\n",
    "# - 'kaggle' per l'ambiente Kaggle\n",
    "# - 'colab' per Google Colab\n",
    "# - 'local' per l'esecuzione in locale\n",
    "MANUAL_ENVIRONMENT = ''  # Impostare su 'kaggle', 'colab', o 'local' per forzare l'ambiente\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Rileva se il notebook è in esecuzione su Kaggle, Google Colab o in locale.\n",
    "    Rispetta l'impostazione manuale se fornita.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'kaggle', 'colab', o 'local'\n",
    "    \"\"\"\n",
    "    # Se l'ambiente è stato impostato manualmente, usa quello\n",
    "    if MANUAL_ENVIRONMENT in ['kaggle', 'colab', 'local']:\n",
    "        print(f\"Utilizzo ambiente impostato manualmente: {MANUAL_ENVIRONMENT}\")\n",
    "        return MANUAL_ENVIRONMENT\n",
    "    \n",
    "    # Verifica Kaggle con metodo più affidabile\n",
    "    # Verifica l'esistenza di directory specifiche di Kaggle\n",
    "    if os.path.exists('/kaggle/working') and os.path.exists('/kaggle/input'):\n",
    "        print(\"Rilevato ambiente Kaggle\")\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Verifica se è Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Se non è né Kaggle né Colab, allora è locale\n",
    "    return 'local'\n",
    "\n",
    "# Rileva l'ambiente attuale\n",
    "ENVIRONMENT = detect_environment()\n",
    "print(f\"Ambiente rilevato: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Rileva l'ambiente\n",
    "        self.environment = ENVIRONMENT  # Usa la variabile globale impostata in precedenza\n",
    "        \n",
    "        # Imposta i percorsi di base in base all'ambiente\n",
    "        if self.environment == 'kaggle':\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.BASE_DIR = f\"/kaggle/input/{self.COMPETITION_NAME}\"\n",
    "            self.OUTPUT_DIR = \"/kaggle/working\"\n",
    "            self.MODELS_DIR = \"/kaggle/input\"  # Per i modelli pre-addestrati\n",
    "            \n",
    "            # Imposta subito i percorsi derivati per l'ambiente Kaggle\n",
    "            self._setup_derived_paths()\n",
    "            \n",
    "        elif self.environment == 'colab':\n",
    "            # In Colab, inizializza directory base temporanee\n",
    "            self.COMPETITION_NAME = \"birdclef-2025\"\n",
    "            self.OUTPUT_DIR = \"/content/output\"\n",
    "            self.MODELS_DIR = \"/content/models\"\n",
    "            \n",
    "            # Crea le directory di output\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # In Colab, BASE_DIR verrà impostato dopo il download\n",
    "            self.BASE_DIR = \"/content/placeholder\"  # Verrà sovrascritto dopo il download\n",
    "            \n",
    "            # Inizializza i percorsi dei file a None per ora\n",
    "            self.TRAIN_AUDIO_DIR = None\n",
    "            self.TEST_SOUNDSCAPES_DIR = None\n",
    "            self.TRAIN_CSV_PATH = None\n",
    "            self.TAXONOMY_CSV_PATH = None\n",
    "            self.SAMPLE_SUB_PATH = None\n",
    "            \n",
    "        else:  # locale\n",
    "            # In ambiente locale, i percorsi dipenderanno dalla tua configurazione\n",
    "            self.BASE_DIR = os.path.abspath(\".\")\n",
    "            self.OUTPUT_DIR = os.path.join(self.BASE_DIR, \"output\")\n",
    "            self.MODELS_DIR = os.path.join(self.BASE_DIR, \"models\")\n",
    "            \n",
    "            # Crea le directory se non esistono\n",
    "            os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "            os.makedirs(self.MODELS_DIR, exist_ok=True)\n",
    "            \n",
    "            # Imposta i percorsi derivati\n",
    "            self._setup_derived_paths()\n",
    "        \n",
    "        # Parametri per il preprocessing audio - adattati per PASST\n",
    "        self.SR = 32000      # Sample rate\n",
    "        self.DURATION = 5    # Durata dei clip in secondi\n",
    "        self.N_MELS = 128    # Numero di bande Mel\n",
    "        self.N_FFT = 1024    # Dimensione finestra FFT\n",
    "        self.HOP_LENGTH = 500  # Hop length per STFT\n",
    "        self.FMIN = 40       # Frequenza minima per lo spettrogramma Mel\n",
    "        self.FMAX = 15000    # Frequenza massima\n",
    "        self.POWER = 2       # Esponente per calcolo spettrogramma\n",
    "            \n",
    "        # Parametri per il training\n",
    "        self.BATCH_SIZE = 64  # Dimensione del batch per PASST\n",
    "        self.EPOCHS = 10     # Numero di epoche per il training\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.NUM_WORKERS = 4  # Lavoratori per il data loading\n",
    "        self.LEARNING_RATE = 1e-4  # Learning rate più basso per transformer\n",
    "        self.WEIGHT_DECAY = 1e-5   # Weight decay per la regolarizzazione\n",
    "\n",
    "        # Parametri specifici per PASST\n",
    "        self.PATCH_SIZE = 16      # Dimensione dei patch per il transformer\n",
    "        self.HIDDEN_DIM = 768     # Dimensione dello strato nascosto\n",
    "        self.NUM_HEADS = 12       # Numero di teste di attention\n",
    "        self.NUM_LAYERS = 12      # Numero di layer transformer\n",
    "        self.MLP_RATIO = 4        # Rapporto di espansione per MLP\n",
    "        self.DROPOUT = 0.1        # Dropout rate\n",
    "\n",
    "        # Parametri per inference/submission\n",
    "        self.TEST_CLIP_DURATION = 5  # Durata dei segmenti per la predizione (secondi)\n",
    "        self.N_CLASSES = 0  # Sarà impostato dopo aver caricato i dati\n",
    "\n",
    "    def _setup_derived_paths(self):\n",
    "        \"\"\"Imposta i percorsi derivati basati su BASE_DIR\"\"\"\n",
    "        # Utilizza la normale divisione di percorso di OS (non il backslash hardcoded)\n",
    "        self.TRAIN_AUDIO_DIR = os.path.join(self.BASE_DIR, \"train_audio\")\n",
    "        self.TEST_SOUNDSCAPES_DIR = os.path.join(self.BASE_DIR, \"test_soundscapes\")\n",
    "        self.TRAIN_CSV_PATH = os.path.join(self.BASE_DIR, \"train.csv\")\n",
    "        self.TAXONOMY_CSV_PATH = os.path.join(self.BASE_DIR, \"taxonomy.csv\") \n",
    "        self.SAMPLE_SUB_PATH = os.path.join(self.BASE_DIR, \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Gestione download dati in Colab con kagglehub\n",
    "if config.environment == 'colab':\n",
    "    # Percorsi nella cache di kagglehub\n",
    "    cache_competition_path = \"/root/.cache/kagglehub/competitions/birdclef-2025\"\n",
    "    cache_model_path = \"/root/.cache/kagglehub/models/maurocarlu/passt-bird/PyTorch/default/1\"\n",
    "    \n",
    "    # Verifica se i dati sono già presenti nella cache\n",
    "    data_exists = os.path.exists(os.path.join(cache_competition_path, \"train.csv\"))\n",
    "    model_exists = os.path.exists(os.path.join(cache_model_path, \"passt_model.pth\"))\n",
    "    \n",
    "    if data_exists and model_exists:\n",
    "        print(\"I dati e il modello sono già presenti nella cache. Utilizzo copie esistenti.\")\n",
    "        birdclef_path = cache_competition_path\n",
    "        model_path = cache_model_path\n",
    "    else:\n",
    "        print(\"Scaricamento dati con kagglehub...\")\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "            \n",
    "            # Scarica solo i dati della competizione se necessario\n",
    "            if not data_exists:\n",
    "                print(\"Download dataset...\")\n",
    "                kagglehub.login()  # Mostra dialog di login interattivo\n",
    "                birdclef_path = kagglehub.competition_download('birdclef-2025')\n",
    "            else:\n",
    "                print(\"Dataset già presente nella cache.\")\n",
    "                birdclef_path = cache_competition_path\n",
    "                \n",
    "            # Scarica solo il modello se necessario\n",
    "            if not model_exists:\n",
    "                print(\"Download modello...\")\n",
    "                kagglehub.login()  # Potrebbe essere necessario riautenticarsi\n",
    "                model_path = kagglehub.model_download('maurocarlu/passt-bird/PyTorch/default/1')\n",
    "            else:\n",
    "                print(\"Modello già presente nella cache.\")\n",
    "                model_path = cache_model_path\n",
    "                \n",
    "            print(f\"Download completato.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il download dei dati: {e}\")\n",
    "            print(\"Prova ad usare Google Drive o esegui su Kaggle.\")\n",
    "            \n",
    "            # Se il download fallisce ma i dati esistono parzialmente, usa quelli\n",
    "            if os.path.exists(cache_competition_path):\n",
    "                birdclef_path = cache_competition_path\n",
    "                print(f\"Usando i dati esistenti in: {birdclef_path}\")\n",
    "            if os.path.exists(cache_model_path):\n",
    "                model_path = cache_model_path\n",
    "                print(f\"Usando il modello esistente in: {model_path}\")\n",
    "    \n",
    "    # Aggiorna i percorsi nella configurazione\n",
    "    config.BASE_DIR = birdclef_path\n",
    "    config._setup_derived_paths()\n",
    "    config.MODELS_DIR = model_path\n",
    "    \n",
    "    print(f\"Dati disponibili in: {config.BASE_DIR}\")\n",
    "    print(f\"Modello disponibile in: {config.MODELS_DIR}\")\n",
    "\n",
    "# Stampa percorsi aggiornati\n",
    "print(f\"\\nPercorso file CSV di training: {config.TRAIN_CSV_PATH}\")\n",
    "print(f\"Percorso directory audio di training: {config.TRAIN_AUDIO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica l'esistenza delle directory e crea quelle necessarie per l'output\n",
    "\n",
    "def setup_output_directories():\n",
    "    \"\"\"\n",
    "    Configura le directory per l'output del progetto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary con i percorsi delle directory di output\n",
    "    \"\"\"\n",
    "    # Directory principale di output\n",
    "    output_dir = config.OUTPUT_DIR\n",
    "    \n",
    "    # Sotto-directory per diversi tipi di output\n",
    "    dirs = {\n",
    "        'checkpoints': os.path.join(output_dir, 'checkpoints'),\n",
    "        'tensorboard': os.path.join(output_dir, 'tensorboard_logs'),\n",
    "        'predictions': os.path.join(output_dir, 'predictions'),\n",
    "        'submissions': os.path.join(output_dir, 'submissions'),\n",
    "        'visualizations': os.path.join(output_dir, 'visualizations'),\n",
    "    }\n",
    "    \n",
    "    # Crea tutte le directory\n",
    "    for dir_name, dir_path in dirs.items():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Directory '{dir_name}' creata/verificata in: {dir_path}\")\n",
    "    \n",
    "    return dirs\n",
    "\n",
    "# Configura le directory di output\n",
    "output_dirs = setup_output_directories()\n",
    "\n",
    "# Crea un file di log per tenere traccia dei risultati\n",
    "log_file_path = os.path.join(config.OUTPUT_DIR, f\"experiment_log_passt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")\n",
    "\n",
    "with open(log_file_path, 'w') as log_file:\n",
    "    log_file.write(f\"=== BirdClef PASST Experiment Log ===\\n\")\n",
    "    log_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Environment: {config.environment}\\n\\n\")\n",
    "    log_file.write(\"Output directories:\\n\")\n",
    "    for dir_name, dir_path in output_dirs.items():\n",
    "        log_file.write(f\"- {dir_name}: {dir_path}\\n\")\n",
    "\n",
    "print(f\"File di log creato in: {log_file_path}\")\n",
    "\n",
    "# Memorizziamo i parametri di configurazione principali per l'addestramento\n",
    "print(\"\\nParametri di configurazione principali:\")\n",
    "print(f\"- Sample rate: {config.SR} Hz\")\n",
    "print(f\"- Durata clip audio: {config.DURATION} secondi\")\n",
    "print(f\"- Numero bande Mel: {config.N_MELS}\")\n",
    "print(f\"- Dimensione FFT: {config.N_FFT}\")\n",
    "print(f\"- Hop length: {config.HOP_LENGTH}\")\n",
    "print(f\"- Device: {config.DEVICE}\")\n",
    "print(f\"- Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"- Epoche: {config.EPOCHS}\")\n",
    "print(f\"- Parametri PASST: {config.HIDDEN_DIM} hidden dim, {config.NUM_HEADS} attention heads, {config.NUM_LAYERS} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dei metadati\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Carica e prepara i metadati dal file CSV di training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: training_df, all_species, labels_one_hot\n",
    "    \"\"\"\n",
    "    print(f\"Caricamento metadati da: {config.TRAIN_CSV_PATH}\")\n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    sample_sub_df = pd.read_csv(config.SAMPLE_SUB_PATH)\n",
    "    \n",
    "    # Estrai tutte le etichette uniche\n",
    "    train_primary_labels = train_df['primary_label'].unique()\n",
    "    train_secondary_labels = set([lbl for sublist in train_df['secondary_labels'].apply(eval) \n",
    "                                 for lbl in sublist if lbl])\n",
    "    submission_species = sample_sub_df.columns[1:].tolist()  # Escludi row_id\n",
    "    \n",
    "    # Combina tutte le possibili etichette\n",
    "    all_species = sorted(list(set(train_primary_labels) | train_secondary_labels | set(submission_species)))\n",
    "    N_CLASSES = len(all_species)\n",
    "    config.N_CLASSES = N_CLASSES  # Aggiorna il numero di classi nella configurazione\n",
    "    \n",
    "    print(f\"Numero totale di specie trovate: {N_CLASSES}\")\n",
    "    print(f\"Prime 10 specie: {all_species[:10]}\")\n",
    "    \n",
    "    # Crea mappatura etichette-indici\n",
    "    species_to_int = {species: i for i, species in enumerate(all_species)}\n",
    "    int_to_species = {i: species for species, i in species_to_int.items()}\n",
    "    \n",
    "    # Aggiungi indici numerici al dataframe\n",
    "    train_df['primary_label_int'] = train_df['primary_label'].map(species_to_int)\n",
    "    \n",
    "    # Prepara target multi-etichetta\n",
    "    mlb = MultiLabelBinarizer(classes=all_species)\n",
    "    mlb.fit(None)  # Fit con tutte le classi\n",
    "    \n",
    "    def get_multilabel(row):\n",
    "        labels = eval(row['secondary_labels'])  # Valuta la lista di stringhe in modo sicuro\n",
    "        labels.append(row['primary_label'])\n",
    "        return list(set(labels))  # Assicura etichette uniche\n",
    "    \n",
    "    train_df['all_labels'] = train_df.apply(get_multilabel, axis=1)\n",
    "    train_labels_one_hot = mlb.transform(train_df['all_labels'])\n",
    "    \n",
    "    print(f\"Forma delle etichette one-hot: {train_labels_one_hot.shape}\")\n",
    "    \n",
    "    return train_df, all_species, train_labels_one_hot, species_to_int, int_to_species\n",
    "\n",
    "# Carica i metadati\n",
    "train_df, all_species, train_labels_one_hot, species_to_int, int_to_species = load_metadata()\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "def split_data(train_df, labels_one_hot, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Suddivide il dataset in set di training e validation.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame con i metadati\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        test_size: Percentuale dei dati da usare per validation\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "    \"\"\"\n",
    "    # Indici per lo split\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_df)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=train_df['primary_label']  # Stratifica in base alla label primaria\n",
    "    )\n",
    "    \n",
    "    # Crea i dataframe e gli array di etichette splittati\n",
    "    X_train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "    X_val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    y_train_one_hot = labels_one_hot[train_indices]\n",
    "    y_val_one_hot = labels_one_hot[val_indices]\n",
    "    \n",
    "    print(f\"Dimensioni Training Set: {X_train_df.shape}, Etichette: {y_train_one_hot.shape}\")\n",
    "    print(f\"Dimensioni Validation Set: {X_val_df.shape}, Etichette: {y_val_one_hot.shape}\")\n",
    "    \n",
    "    return X_train_df, X_val_df, y_train_one_hot, y_val_one_hot\n",
    "\n",
    "# Suddividi i dati in training e validation\n",
    "X_train_df, X_val_df, y_train_one_hot, y_val_one_hot = split_data(train_df, train_labels_one_hot)\n",
    "\n",
    "# Test_df sarà None per ora\n",
    "X_test_df = None\n",
    "y_test_one_hot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset_df(train_df, labels_one_hot, abundant_class_threshold=200, remove_percentage=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame bilanciato rimuovendo parte degli esempi con rating bassi dalle classi abbondanti.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame originale\n",
    "        labels_one_hot: Array di etichette one-hot\n",
    "        abundant_class_threshold: Soglia per definire una classe come \"abbondante\"\n",
    "        remove_percentage: Percentuale di esempi con rating 1-3 da rimuovere dalle classi abbondanti\n",
    "        random_state: Seed per riproducibilità\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame bilanciato, etichette one-hot bilanciate)\n",
    "    \"\"\"\n",
    "    # Conta esempi per ogni classe\n",
    "    class_counts = train_df['primary_label'].value_counts()\n",
    "    \n",
    "    # Identifica classi abbondanti\n",
    "    abundant_classes = class_counts[class_counts > abundant_class_threshold].index.tolist()\n",
    "    print(f\"Classi identificate come abbondanti (>{abundant_class_threshold} esempi): {len(abundant_classes)}\")\n",
    "    \n",
    "    # Copia il DataFrame originale\n",
    "    balanced_df = train_df.copy()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # Contatori per statistiche\n",
    "    total_removed = 0\n",
    "    removed_by_class = {}\n",
    "    \n",
    "    # Per ogni classe abbondante\n",
    "    for cls in abundant_classes:\n",
    "        # Filtra esempi con rating 1-3 per questa classe\n",
    "        low_quality_mask = (balanced_df['primary_label'] == cls) & (balanced_df['rating'].isin([1, 2, 3]))\n",
    "        low_quality_indices = balanced_df[low_quality_mask].index.tolist()\n",
    "        \n",
    "        # Numero di esempi da rimuovere\n",
    "        n_to_remove = int(len(low_quality_indices) * remove_percentage)\n",
    "        \n",
    "        # Seleziona casualmente gli indici da rimuovere\n",
    "        np.random.seed(random_state)\n",
    "        if n_to_remove > 0:\n",
    "            indices_to_remove = np.random.choice(low_quality_indices, size=n_to_remove, replace=False)\n",
    "            \n",
    "            # Memorizza gli indici da rimuovere\n",
    "            rows_to_drop.extend(indices_to_remove)\n",
    "            \n",
    "            # Aggiorna statistiche\n",
    "            removed_by_class[cls] = n_to_remove\n",
    "            total_removed += n_to_remove\n",
    "    \n",
    "    # Rimuovi le righe selezionate\n",
    "    if rows_to_drop:\n",
    "        balanced_df = balanced_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        \n",
    "        # Aggiorna anche le etichette one-hot rimuovendo gli stessi indici\n",
    "        mask = np.ones(len(train_df), dtype=bool)\n",
    "        mask[rows_to_drop] = False\n",
    "        balanced_labels = labels_one_hot[mask]\n",
    "    else:\n",
    "        balanced_labels = labels_one_hot\n",
    "    \n",
    "    # Statistiche finali\n",
    "    print(f\"Totale esempi rimossi: {total_removed} ({total_removed/len(train_df):.1%} del dataset originale)\")\n",
    "    print(f\"Dimensione dataset originale: {len(train_df)}\")\n",
    "    print(f\"Dimensione dataset bilanciato: {len(balanced_df)}\")\n",
    "    \n",
    "    return balanced_df, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentations:\n",
    "    def __init__(self, p_time_shift=0.5, p_time_mask=0.5, p_freq_mask=0.5, p_mixup=0.3):\n",
    "        \"\"\"\n",
    "        Inizializza le trasformazioni per data augmentation audio per il modello PASST.\n",
    "        \n",
    "        Args:\n",
    "            p_time_shift: Probabilità di applicare time shifting\n",
    "            p_time_mask: Probabilità di applicare mascheramento temporale\n",
    "            p_freq_mask: Probabilità di applicare mascheramento frequenziale\n",
    "            p_mixup: Probabilità di applicare mixup\n",
    "        \"\"\"\n",
    "        self.p_time_shift = p_time_shift\n",
    "        self.p_time_mask = p_time_mask\n",
    "        self.p_freq_mask = p_freq_mask\n",
    "        self.p_mixup = p_mixup\n",
    "        \n",
    "    def apply_time_shift(self, spec):\n",
    "        \"\"\"Applica time shift allo spettrogramma\"\"\"\n",
    "        if torch.rand(1).item() < self.p_time_shift:\n",
    "            shift_amount = int(spec.shape[2] * 0.2)  # Shift fino al 20% della lunghezza\n",
    "            direction = 1 if torch.rand(1).item() > 0.5 else -1\n",
    "            shift = torch.randint(1, shift_amount + 1, (1,)).item() * direction\n",
    "            spec = torch.roll(spec, shifts=shift, dims=2)\n",
    "        return spec\n",
    "        \n",
    "    def apply_time_mask(self, spec):\n",
    "        \"\"\"Applica maschere casuali sull'asse temporale (X) dello spettrogramma\"\"\"\n",
    "        if torch.rand(1).item() < self.p_time_mask:\n",
    "            _, _, width = spec.shape\n",
    "            mask_len = int(width * torch.rand(1).item() * 0.2)  # Maschera fino al 20% della larghezza\n",
    "            mask_start = torch.randint(0, width - mask_len, (1,))\n",
    "            spec[:, :, mask_start:mask_start+mask_len] = 0\n",
    "        return spec\n",
    "        \n",
    "    def apply_freq_mask(self, spec):\n",
    "        \"\"\"Applica maschere casuali sull'asse frequenziale (Y) dello spettrogramma\"\"\"\n",
    "        if torch.rand(1).item() < self.p_freq_mask:\n",
    "            _, height, _ = spec.shape\n",
    "            mask_len = int(height * torch.rand(1).item() * 0.2)  # Maschera fino al 20% dell'altezza\n",
    "            mask_start = torch.randint(0, height - mask_len, (1,))\n",
    "            spec[:, mask_start:mask_start+mask_len, :] = 0\n",
    "        return spec\n",
    "        \n",
    "    def apply_all(self, spec):\n",
    "        \"\"\"Applica tutte le augmentations in cascata\"\"\"\n",
    "        spec = self.apply_time_shift(spec)\n",
    "        spec = self.apply_time_mask(spec)\n",
    "        spec = self.apply_freq_mask(spec)\n",
    "        return spec\n",
    "\n",
    "# Funzione per fare mixup tra esempi nel batch\n",
    "def mixup_batch(inputs, targets, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Applica mixup tra esempi di un batch.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Tensor di input [batch_size, channels, height, width]\n",
    "        targets: Tensor di target [batch_size, num_classes]\n",
    "        alpha: Parametro per distribuzione beta\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (inputs mixati, targets mixati)\n",
    "    \"\"\"\n",
    "    batch_size = inputs.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    \n",
    "    # Preleva lambda dalla distribuzione beta\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    \n",
    "    # Mixa gli input\n",
    "    mixed_inputs = lam * inputs + (1 - lam) * inputs[indices]\n",
    "    \n",
    "    # Mixa i target\n",
    "    mixed_targets = lam * targets + (1 - lam) * targets[indices]\n",
    "    \n",
    "    return mixed_inputs, mixed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, target_sr=config.SR, duration=config.DURATION, segment_position='center', random_segment=False):\n",
    "    \"\"\"\n",
    "    Carica un file audio, estrae un segmento specifico e lo converte in spettrogramma Mel.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Percorso del file audio\n",
    "        target_sr: Sample rate target\n",
    "        duration: Durata target in secondi\n",
    "        segment_position: Posizione del segmento ('start', 'center', 'end')\n",
    "        random_segment: Se True, estrae un segmento casuale\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Spettrogramma Mel log-normalizzato\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carica il file audio\n",
    "        y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        \n",
    "        # Lunghezza target in campioni\n",
    "        target_len = int(target_sr * duration)\n",
    "        total_len = len(y)\n",
    "        \n",
    "        # Gestisci clip troppo corte\n",
    "        if total_len < target_len:\n",
    "            import math\n",
    "            n_copy = math.ceil(target_len / total_len)\n",
    "            if n_copy > 1:\n",
    "                y = np.tile(y, n_copy)\n",
    "            total_len = len(y)\n",
    "        \n",
    "        # Seleziona il segmento\n",
    "        if random_segment and total_len > target_len:\n",
    "            # Estrai segmento casuale\n",
    "            max_start_idx = total_len - target_len\n",
    "            start_idx = np.random.randint(0, max_start_idx)\n",
    "        else:\n",
    "            # Usa le posizioni predefinite\n",
    "            if segment_position == 'start':\n",
    "                start_idx = int(total_len * 0.2)\n",
    "                if start_idx + target_len > total_len:\n",
    "                    start_idx = max(0, total_len - target_len)\n",
    "            elif segment_position == 'end':\n",
    "                end_point = int(total_len * 0.8)\n",
    "                start_idx = max(0, end_point - target_len)\n",
    "            else:  # 'center' (default)\n",
    "                start_idx = max(0, int(total_len / 2 - target_len / 2))\n",
    "        \n",
    "        # Estrai il segmento\n",
    "        y = y[start_idx:start_idx + target_len]\n",
    "        \n",
    "        # Padda se necessario\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)), mode='constant')\n",
    "        \n",
    "        # Calcola lo spettrogramma Mel\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr,\n",
    "            n_fft=config.N_FFT,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            n_mels=config.N_MELS,\n",
    "            fmin=config.FMIN,\n",
    "            fmax=config.FMAX,\n",
    "            power=config.POWER\n",
    "        )\n",
    "        \n",
    "        # Converti in scala logaritmica (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalizza\n",
    "        min_val = np.min(log_mel_spec)\n",
    "        max_val = np.max(log_mel_spec)\n",
    "        if max_val > min_val:\n",
    "            log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            log_mel_spec = np.zeros_like(log_mel_spec)\n",
    "        \n",
    "        return log_mel_spec\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "        time_steps = int(target_sr * duration / config.HOP_LENGTH) + 1\n",
    "        return np.zeros((config.N_MELS, time_steps), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset che estrae solo il segmento centrale per ogni clip audio.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Attenzione: File non trovato in {file_path}.\")\n",
    "            time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "            dummy_spec = torch.zeros((1, config.N_MELS, time_steps), dtype=torch.float32)\n",
    "            dummy_label = torch.zeros(config.N_CLASSES, dtype=torch.float32)\n",
    "            return dummy_spec, dummy_label\n",
    "            \n",
    "        # Carica e preprocessa l'audio con solo segmento centrale\n",
    "        mel_spec = load_and_preprocess_audio(file_path, segment_position='center')\n",
    "        \n",
    "        # Aggiungi dimensione del canale e converti in tensor\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "        \n",
    "        # Ottieni le etichette\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            mel_spec_tensor = self.transform(mel_spec_tensor)\n",
    "            \n",
    "        return mel_spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBirdDatasetPASST(Dataset):\n",
    "    def __init__(self, df, audio_dir, labels_one_hot, augmentations=None):\n",
    "        \"\"\"\n",
    "        Dataset adattivo con supporto per augmentation specifiche per PASST.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con i metadati\n",
    "            audio_dir: Directory contenente i file audio\n",
    "            labels_one_hot: Array di etichette one-hot\n",
    "            augmentations: Istanza di AudioAugmentations\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.labels = labels_one_hot\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "        # Pre-calcola quali segmenti usare per ogni clip\n",
    "        self.segments_to_use = []\n",
    "        print(\"Analizzando le lunghezze delle clip audio...\")\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparazione dataset PASST\"):\n",
    "            file_path = os.path.join(audio_dir, row['filename'])\n",
    "            try:\n",
    "                # Carica solo l'informazione sulla durata senza caricare l'intero file\n",
    "                y, sr = librosa.load(file_path, sr=None, duration=0.1)  # Carica solo un breve segmento per ottenere SR\n",
    "                info = librosa.get_duration(filename=file_path, sr=sr)\n",
    "                total_duration = info  # Durata in secondi\n",
    "                \n",
    "                # Determina quali segmenti usare in base alla durata\n",
    "                if total_duration < config.DURATION * 1.5:\n",
    "                    # Clip troppo corta per multiple segmenti, usa solo il centro\n",
    "                    segments = ['center']\n",
    "                elif total_duration < config.DURATION * 2.5:\n",
    "                    # Clip media, usa inizio e fine\n",
    "                    segments = ['start', 'end']\n",
    "                else:\n",
    "                    # Clip abbastanza lunga, usa tutti e tre i segmenti\n",
    "                    segments = ['start', 'center', 'end']\n",
    "                \n",
    "                # Memorizza l'indice originale e i segmenti da utilizzare\n",
    "                for segment in segments:\n",
    "                    self.segments_to_use.append((idx, segment))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # In caso di errore, usa solo il segmento centrale\n",
    "                self.segments_to_use.append((idx, 'center'))\n",
    "                print(f\"Errore nell'elaborazione di {file_path}: {e}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segments_to_use)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df_idx, segment_position = self.segments_to_use[idx]\n",
    "        \n",
    "        # Ottieni il record dal DataFrame originale\n",
    "        row = self.df.iloc[df_idx]\n",
    "        filename = row['filename']\n",
    "        file_path = os.path.join(self.audio_dir, filename)\n",
    "        \n",
    "        # Determina se usare un segmento casuale\n",
    "        use_random_segment = self.augmentations is not None\n",
    "        \n",
    "        # Carica e preprocessa l'audio con il segmento selezionato o casuale\n",
    "        mel_spec = load_and_preprocess_audio(\n",
    "            file_path, \n",
    "            segment_position=segment_position,\n",
    "            random_segment=use_random_segment\n",
    "        )\n",
    "        \n",
    "        # Aggiungi dimensione del canale e converti in tensor\n",
    "        mel_spec = np.expand_dims(mel_spec, axis=0)\n",
    "        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "        \n",
    "        # Applica le augmentations se attive\n",
    "        if self.augmentations is not None:\n",
    "            mel_spec_tensor = self.augmentations.apply_all(mel_spec_tensor)\n",
    "            \n",
    "        # Ottieni le etichette corrispondenti\n",
    "        label_tensor = torch.tensor(self.labels[df_idx], dtype=torch.float32)\n",
    "            \n",
    "        return mel_spec_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Convertire le immagini in embedded patches\"\"\"\n",
    "    def __init__(self, img_size=(128, 320), patch_size=16, in_channels=1, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size)\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, patches_H, patches_W)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, drop=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            out_features=dim,\n",
    "            drop=drop\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PASSTransformer(nn.Module):\n",
    "    def __init__(self, img_size=(128, 320), patch_size=16, in_channels=1, \n",
    "                 n_classes=config.N_CLASSES, embed_dim=768, depth=12, \n",
    "                 n_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Numero di token\n",
    "        self.n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Posizione dei token+CLS\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, embed_dim))\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        # Blocchi transformer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Normalizzazione finale\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Head di classificazione\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "        # Inizializzazione\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "            \n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Embedding dei patch\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Aggiungi il token CLS\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Aggiungi l'embedding posizionale\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Applica i blocchi transformer\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Normalizzazione finale\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Restituisci solo il token CLS\n",
    "        return x[:, 0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlla se esiste un checkpoint precedente\n",
    "has_previous_checkpoint = False\n",
    "if config.environment == 'kaggle':\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(latest_checkpoint)\n",
    "elif config.environment == 'colab':\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint.pth'\n",
    "    has_previous_checkpoint = os.path.exists(drive_checkpoint)\n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint.pth')\n",
    "    has_previous_checkpoint = os.path.exists(local_checkpoint)\n",
    "\n",
    "# Calcola le dimensioni dell'input\n",
    "time_steps = int(config.SR * config.DURATION / config.HOP_LENGTH) + 1\n",
    "img_size = (config.N_MELS, time_steps)\n",
    "print(f\"Dimensione spettrogrammi: {img_size}\")\n",
    "\n",
    "# Inizializza il modello PASST\n",
    "model = PASSTransformer(\n",
    "    img_size=img_size,\n",
    "    patch_size=config.PATCH_SIZE,\n",
    "    in_channels=1,\n",
    "    n_classes=config.N_CLASSES,\n",
    "    embed_dim=config.HIDDEN_DIM,\n",
    "    depth=config.NUM_LAYERS,\n",
    "    n_heads=config.NUM_HEADS,\n",
    "    mlp_ratio=config.MLP_RATIO,\n",
    "    drop_rate=config.DROPOUT\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Stampa il riepilogo del modello\n",
    "print(f\"Modello PASST creato con {sum(p.numel() for p in model.parameters())/1e6:.1f}M parametri\")\n",
    "print(f\"Immagine di input: {img_size}, Patch: {config.PATCH_SIZE}, Canali: 1\")\n",
    "print(f\"Dim embed: {config.HIDDEN_DIM}, Heads: {config.NUM_HEADS}, Layers: {config.NUM_LAYERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica il bilanciamento strategico solo al dataset di training\n",
    "print(\"\\n=== Bilanciamento Strategico del Dataset di Training ===\")\n",
    "X_train_df_balanced, y_train_one_hot_balanced = create_balanced_dataset_df(\n",
    "    X_train_df, \n",
    "    y_train_one_hot,\n",
    "    abundant_class_threshold=150,  # Classi con più di 150 esempi sono considerate abbondanti\n",
    "    remove_percentage=0.4  # Rimuove il 40% degli esempi con rating bassi\n",
    ")\n",
    "\n",
    "# Crea un'istanza delle augmentations audio specifiche per PASST\n",
    "audio_augmentations = AudioAugmentations(\n",
    "    p_time_shift=0.5, \n",
    "    p_time_mask=0.5, \n",
    "    p_freq_mask=0.5, \n",
    "    p_mixup=0.3\n",
    ")\n",
    "\n",
    "# Creiamo i dataset utilizzando il dataset adattivo per il training con augmentations\n",
    "print(\"Creazione dataset di training con approccio adattivo e augmentations...\")\n",
    "train_dataset = AdaptiveBirdDatasetPASST(\n",
    "    X_train_df_balanced, \n",
    "    config.TRAIN_AUDIO_DIR, \n",
    "    y_train_one_hot_balanced,\n",
    "    augmentations=audio_augmentations\n",
    ")\n",
    "\n",
    "# Per validation, non usiamo augmentations\n",
    "print(\"Creazione dataset di validation con segmento centrale...\")\n",
    "val_dataset = BirdDataset(X_val_df, config.TRAIN_AUDIO_DIR, y_val_one_hot)\n",
    "\n",
    "# Stampa informazioni sulla dimensione effettiva del dataset\n",
    "print(f\"\\nNumero di record originali nel training set: {len(X_train_df)}\")\n",
    "print(f\"Numero di campioni effettivi nel training set dopo l'adattamento: {len(train_dataset)}\")\n",
    "print(f\"Rapporto di espansione: {len(train_dataset) / len(X_train_df):.2f}x\")\n",
    "\n",
    "# Implementa una funzione di collate personalizzata per mixup\n",
    "def mixup_collate_fn(batch):\n",
    "    \"\"\"Collate function con supporto per mixup batch-wise\"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Estrai input e target dal batch\n",
    "    for input_tensor, target_tensor in batch:\n",
    "        inputs.append(input_tensor)\n",
    "        targets.append(target_tensor)\n",
    "    \n",
    "    # Stack per creare tensor batch\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "    \n",
    "    # Applica mixup con 30% di probabilità\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        inputs, targets = mixup_batch(inputs, targets, alpha=0.4)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "# Creiamo i dataloader con mixup per il training\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    collate_fn=mixup_collate_fn\n",
    ")\n",
    "\n",
    "# Per validation, non usiamo mixup\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=config.NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Non creiamo un test_loader per ora\n",
    "test_loader = None\n",
    "    \n",
    "print(f\"Numero di batch di training per epoca: {len(train_loader)}\")\n",
    "print(f\"Numero di batch di validation per epoca: {len(val_loader)}\")\n",
    "print(\"Test set: utilizzeremo direttamente i file nella cartella test_soundscapes\")\n",
    "\n",
    "# Ottimizzatore con weight decay - stile ViT\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler - CosineAnnealingLR con warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr=1e-6):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(min_lr, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Calcola il numero totale di step di training\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "warmup_steps = int(total_steps * 0.1)  # 10% di warmup\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Loss function - BCEWithLogitsLoss per classificazione multi-label\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                epochs=config.EPOCHS, device=config.DEVICE, \n",
    "                model_save_path=None, model_load_path=None, patience=3,\n",
    "                resume_training=True):\n",
    "    \"\"\"\n",
    "    Addestra il modello PASST e valuta su validation set con supporto per checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PyTorch da addestrare\n",
    "        train_loader: DataLoader per dati di training\n",
    "        val_loader: DataLoader per dati di validation\n",
    "        criterion: Funzione di loss\n",
    "        optimizer: Ottimizzatore\n",
    "        scheduler: Learning rate scheduler\n",
    "        epochs: Numero di epoche di training\n",
    "        device: Device per l'addestramento ('cuda' o 'cpu')\n",
    "        model_save_path: Path dove salvare il modello addestrato\n",
    "        model_load_path: Path da cui caricare un modello pre-addestrato\n",
    "        patience: Numero di epoche senza miglioramento prima di terminare l'addestramento\n",
    "        resume_training: Se True, riprende il training da un checkpoint (se disponibile)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_losses, val_losses, total_training_time)\n",
    "    \"\"\"\n",
    "    # Directory per i checkpoint in base all'ambiente\n",
    "    checkpoint_dir = None\n",
    "    drive_mounted = False\n",
    "    \n",
    "    # Configura la directory per i checkpoint a seconda dell'ambiente\n",
    "    if config.environment == 'colab':\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            # Controlla se il drive è già montato\n",
    "            if not os.path.exists('/content/drive'):\n",
    "                print(\"Montaggio di Google Drive...\")\n",
    "                drive.mount('/content/drive')\n",
    "                print(\"Google Drive montato con successo.\")\n",
    "            \n",
    "            # Crea directory per i checkpoint se non esiste\n",
    "            checkpoint_dir = '/content/drive/MyDrive/birdclef_checkpoints'\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            print(f\"Directory per i checkpoint creata su Google Drive: {checkpoint_dir}\")\n",
    "            \n",
    "            # Aggiorna il percorso di salvataggio per usare Google Drive\n",
    "            if model_save_path:\n",
    "                filename = os.path.basename(model_save_path)\n",
    "                model_save_path = os.path.join(checkpoint_dir, filename)\n",
    "                print(f\"Il modello sarà salvato in: {model_save_path}\")\n",
    "            \n",
    "            drive_mounted = True\n",
    "        except ImportError:\n",
    "            print(\"Errore: Non riesco ad accedere a Google Drive. Continuo senza persistenza.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il montaggio di Google Drive: {e}\")\n",
    "            print(\"Continuo senza persistenza su Drive.\")\n",
    "    elif config.environment == 'kaggle':\n",
    "        # In Kaggle, usa la directory di working\n",
    "        checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in Kaggle: {checkpoint_dir}\")\n",
    "    else:\n",
    "        # In locale, usa la directory 'checkpoints' nell'OUTPUT_DIR\n",
    "        checkpoint_dir = os.path.join(config.OUTPUT_DIR, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Directory per i checkpoint creata in locale: {checkpoint_dir}\")\n",
    "    \n",
    "    # Inizializzazione variabili\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    total_training_time = 0\n",
    "    start_epoch = 0\n",
    "    needs_training = True\n",
    "    checkpoint_exists = False\n",
    "    model_loaded = False\n",
    "    \n",
    "    # Verifica se esiste un modello pre-addestrato da caricare\n",
    "    if model_load_path and os.path.exists(model_load_path):\n",
    "        print(f\"Modello trovato in {model_load_path}. Tentativo di caricamento...\")\n",
    "        try:\n",
    "            checkpoint = torch.load(model_load_path, map_location=device)\n",
    "            \n",
    "            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            print(\"Modello caricato con successo.\")\n",
    "            model_loaded = True\n",
    "            needs_training = False\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "            print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            needs_training = True\n",
    "    else:\n",
    "        print(f\"Modello non trovato in {model_load_path}.\")\n",
    "    \n",
    "    # Cerca un checkpoint SOLO se il caricamento del modello è fallito E resume_training è True\n",
    "    if needs_training and resume_training and checkpoint_dir and not model_loaded:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, \"latest_checkpoint_passt.pth\")\n",
    "        if os.path.exists(latest_checkpoint):\n",
    "            print(f\"Trovato checkpoint in {latest_checkpoint}. Tentativo di caricamento...\")\n",
    "            try:\n",
    "                checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "                \n",
    "                # Verifica che sia un checkpoint compatibile prima di caricarlo\n",
    "                if isinstance(checkpoint, dict) and 'epoch' in checkpoint:\n",
    "                    try:\n",
    "                        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                        start_epoch = checkpoint['epoch'] + 1\n",
    "                        train_losses = checkpoint['train_losses']\n",
    "                        val_losses = checkpoint['val_losses']\n",
    "                        best_val_loss = checkpoint['best_val_loss']\n",
    "                        epochs_without_improvement = checkpoint['epochs_without_improvement']\n",
    "                        total_training_time = checkpoint.get('total_training_time', 0)\n",
    "                        \n",
    "                        # Ricrea lo scheduler con lo stato salvato se presente\n",
    "                        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "                            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                        \n",
    "                        print(f\"Checkpoint caricato con successo (epoca {start_epoch-1})\")\n",
    "                        print(f\"Si riparte dall'epoca {start_epoch}/{epochs}\")\n",
    "                        \n",
    "                        if start_epoch >= epochs:\n",
    "                            needs_training = False\n",
    "                        \n",
    "                        checkpoint_exists = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Il checkpoint non è compatibile con il modello attuale: {e}\")\n",
    "                        print(\"Verrà avviato l'addestramento da zero.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il caricamento del checkpoint: {e}\")\n",
    "                print(\"Si procederà con il training da zero.\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Esegui training solo se necessario\n",
    "    if needs_training:\n",
    "        start_time_total = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        # Loop di training sulle epoche (inizia da start_epoch)\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # --- Fase di Training ---\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            pbar_train = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                             desc=f\"Epoca {epoch+1}/{epochs} [Train]\", leave=True)\n",
    "            \n",
    "            for i, (inputs, labels) in pbar_train:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Aggiorna lo scheduler ad ogni step\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                pbar_train.set_postfix({'loss': f\"{avg_loss:.4f}\"})\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            \n",
    "            # --- Fase di Validation ---\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), \n",
    "                           desc=f\"Epoca {epoch+1}/{epochs} [Val]\", leave=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (val_inputs, val_labels) in pbar_val:\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    \n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    avg_val_loss = running_val_loss / (i + 1)\n",
    "                    pbar_val.set_postfix({'val_loss': f\"{avg_val_loss:.4f}\"})\n",
    "            \n",
    "            epoch_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            total_training_time += epoch_duration\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Duration: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            # Salvataggio checkpoint per ogni epoca (in qualsiasi ambiente)\n",
    "            if checkpoint_dir:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f\"passt_epoch_{epoch+1}.pth\")\n",
    "                \n",
    "                # Salva checkpoint completo con tutte le informazioni di stato\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_without_improvement': epochs_without_improvement,\n",
    "                    'total_training_time': total_training_time\n",
    "                }\n",
    "                \n",
    "                # Salva anche lo stato dello scheduler se esiste\n",
    "                if scheduler is not None:\n",
    "                    checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "                print(f\"Checkpoint completo salvato in {checkpoint_path}\")\n",
    "                \n",
    "                # Aggiorna anche il checkpoint più recente (sovrascrive)\n",
    "                torch.save(checkpoint, os.path.join(checkpoint_dir, \"latest_checkpoint_passt.pth\"))\n",
    "            \n",
    "            # Early stopping\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                # Salva il miglior modello separatamente\n",
    "                if model_save_path:\n",
    "                    best_path = model_save_path.replace('.pth', '_best.pth')\n",
    "                    \n",
    "                    # Salva checkpoint completo\n",
    "                    best_checkpoint = {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_losses': train_losses,\n",
    "                        'val_losses': val_losses,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    }\n",
    "                    \n",
    "                    # Salva anche lo stato dello scheduler\n",
    "                    if scheduler is not None:\n",
    "                        best_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                    \n",
    "                    torch.save(best_checkpoint, best_path)\n",
    "                    print(f\"Salvato miglior modello in {best_path}\")\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping attivato! Nessun miglioramento per {patience} epoche consecutive.\")\n",
    "                break\n",
    "        \n",
    "        end_time_total = time.time()\n",
    "        if checkpoint_exists:\n",
    "            total_training_time += (end_time_total - start_time_total)\n",
    "        else:\n",
    "            total_training_time = end_time_total - start_time_total\n",
    "            \n",
    "        print(f\"\\nTraining terminato in {total_training_time/60:.2f} minuti totali\")\n",
    "        \n",
    "        # Salva il modello finale\n",
    "        if model_save_path:\n",
    "            final_checkpoint = {\n",
    "                'epoch': epochs-1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'total_training_time': total_training_time\n",
    "            }\n",
    "            \n",
    "            # Salva anche lo stato dello scheduler\n",
    "            if scheduler is not None:\n",
    "                final_checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "                \n",
    "            torch.save(final_checkpoint, model_save_path)\n",
    "            print(f\"Modello finale salvato in {model_save_path}\")\n",
    "    else:\n",
    "        print(\"Training non necessario: modello già caricato o training ripreso e completato.\")\n",
    "    \n",
    "    # Visualizza le curve di loss\n",
    "    if train_losses and val_losses:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoche')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Curve di Loss di Training e Validation')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Salva il grafico\n",
    "        if checkpoint_dir:\n",
    "            plt_path = os.path.join(checkpoint_dir, 'loss_curves_passt.png')\n",
    "            plt.savefig(plt_path)\n",
    "            print(f\"Grafico delle curve di loss salvato in {plt_path}\")\n",
    "    \n",
    "    return train_losses, val_losses, total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorsi per caricamento/salvataggio del modello\n",
    "if config.environment == 'kaggle':\n",
    "    # Directory per i checkpoint in Kaggle\n",
    "    os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Verifica se esiste un checkpoint precedente\n",
    "    latest_checkpoint = '/kaggle/working/checkpoints/latest_checkpoint_passt.pth'\n",
    "    if os.path.exists(latest_checkpoint):\n",
    "        model_load_path = latest_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente in {latest_checkpoint}\")\n",
    "    else:\n",
    "        # Usa un modello base precaricato se disponibile\n",
    "        model_load_path = \"/kaggle/input/passt-model/passt_pretrained.pth\"\n",
    "        \n",
    "    # Imposta il percorso di salvataggio\n",
    "    model_save_path = \"/kaggle/working/birdclef_trained_model_passt.pth\"\n",
    "    \n",
    "elif config.environment == 'colab':\n",
    "    # Per Colab, verifica se esiste un checkpoint su Drive\n",
    "    drive_checkpoint = '/content/drive/MyDrive/birdclef_checkpoints/latest_checkpoint_passt.pth'\n",
    "    if os.path.exists(drive_checkpoint):\n",
    "        model_load_path = drive_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente su Drive: {drive_checkpoint}\")\n",
    "    else:\n",
    "        # Usa un modello preaddestrato se disponibile\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"passt_model.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"passt_model.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(config.OUTPUT_DIR, f\"birdclef_model_passt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "    \n",
    "else:\n",
    "    # Per ambienti locali\n",
    "    local_checkpoint = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'latest_checkpoint_passt.pth')\n",
    "    if os.path.exists(local_checkpoint):\n",
    "        model_load_path = local_checkpoint\n",
    "        print(f\"Trovato checkpoint precedente: {local_checkpoint}\")\n",
    "    else:\n",
    "        model_load_path = os.path.join(config.MODELS_DIR, \"passt_model.pth\") if os.path.exists(os.path.join(config.MODELS_DIR, \"passt_model.pth\")) else None\n",
    "    \n",
    "    model_save_path = os.path.join(output_dirs['checkpoints'], f\"birdclef_model_passt_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\")\n",
    "\n",
    "# Esegui l'addestramento\n",
    "print(\"\\n=== Avvio dell'addestramento del modello PASST ===\")\n",
    "train_losses, val_losses, training_time = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,  # Passa lo scheduler\n",
    "    epochs=config.EPOCHS,\n",
    "    device=config.DEVICE,\n",
    "    model_save_path=model_save_path,\n",
    "    model_load_path=model_load_path,\n",
    "    resume_training=True\n",
    ")\n",
    "\n",
    "print(f\"\\nAddestramento completato in {training_time/60:.2f} minuti.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di valutazione del modello\n",
    "def evaluate_model(model, val_loader, criterion, device=config.DEVICE):\n",
    "    \"\"\"\n",
    "    Valuta il modello sul set di validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello da valutare\n",
    "        val_loader: DataLoader per validation\n",
    "        criterion: Funzione di loss\n",
    "        device: Device per l'inferenza\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (val_loss, top1_acc, top3_acc, top5_acc, mAP)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validazione\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Converti output in probabilità\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Salva targets e predictions per il calcolo delle metriche\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    \n",
    "    # Calcola la loss media\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Concatena i risultati\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    \n",
    "    # Calcola metriche\n",
    "    from sklearn.metrics import average_precision_score, accuracy_score\n",
    "    \n",
    "    # Funzione per calcolare l'accuratezza top-k\n",
    "    def top_k_accuracy(y_true, y_pred, k):\n",
    "        # Per ogni esempio, trova l'indice delle k classi con score più alto\n",
    "        top_k_indices = np.argsort(y_pred, axis=1)[:, -k:]\n",
    "        # Crea una maschera per le top-k predizioni\n",
    "        top_k_mask = np.zeros_like(y_pred)\n",
    "        for i, indices in enumerate(top_k_indices):\n",
    "            top_k_mask[i, indices] = 1\n",
    "        # Un esempio è corretto se almeno una delle top-k classi è positiva\n",
    "        correct = ((top_k_mask * y_true) > 0).sum(axis=1) > 0\n",
    "        return correct.mean()\n",
    "    \n",
    "    # Calcola le metriche\n",
    "    top1_acc = top_k_accuracy(all_targets, all_predictions, 1)\n",
    "    top3_acc = top_k_accuracy(all_targets, all_predictions, 3)\n",
    "    top5_acc = top_k_accuracy(all_targets, all_predictions, 5)\n",
    "    \n",
    "    # Mean Average Precision\n",
    "    mAP = average_precision_score(all_targets, all_predictions, average='macro')\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {top3_acc:.4f}\")\n",
    "    print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "    print(f\"Mean Average Precision: {mAP:.4f}\")\n",
    "    \n",
    "    return val_loss, top1_acc, top3_acc, top5_acc, mAP\n",
    "\n",
    "# Valuta il modello addestrato\n",
    "print(\"\\n=== Valutazione del Modello PASST ===\")\n",
    "val_metrics = evaluate_model(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, device=config.DEVICE):\n",
    "    \"\"\"\n",
    "    Genera un file di submission per Kaggle.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello addestrato\n",
    "        device: Device per inferenza\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame di submission\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Set seed per riproducibilità\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Percorso dei test soundscapes\n",
    "    test_soundscape_path = config.TEST_SOUNDSCAPES_DIR\n",
    "    test_soundscapes = [os.path.join(test_soundscape_path, afile) \n",
    "                        for afile in sorted(os.listdir(test_soundscape_path)) \n",
    "                        if afile.endswith('.ogg')]\n",
    "    \n",
    "    print(f\"Elaborazione di {len(test_soundscapes)} file soundscape...\")\n",
    "    \n",
    "    # Crea DataFrame per le predizioni\n",
    "    predictions = pd.DataFrame(columns=['row_id'] + all_species)\n",
    "    \n",
    "    for soundscape in tqdm(test_soundscapes, desc=\"Elaborazione soundscapes\"):\n",
    "        # Carica audio\n",
    "        sig, rate = librosa.load(path=soundscape, sr=config.SR)\n",
    "        \n",
    "        # Split in segmenti da 5 secondi\n",
    "        segment_length = rate * config.TEST_CLIP_DURATION\n",
    "        chunks = []\n",
    "        for i in range(0, len(sig), segment_length):\n",
    "            chunk = sig[i:i+segment_length]\n",
    "            # Padda se necessario\n",
    "            if len(chunk) < segment_length:\n",
    "                chunk = np.pad(chunk, (0, segment_length - len(chunk)), mode='constant')\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Genera predizioni per ogni segmento\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Calcola row_id (nome file + tempo finale del segmento in secondi)\n",
    "            file_name = os.path.basename(soundscape).split('.')[0]\n",
    "            row_id = f\"{file_name}_{i * config.TEST_CLIP_DURATION + config.TEST_CLIP_DURATION}\"\n",
    "            \n",
    "            # Calcola spettrogramma Mel\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=chunk, sr=config.SR,\n",
    "                n_fft=config.N_FFT,\n",
    "                hop_length=config.HOP_LENGTH,\n",
    "                n_mels=config.N_MELS,\n",
    "                fmin=config.FMIN,\n",
    "                fmax=config.FMAX\n",
    "            )\n",
    "            \n",
    "            # Converti in scala logaritmica (dB) e normalizza\n",
    "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            min_val = np.min(log_mel_spec)\n",
    "            max_val = np.max(log_mel_spec)\n",
    "            if max_val > min_val:\n",
    "                log_mel_spec = (log_mel_spec - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                log_mel_spec = np.zeros_like(log_mel_spec)\n",
    "            \n",
    "            # Aggiungi dimensione batch e canale\n",
    "            log_mel_spec = np.expand_dims(np.expand_dims(log_mel_spec, axis=0), axis=0)\n",
    "            \n",
    "            # Converti in tensor\n",
    "            input_tensor = torch.tensor(log_mel_spec, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Effettua predizione\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)\n",
    "                scores = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Aggiungi riga al DataFrame di predizioni\n",
    "            new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + all_species)\n",
    "            predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Salva la submission come CSV\n",
    "    submission_path = os.path.join(config.OUTPUT_DIR, \"submission_passt.csv\")\n",
    "    predictions.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission salvata in {submission_path}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Genera submission solo se siamo su Kaggle\n",
    "if config.environment == 'kaggle':\n",
    "    print(\"\\n=== Generazione del File di Submission ===\")\n",
    "    submission_df = generate_submission(model)\n",
    "    \n",
    "    if submission_df is not None:\n",
    "        print(\"\\nAnteprima del file di submission:\")\n",
    "        print(submission_df.head())\n",
    "else:\n",
    "    print(\"\\nSalto la generazione della submission perché non siamo su Kaggle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempi di visualizzazione delle attivazioni di attention\n",
    "def visualize_attention_maps(model, val_loader, device=config.DEVICE, num_examples=3):\n",
    "    \"\"\"\n",
    "    Visualizza le mappe di attenzione del modello PASST su esempi di validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Modello PASST\n",
    "        val_loader: DataLoader di validation\n",
    "        device: Dispositivo di esecuzione\n",
    "        num_examples: Numero di esempi da visualizzare\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ottieni alcuni esempi dal validation set\n",
    "    examples = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            examples.append((inputs, targets))\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 3, figsize=(15, 5 * num_examples))\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(examples):\n",
    "        # Seleziona un solo esempio dal batch\n",
    "        input_mel = inputs[0]  # [1, H, W]\n",
    "        target = targets[0]    # [num_classes]\n",
    "        \n",
    "        # Visualizza lo spettrogramma originale\n",
    "        axes[i, 0].imshow(input_mel[0].cpu().numpy(), aspect='auto', origin='lower')\n",
    "        axes[i, 0].set_title(\"Spettrogramma Mel\")\n",
    "        axes[i, 0].set_ylabel(\"Mel Bins\")\n",
    "        axes[i, 0].set_xlabel(\"Frames\")\n",
    "        \n",
    "        # Ottieni la classificazione\n",
    "        model_input = input_mel.unsqueeze(0).to(device)  # [1, 1, H, W]\n",
    "        with torch.no_grad():\n",
    "            output = model(model_input)\n",
    "            probs = torch.sigmoid(output)[0].cpu().numpy()\n",
    "        \n",
    "        # Trova le classi con probabilità più alta\n",
    "        top_indices = np.argsort(probs)[-5:][::-1]\n",
    "        top_species = [all_species[idx] for idx in top_indices]\n",
    "        top_probs = [probs[idx] for idx in top_indices]\n",
    "        \n",
    "        # Visualizza le probabilità predette\n",
    "        axes[i, 1].barh(range(5), top_probs)\n",
    "        axes[i, 1].set_yticks(range(5))\n",
    "        axes[i, 1].set_yticklabels(top_species)\n",
    "        axes[i, 1].set_title(\"Top-5 Predizioni\")\n",
    "        axes[i, 1].set_xlim(0, 1)\n",
    "        \n",
    "        # Calcola le classi vere\n",
    "        true_classes = []\n",
    "        for j, val in enumerate(target.cpu().numpy()):\n",
    "            if val > 0:\n",
    "                true_classes.append(all_species[j])\n",
    "        \n",
    "        # Visualizza un'approssimazione dell'attention map (dal token CLS alle posizioni)\n",
    "        # Nota: questo è solo un esempio, in un modello reale dovremmo estrarre l'attention\n",
    "        # La simuliamo prendendo l'attivazione delle features\n",
    "        features = model.forward_features(model_input).cpu().numpy()[0]\n",
    "        \n",
    "        # Simuliamo una mappa di attenzione ridimensionandola alle dimensioni dello spettrogramma\n",
    "        # In un'implementazione reale, accederemmo alle vere mappe di attention del transformer\n",
    "        attention_map = np.ones((config.N_MELS, time_steps))\n",
    "        \n",
    "        # Visualizza l'attention map simulata\n",
    "        axes[i, 2].imshow(attention_map, aspect='auto', origin='lower', cmap='viridis')\n",
    "        axes[i, 2].set_title(f\"Classi vere: {', '.join(true_classes)}\")\n",
    "        axes[i, 2].set_ylabel(\"Mel Bins\")\n",
    "        axes[i, 2].set_xlabel(\"Frames\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Salva la figura\n",
    "    if config.environment != 'local':\n",
    "        fig_path = os.path.join(config.OUTPUT_DIR, 'passt_visualizations.png')\n",
    "        plt.savefig(fig_path)\n",
    "        print(f\"Visualizzazioni salvate in {fig_path}\")\n",
    "\n",
    "# Visualizza alcuni esempi se non siamo in ambiente Kaggle\n",
    "if config.environment != 'kaggle':\n",
    "    print(\"\\n=== Visualizzazione di Esempi ===\")\n",
    "    try:\n",
    "        visualize_attention_maps(model, val_loader, num_examples=3)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la visualizzazione: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto tra i modelli EfficientNet e PASST (se disponibili)\n",
    "try:\n",
    "    # Tenta di caricare un modello EfficientNet (se è stato addestrato)\n",
    "    efficientnet_path = os.path.join(config.OUTPUT_DIR, 'checkpoints', 'birdclef_model_efficientnet_best.pth')\n",
    "    \n",
    "    if os.path.exists(efficientnet_path):\n",
    "        print(\"\\n=== Confronto tra EfficientNet e PASST ===\")\n",
    "        print(\"Caricamento del modello EfficientNet per confronto...\")\n",
    "        \n",
    "        # Qui si potrebbe implementare il caricamento del modello EfficientNet\n",
    "        # e confrontare le performance con PASST\n",
    "        \n",
    "        print(\"Confronto completato.\")\n",
    "    else:\n",
    "        print(\"\\nModello EfficientNet non trovato. Il confronto non verrà effettuato.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel tentativo di confronto tra modelli: {e}\")\n",
    "\n",
    "print(\"\\n=== Progetto PASST per BirdClef Completato ===\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
